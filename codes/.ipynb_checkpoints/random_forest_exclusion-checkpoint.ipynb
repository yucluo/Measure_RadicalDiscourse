{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import random \n",
    "import os\n",
    "import csv\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "import nltk\n",
    "\n",
    "## for word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "from numpy import loadtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf RF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8072892938496584\n",
      "PR AUC: 0.16302770766282285\n",
      "AUC: 0.7131818181818181\n",
      "PR AUC: 0.22040660343565313\n",
      "AUC: 0.8086363636363636\n",
      "PR AUC: 0.06949614583647451\n",
      "AUC: 0.6924829157175398\n",
      "PR AUC: 0.2294438174535073\n",
      "AUC: 0.4381818181818182\n",
      "PR AUC: 0.027598573239031858\n",
      "AUC: 0.8104545454545455\n",
      "PR AUC: 0.5505564537768415\n",
      "AUC: 0.8440909090909091\n",
      "PR AUC: 0.10832624777477803\n",
      "AUC: 0.8277272727272728\n",
      "PR AUC: 0.22490187853656177\n",
      "AUC: 0.7398633257403189\n",
      "PR AUC: 0.11090422993664077\n",
      "AUC: 0.7104545454545454\n",
      "PR AUC: 0.4096055695190565\n",
      "AUC: 0.7372727272727272\n",
      "PR AUC: 0.06398217678174048\n",
      "AUC: 0.6904545454545455\n",
      "PR AUC: 0.411083053192737\n",
      "AUC: 0.9325740318906606\n",
      "PR AUC: 0.6206149406149406\n",
      "AUC: 0.9427272727272726\n",
      "PR AUC: 0.33416623868685724\n",
      "AUC: 0.7263636363636363\n",
      "PR AUC: 0.24823054454054208\n",
      "AUC: 0.6031818181818182\n",
      "PR AUC: 0.06246345019812863\n",
      "AUC: 0.6177272727272727\n",
      "PR AUC: 0.2628521338107989\n",
      "AUC: 0.6927272727272727\n",
      "PR AUC: 0.2168243656340853\n",
      "AUC: 0.849090909090909\n",
      "PR AUC: 0.26325078479025843\n",
      "AUC: 0.8777272727272727\n",
      "PR AUC: 0.23866968706919464\n",
      "AUC: 0.9431818181818182\n",
      "PR AUC: 0.44222580645161286\n",
      "AUC: 0.6254545454545455\n",
      "PR AUC: 0.2162345174812661\n",
      "AUC: 0.7940774487471526\n",
      "PR AUC: 0.3683301949583306\n",
      "AUC: 0.5786363636363636\n",
      "PR AUC: 0.40793424355170693\n",
      "AUC: 0.5786363636363636\n",
      "PR AUC: 0.40865533586069724\n",
      "Mean AUC: 0.7432878442741768\n",
      "Mean PR AUC: 0.2671913880317706\n",
      "[[440   0]\n",
      " [  5   0]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores = []\n",
    "pr_auc = []\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "annotated = pd.read_csv(\"All_annotated_data_exclusion_LO_recodedd.csv\")\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_exclusion/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_exclusion/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # pipeline BOW, tfidf and RF\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', RandomForestClassifier(n_estimators=5000, max_depth=3,max_features='sqrt', random_state=0, class_weight=\"balanced\")),\n",
    "                                                 ])\n",
    "    ## Fit the model to the training set\n",
    "    text_clf = text_clf.fit(train['text'], train['pred_class'])\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = text_clf.predict_proba(test['text'])\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(test['true_class'], preds[:,1], pos_label=1)\n",
    "    auc_scores = auc_scores + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(test['true_class'],  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    print(\"PR AUC: \" + str(lr_auc))\n",
    "    \n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores)))\n",
    "print(\"Mean PR AUC: \" + str(np.mean(pr_auc)))\n",
    "\n",
    "confusion = confusion_matrix(test['true_class'],text_clf.predict(test['text']) )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"exclusion_pr_auc_tfidf.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"exclusion_auc_tfidf.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Local Word2Vec RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6158730158730158\n",
      "PR AUC: 0.015636972002978325\n",
      "AUC: 0.5004545454545455\n",
      "PR AUC: 0.20920039050125813\n",
      "AUC: 0.7024943310657596\n",
      "PR AUC: 0.0556115056729728\n",
      "AUC: 0.781489841986456\n",
      "PR AUC: 0.12246772996896844\n",
      "AUC: 0.5148648648648648\n",
      "PR AUC: 0.21570140836014381\n",
      "AUC: 0.902262443438914\n",
      "PR AUC: 0.3011859379201029\n",
      "AUC: 0.511617312072893\n",
      "PR AUC: 0.2707537489071317\n",
      "AUC: 0.6308390022675736\n",
      "PR AUC: 0.17013016150947183\n",
      "AUC: 0.7040909090909091\n",
      "PR AUC: 0.24687552802729268\n",
      "AUC: 0.8710407239819005\n",
      "PR AUC: 0.0820941758796296\n",
      "AUC: 0.650113895216401\n",
      "PR AUC: 0.056536453122667814\n",
      "AUC: 0.5682539682539682\n",
      "PR AUC: 0.21368109812017871\n",
      "AUC: 0.7325791855203619\n",
      "PR AUC: 0.02629850934361664\n",
      "AUC: 0.8824324324324324\n",
      "PR AUC: 0.12620695765058096\n",
      "AUC: 0.7054545454545454\n",
      "PR AUC: 0.03544204648296536\n",
      "AUC: 0.6988713318284425\n",
      "PR AUC: 0.024945542386272477\n",
      "AUC: 0.8439909297052155\n",
      "PR AUC: 0.3038578927841468\n",
      "AUC: 0.4444444444444444\n",
      "PR AUC: 0.011566276068572991\n",
      "AUC: 0.4303854875283447\n",
      "PR AUC: 0.012812532256351282\n",
      "AUC: 0.6825396825396826\n",
      "PR AUC: 0.2549989898631083\n",
      "AUC: 0.6158730158730159\n",
      "PR AUC: 0.08240669079795357\n",
      "AUC: 0.5723981900452488\n",
      "PR AUC: 0.01634729900291684\n",
      "AUC: 0.5782805429864253\n",
      "PR AUC: 0.01328236900800172\n",
      "AUC: 0.7822727272727273\n",
      "PR AUC: 0.07513757446594715\n",
      "AUC: 0.3149321266968326\n",
      "PR AUC: 0.007493052360777314\n",
      "Mean AUC: 0.6495139798357968\n",
      "Mean PR AUC: 0.11802683369856032\n",
      "[[442   0]\n",
      " [  5   0]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores = []\n",
    "pr_auc = []\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "annotated = pd.read_csv(\"All_annotated_data_round_1_LO_recodedd_20220209.csv\")\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "avg_embeddings = loadtxt('avg_embeddings.csv', delimiter=',')\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_exclusion/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_exclusion/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # attach train/test set to original df\n",
    "    train_id = train['speech_par_id']\n",
    "    test_id = test['speech_par_id']    \n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test_id), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train_id), 1, 0)\n",
    "    # get the embeddings to train/test set\n",
    "    train_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(train_id)].index.tolist()])\n",
    "    test_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(test_id)].index.tolist()])\n",
    "    ## Fit the model to the training set\n",
    "    text_clf = RandomForestClassifier(n_estimators=500, max_depth=3, random_state=0, max_features = \"sqrt\", class_weight=\"balanced\")\n",
    "    text_clf.fit(train_vec,  dat[dat['train']==1].pred_class) # use the original df to match the order\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = text_clf.predict_proba(test_vec)\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores = auc_scores + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    print(\"PR AUC: \" + str(lr_auc))\n",
    "    \n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores)))\n",
    "print(\"Mean PR AUC: \" + str(np.mean(pr_auc)))\n",
    "\n",
    "confusion = confusion_matrix(dat[dat['test']==1].pred_class,text_clf.predict(test_vec) )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"exclusion_pr_auc_w2v.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"exclusion_auc_w2v.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8656036446469249\n",
      "PR AUC: 0.2960334610330724\n",
      "[[439   0]\n",
      " [  4   1]]\n",
      "AUC: 0.9418181818181818\n",
      "PR AUC: 0.2677291121848854\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "AUC: 0.9481818181818182\n",
      "PR AUC: 0.09340525887774465\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "AUC: 0.8605922551252847\n",
      "PR AUC: 0.09020839567822889\n",
      "[[439   0]\n",
      " [  5   0]]\n",
      "AUC: 0.6354545454545455\n",
      "PR AUC: 0.022414069047207143\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "AUC: 0.8781818181818182\n",
      "PR AUC: 0.4883231881826092\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "AUC: 0.9445454545454546\n",
      "PR AUC: 0.24385519614435275\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "AUC: 0.8068181818181819\n",
      "PR AUC: 0.2458434198201073\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "AUC: 0.8405466970387243\n",
      "PR AUC: 0.3022948009621772\n",
      "[[439   0]\n",
      " [  4   1]]\n",
      "AUC: 0.8754545454545455\n",
      "PR AUC: 0.2632890333747567\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "AUC: 0.8781818181818182\n",
      "PR AUC: 0.14823392411338274\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "AUC: 0.8186363636363636\n",
      "PR AUC: 0.28241185263278284\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "AUC: 0.9858769931662871\n",
      "PR AUC: 0.5453366174055829\n",
      "[[439   0]\n",
      " [  5   0]]\n",
      "AUC: 0.9336363636363636\n",
      "PR AUC: 0.2844185259185259\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "AUC: 0.9131818181818182\n",
      "PR AUC: 0.17178165646728522\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "AUC: 0.8727272727272728\n",
      "PR AUC: 0.2694767424253088\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "AUC: 0.8318181818181818\n",
      "PR AUC: 0.10320117843718883\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "AUC: 0.7290909090909091\n",
      "PR AUC: 0.21909516966646853\n",
      "[[440   0]\n",
      " [  4   1]]\n",
      "AUC: 0.9195454545454544\n",
      "PR AUC: 0.48774393781181113\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "AUC: 0.9513636363636363\n",
      "PR AUC: 0.19177352458585262\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "AUC: 0.9531818181818182\n",
      "PR AUC: 0.6439865689865689\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "AUC: 0.8613636363636363\n",
      "PR AUC: 0.08887939656000476\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "AUC: 0.9111617312072893\n",
      "PR AUC: 0.6389776616825796\n",
      "[[439   0]\n",
      " [  5   0]]\n",
      "AUC: 0.855\n",
      "PR AUC: 0.5201301281178531\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "AUC: 0.8763636363636363\n",
      "PR AUC: 0.2803222173700632\n",
      "[[440   0]\n",
      " [  5   0]]\n",
      "Mean AUC: 0.8755330710291984\n",
      "Mean PR AUC: 0.287566601499456\n"
     ]
    }
   ],
   "source": [
    "auc_scores = []\n",
    "pr_auc = []\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "annotated = pd.read_csv(\"All_annotated_data_exclusion_LO_recodedd.csv\")\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "avg_embeddings = loadtxt('avg_embeddings_pretrained.csv', delimiter=',')\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_exclusion/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_exclusion/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "# attach train/test set to original df\n",
    "    train_id = train['speech_par_id']\n",
    "    test_id = test['speech_par_id']    \n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test_id), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train_id), 1, 0)\n",
    "    # get the embeddings to train/test set\n",
    "    train_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(train_id)].index.tolist()])\n",
    "    test_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(test_id)].index.tolist()])\n",
    "    ## Fit the model to the training set\n",
    "    text_clf = RandomForestClassifier(n_estimators=500, max_depth=3, random_state=0, max_features = \"sqrt\", class_weight=\"balanced\")\n",
    "    text_clf.fit(train_vec,  dat[dat['train']==1].pred_class) # use the original df to match the order\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = text_clf.predict_proba(test_vec)\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores = auc_scores + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    print(\"PR AUC: \" + str(lr_auc))\n",
    "    confusion = confusion_matrix(dat[dat['test']==1].pred_class,text_clf.predict(test_vec) )\n",
    "    print(confusion)\n",
    "    \n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores)))\n",
    "print(\"Mean PR AUC: \" + str(np.mean(pr_auc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"exclusion_pr_auc_pretrained.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"exclusion_auc_pretrained.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.938496583143508\n",
      "Accuracy: 0.9887387387387387\n",
      "PR_AUC: 0.2767286518195985\n",
      "AUC: 0.9409090909090909\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.3611857497955896\n",
      "AUC: 0.9268181818181818\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.0911339289379089\n",
      "AUC: 0.7881548974943052\n",
      "Accuracy: 0.9887387387387387\n",
      "PR_AUC: 0.1206855183232976\n",
      "AUC: 0.5772727272727273\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.01859175160330928\n",
      "AUC: 0.7895454545454546\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.2739100618618519\n",
      "AUC: 0.9804545454545455\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.27795006747638323\n",
      "AUC: 0.6277272727272727\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.22082433780158728\n",
      "AUC: 0.7111617312072893\n",
      "Accuracy: 0.990990990990991\n",
      "PR_AUC: 0.2267144556808242\n",
      "AUC: 0.8472727272727273\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.261560525600586\n",
      "AUC: 0.865909090909091\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.08638513086415708\n",
      "AUC: 0.8340909090909092\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.321562306053324\n",
      "AUC: 0.9722095671981776\n",
      "Accuracy: 0.9887387387387387\n",
      "PR_AUC: 0.44301371584390453\n",
      "AUC: 0.9381818181818181\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.3907949827169001\n",
      "AUC: 0.9804545454545455\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.38068495514147693\n",
      "AUC: 0.8286363636363636\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.23410770205110076\n",
      "AUC: 0.8377272727272727\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.34018347453078357\n",
      "AUC: 0.6672727272727274\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.22047226542459186\n",
      "AUC: 0.8572727272727273\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.7578341566970219\n",
      "AUC: 0.9786363636363636\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.46037793905440966\n",
      "AUC: 0.9754545454545455\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.2903309555941134\n",
      "AUC: 0.7922727272727272\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.4199804475482667\n",
      "AUC: 0.9535307517084283\n",
      "Accuracy: 0.9887387387387387\n",
      "PR_AUC: 0.392621525125213\n",
      "AUC: 0.8954545454545454\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.4722487062439267\n",
      "AUC: 0.8545454545454545\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.15390740922531165\n",
      "Mean AUC: 0.854378504866432\n",
      "Mean Accuracy: 0.988849073792894\n",
      "[[440   0]\n",
      " [  5   0]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores_d2v = []\n",
    "pr_auc = []\n",
    "accuracy_scores_d2v = []\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "\n",
    "model = Word2Vec.load(\"doc2vec_wordvecs.model\") \n",
    "annotated = pd.read_csv(\"All_annotated_data_exclusion_LO_recodedd.csv\")\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_exclusion/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_exclusion/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # merge test and train sets back onto the total df\n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test[\"speech_par_id\"]), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train[\"speech_par_id\"]), 1, 0)\n",
    "    # so that teh order of rows are the same with the embeddings\n",
    "    test_set = np.asarray([model.docvecs[i] for i in dat[dat['test'] == 1].index.tolist()])\n",
    "    train_set = np.asarray([model.docvecs[i] for i in dat[dat['train']== 1].index.tolist()])\n",
    "    ## Initialize a random forest classifier\n",
    "    gbc = RandomForestClassifier(n_estimators=500, max_depth=3, random_state=0, max_features = \"sqrt\", class_weight=\"balanced\")\n",
    "    ## Fit the model to the training set\n",
    "    gbc.fit(train_set, dat[dat['train']==1].pred_class)\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = gbc.predict_proba(test_set)\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores_d2v = auc_scores_d2v + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    accuracy_d2v = metrics.accuracy_score(dat[dat['test']==1].pred_class, gbc.predict(test_set))\n",
    "    accuracy_scores_d2v = accuracy_scores_d2v + [accuracy_d2v]\n",
    "    print(\"Accuracy: \" + str(accuracy_d2v))\n",
    "    print(\"PR_AUC: \" + str(lr_auc))\n",
    "\n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores_d2v)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_d2v)))\n",
    "\n",
    "confusion = confusion_matrix(dat[dat['test']==1].pred_class, gbc.predict(test_set))\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"exclusion_pr_auc.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"exclusion_auc.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores_d2v:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC-AUC: 0.854378504866432\n",
      "Mean Accuracy: 0.988849073792894\n",
      "Mean PR_AUC: 0.2997516288406175\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean ROC-AUC: \" + str(np.mean(auc_scores_d2v)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_d2v)))\n",
    "print(\"Mean PR_AUC: \" + str(np.mean(pr_auc)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balanced RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9649202733485194\n",
      "Accuracy: 0.7522522522522522\n",
      "PR_AUC: 0.32055285341730355\n",
      "AUC: 0.9740909090909091\n",
      "Accuracy: 0.755056179775281\n",
      "PR_AUC: 0.39837662337662333\n",
      "AUC: 0.9627272727272728\n",
      "Accuracy: 0.7775280898876404\n",
      "PR_AUC: 0.12550003653550562\n",
      "AUC: 0.8719817767653759\n",
      "Accuracy: 0.7882882882882883\n",
      "PR_AUC: 0.36450211732303384\n",
      "AUC: 0.5800000000000001\n",
      "Accuracy: 0.849438202247191\n",
      "PR_AUC: 0.03661705313773907\n",
      "AUC: 0.7545454545454546\n",
      "Accuracy: 0.8\n",
      "PR_AUC: 0.33776102743365277\n",
      "AUC: 0.9854545454545455\n",
      "Accuracy: 0.7752808988764045\n",
      "PR_AUC: 0.2667919799498747\n",
      "AUC: 0.6477272727272727\n",
      "Accuracy: 0.8651685393258427\n",
      "PR_AUC: 0.2443677860595389\n",
      "AUC: 0.7865603644646926\n",
      "Accuracy: 0.7905405405405406\n",
      "PR_AUC: 0.23932065401071553\n",
      "AUC: 0.7993181818181818\n",
      "Accuracy: 0.7775280898876404\n",
      "PR_AUC: 0.3053825035670955\n",
      "AUC: 0.9618181818181818\n",
      "Accuracy: 0.8247191011235955\n",
      "PR_AUC: 0.34073605679237623\n",
      "AUC: 0.7879545454545456\n",
      "Accuracy: 0.8134831460674158\n",
      "PR_AUC: 0.3725020951780573\n",
      "AUC: 0.9904328018223235\n",
      "Accuracy: 0.7612612612612613\n",
      "PR_AUC: 0.5888157894736842\n",
      "AUC: 0.9586363636363635\n",
      "Accuracy: 0.802247191011236\n",
      "PR_AUC: 0.6321687479111597\n",
      "AUC: 0.9820454545454546\n",
      "Accuracy: 0.797752808988764\n",
      "PR_AUC: 0.6675090504037873\n",
      "AUC: 0.8865909090909091\n",
      "Accuracy: 0.802247191011236\n",
      "PR_AUC: 0.22231067751716294\n",
      "AUC: 0.8186363636363637\n",
      "Accuracy: 0.7910112359550562\n",
      "PR_AUC: 0.42449155435475444\n",
      "AUC: 0.6361363636363637\n",
      "Accuracy: 0.8606741573033708\n",
      "PR_AUC: 0.23137728524592785\n",
      "AUC: 0.8354545454545456\n",
      "Accuracy: 0.7865168539325843\n",
      "PR_AUC: 0.7096184118911391\n",
      "AUC: 0.9784090909090909\n",
      "Accuracy: 0.7258426966292135\n",
      "PR_AUC: 0.4107104832104832\n",
      "AUC: 0.978409090909091\n",
      "Accuracy: 0.7303370786516854\n",
      "PR_AUC: 0.42286052009456265\n",
      "AUC: 0.8515909090909092\n",
      "Accuracy: 0.7887640449438202\n",
      "PR_AUC: 0.3468376408983423\n",
      "AUC: 0.9735763097949887\n",
      "Accuracy: 0.7342342342342343\n",
      "PR_AUC: 0.4239250194250194\n",
      "AUC: 0.8918181818181818\n",
      "Accuracy: 0.7955056179775281\n",
      "PR_AUC: 0.6143638412869182\n",
      "AUC: 0.8277272727272729\n",
      "Accuracy: 0.8426966292134831\n",
      "PR_AUC: 0.19109399627398338\n",
      "Mean AUC: 0.8674624974114724\n",
      "Mean Accuracy: 0.7915349731754227\n"
     ]
    }
   ],
   "source": [
    "auc_scores_balanced = []\n",
    "pr_auc_balanced = []\n",
    "accuracy_scores_balanced = []\n",
    "\n",
    "np.random.seed(234) \n",
    "random.seed(234)\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "# dat['speech_par_id'] = dat['Speech_id'].astype(str) + \"_\" + dat['par_id'].astype(str)\n",
    "# X = np.asarray([model.docvecs[i] for i in label_dat.index.tolist()])\n",
    "# Y = np.asarray(label_dat['label'].tolist(), dtype=\"int\")\n",
    "model = Word2Vec.load(\"doc2vec_wordvecs.model\") \n",
    "annotated = pd.read_csv(\"All_annotated_data_exclusion_LO_recodedd.csv\")\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_exclusion/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_exclusion/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # merge test and train sets back onto the total df\n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test[\"speech_par_id\"]), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train[\"speech_par_id\"]), 1, 0)\n",
    "    # so that teh order of rows are the same with the embeddings\n",
    "    test_set = np.asarray([model.docvecs[i] for i in dat[dat['test'] == 1].index.tolist()])\n",
    "    train_set = np.asarray([model.docvecs[i] for i in dat[dat['train']== 1].index.tolist()])\n",
    "    ## Initialize a random forest classifier\n",
    "    brfc = BalancedRandomForestClassifier(n_estimators=5000, max_depth=10, random_state=0, max_features = \"sqrt\")\n",
    "    ## Fit the model to the training set\n",
    "    brfc.fit(train_set, dat[dat['train']==1].pred_class)\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = brfc.predict_proba(test_set)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores_balanced = auc_scores_balanced + [metrics.auc(fpr, tpr)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc_balanced = pr_auc_balanced + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr, tpr)))\n",
    "    accuracy_balanced = metrics.accuracy_score(dat[dat['test']==1].pred_class, brfc.predict(test_set))\n",
    "    accuracy_scores_balanced = accuracy_scores_balanced + [accuracy_balanced]\n",
    "    print(\"Accuracy: \" + str(accuracy_balanced))\n",
    "    print(\"PR_AUC: \" + str(lr_auc))\n",
    "\n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores_balanced)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"exclusion_pr_auc_balanced.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc_balanced:\n",
    "        csvwriter.writerow([row])\n",
    "    \n",
    "with open(\"exclusion_auc_balanced.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores_balanced:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC-AUC: 0.8674624974114724\n",
      "Mean Accuracy: 0.7915349731754227\n",
      "Mean PR_AUC: 0.3695397521907376\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean ROC-AUC: \" + str(np.mean(auc_scores_balanced)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_balanced)))\n",
    "print(\"Mean PR_AUC: \" + str(np.mean(pr_auc_balanced)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
