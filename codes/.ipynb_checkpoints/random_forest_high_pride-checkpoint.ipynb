{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import random \n",
    "import os\n",
    "import csv\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf RF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_scores = []\n",
    "pr_auc = []\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "annotated = pd.read_csv(\"All_annotated_data_exclusion_LO_recodedd.csv\")\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_high_pride/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_high_pride/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # pipeline BOW, tfidf and RF\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', RandomForestClassifier(n_estimators=500, max_depth=3,max_features='sqrt', random_state=0,class_weight=\"balanced\")),\n",
    "                        ])\n",
    "    ## Fit the model to the training set\n",
    "    text_clf = text_clf.fit(train['text'], train['pred_class'])\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = text_clf.predict_proba(test['text'])\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(test['true_class'], preds[:,1], pos_label=1)\n",
    "    auc_scores = auc_scores + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(test['true_class'],  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    print(\"PR AUC: \" + str(lr_auc))\n",
    "    \n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores)))\n",
    "print(\"Mean PR AUC: \" + str(np.mean(pr_auc)))\n",
    "\n",
    "confusion = confusion_matrix(test['true_class'],text_clf.predict(test['text']) )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"high_pride_pr_auc_tfidf.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"high_pride_auc_tfidf.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7762988407041649\n",
      "Accuracy: 0.9078651685393259\n",
      "PR_AUC: 0.3284236937986967\n",
      "AUC: 0.737655646200086\n",
      "Accuracy: 0.9078651685393259\n",
      "PR_AUC: 0.32778078494025126\n",
      "AUC: 0.8115786460569628\n",
      "Accuracy: 0.9123595505617977\n",
      "PR_AUC: 0.31321313789989236\n",
      "AUC: 0.7523257478173752\n",
      "Accuracy: 0.8808988764044944\n",
      "PR_AUC: 0.20165313726440898\n",
      "AUC: 0.7974094747388006\n",
      "Accuracy: 0.9056179775280899\n",
      "PR_AUC: 0.285555874824839\n",
      "AUC: 0.8130559540889527\n",
      "Accuracy: 0.9099099099099099\n",
      "PR_AUC: 0.3724817284873316\n",
      "AUC: 0.7287820237584084\n",
      "Accuracy: 0.9056179775280899\n",
      "PR_AUC: 0.22774335172755625\n",
      "AUC: 0.755689136968656\n",
      "Accuracy: 0.9011235955056179\n",
      "PR_AUC: 0.2284835341128127\n",
      "AUC: 0.7840274796049806\n",
      "Accuracy: 0.8831460674157303\n",
      "PR_AUC: 0.20866919803414807\n",
      "AUC: 0.8068555889509088\n",
      "Accuracy: 0.8921348314606742\n",
      "PR_AUC: 0.2800463428579879\n",
      "AUC: 0.7043795620437956\n",
      "Accuracy: 0.9078651685393259\n",
      "PR_AUC: 0.30219831358264604\n",
      "AUC: 0.8262487476742523\n",
      "Accuracy: 0.9168539325842696\n",
      "PR_AUC: 0.3954022476457173\n",
      "AUC: 0.7789466151424074\n",
      "Accuracy: 0.903370786516854\n",
      "PR_AUC: 0.29878490210033065\n",
      "AUC: 0.7878766140602581\n",
      "Accuracy: 0.918918918918919\n",
      "PR_AUC: 0.29966434173504475\n",
      "AUC: 0.8454987834549879\n",
      "Accuracy: 0.9191011235955057\n",
      "PR_AUC: 0.4279025774101621\n",
      "AUC: 0.7974094747388005\n",
      "Accuracy: 0.9101123595505618\n",
      "PR_AUC: 0.32943546550654196\n",
      "AUC: 0.810832137733142\n",
      "Accuracy: 0.9121621621621622\n",
      "PR_AUC: 0.2860279824073712\n",
      "AUC: 0.7856733934449692\n",
      "Accuracy: 0.9123595505617977\n",
      "PR_AUC: 0.25376282279971346\n",
      "AUC: 0.7910405037927579\n",
      "Accuracy: 0.8966292134831461\n",
      "PR_AUC: 0.24348047846672755\n",
      "AUC: 0.7468155145269787\n",
      "Accuracy: 0.9078651685393259\n",
      "PR_AUC: 0.2502279915201003\n",
      "AUC: 0.8050665521683125\n",
      "Accuracy: 0.9078651685393259\n",
      "PR_AUC: 0.2540053913944465\n",
      "AUC: 0.8195935308429941\n",
      "Accuracy: 0.9101123595505618\n",
      "PR_AUC: 0.32986956393289385\n",
      "AUC: 0.830184628595964\n",
      "Accuracy: 0.9123595505617977\n",
      "PR_AUC: 0.45300685988630657\n",
      "AUC: 0.7019368723098994\n",
      "Accuracy: 0.8873873873873874\n",
      "PR_AUC: 0.19719525841159302\n",
      "AUC: 0.8369829683698298\n",
      "Accuracy: 0.9123595505617977\n",
      "PR_AUC: 0.3618995150654552\n",
      "Mean AUC: 0.7852865775115458\n",
      "Mean Accuracy: 0.9056744609778319\n",
      "[[392  19]\n",
      " [ 20  14]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores_d2v = []\n",
    "pr_auc = []\n",
    "accuracy_scores_d2v = []\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "# dat['speech_par_id'] = dat['Speech_id'].astype(str) + \"_\" + dat['par_id'].astype(str)\n",
    "# X = np.asarray([model.docvecs[i] for i in label_dat.index.tolist()])\n",
    "# Y = np.asarray(label_dat['label'].tolist(), dtype=\"int\")\n",
    "model = Word2Vec.load(\"doc2vec_wordvecs.model\") \n",
    "annotated = pd.read_csv(\"All_annotated_data_round_1_high_pride.csv\")\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_high_pride/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_high_pride/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # merge test and train sets back onto the total df\n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test[\"speech_par_id\"]), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train[\"speech_par_id\"]), 1, 0)\n",
    "    # so that teh order of rows are the same with the embeddings\n",
    "    test_set = np.asarray([model.docvecs[i] for i in dat[dat['test'] == 1].index.tolist()])\n",
    "    train_set = np.asarray([model.docvecs[i] for i in dat[dat['train']== 1].index.tolist()])\n",
    "    ## Initialize a random forest classifier\n",
    "    gbc = RandomForestClassifier(n_estimators=500, max_depth=3, random_state=0, max_features = \"sqrt\", class_weight=\"balanced\")\n",
    "    ## Fit the model to the training set\n",
    "    gbc.fit(train_set, dat[dat['train']==1].pred_class)\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = gbc.predict_proba(test_set)\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores_d2v = auc_scores_d2v + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    accuracy_d2v = metrics.accuracy_score(dat[dat['test']==1].pred_class, gbc.predict(test_set))\n",
    "    accuracy_scores_d2v = accuracy_scores_d2v + [accuracy_d2v]\n",
    "    print(\"Accuracy: \" + str(accuracy_d2v))\n",
    "    print(\"PR_AUC: \" + str(lr_auc))\n",
    "\n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores_d2v)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_d2v)))\n",
    "\n",
    "confusion = confusion_matrix(dat[dat['test']==1].pred_class, gbc.predict(test_set))\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"high_pride_pr_auc.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"high_pride_auc.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores_d2v:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC-AUC: 0.7852865775115458\n",
      "Mean Accuracy: 0.9056744609778319\n",
      "Mean PR_AUC: 0.298276579832519\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean ROC-AUC: \" + str(np.mean(auc_scores_d2v)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_d2v)))\n",
    "print(\"Mean PR_AUC: \" + str(np.mean(pr_auc)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balanced RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7802347216258767\n",
      "Accuracy: 0.7595505617977528\n",
      "PR_AUC: 0.31552716971681033\n",
      "AUC: 0.7395162444539859\n",
      "Accuracy: 0.7213483146067415\n",
      "PR_AUC: 0.33619873789132804\n",
      "AUC: 0.8267496779733792\n",
      "Accuracy: 0.7348314606741573\n",
      "PR_AUC: 0.33991834431150103\n",
      "AUC: 0.7483898668956633\n",
      "Accuracy: 0.7146067415730337\n",
      "PR_AUC: 0.21691886402572802\n",
      "AUC: 0.8233862888220982\n",
      "Accuracy: 0.6898876404494382\n",
      "PR_AUC: 0.3461727773346006\n",
      "AUC: 0.8092539454806312\n",
      "Accuracy: 0.6959459459459459\n",
      "PR_AUC: 0.36415952182015415\n",
      "AUC: 0.7544725919564907\n",
      "Accuracy: 0.7168539325842697\n",
      "PR_AUC: 0.25761118516842935\n",
      "AUC: 0.7760841562902534\n",
      "Accuracy: 0.7101123595505618\n",
      "PR_AUC: 0.291597257541525\n",
      "AUC: 0.8029197080291971\n",
      "Accuracy: 0.7393258426966293\n",
      "PR_AUC: 0.2262410586027561\n",
      "AUC: 0.7983397738657507\n",
      "Accuracy: 0.7280898876404495\n",
      "PR_AUC: 0.32649325066788304\n",
      "AUC: 0.7233433519393159\n",
      "Accuracy: 0.7528089887640449\n",
      "PR_AUC: 0.32157362054134886\n",
      "AUC: 0.8491484184914843\n",
      "Accuracy: 0.755056179775281\n",
      "PR_AUC: 0.40110430385218004\n",
      "AUC: 0.7917561185057965\n",
      "Accuracy: 0.7370786516853932\n",
      "PR_AUC: 0.3361903968176304\n",
      "AUC: 0.802725968436155\n",
      "Accuracy: 0.7319819819819819\n",
      "PR_AUC: 0.3439204577188214\n",
      "AUC: 0.8598110777157578\n",
      "Accuracy: 0.7325842696629213\n",
      "PR_AUC: 0.4150492694867497\n",
      "AUC: 0.797194790324889\n",
      "Accuracy: 0.7370786516853932\n",
      "PR_AUC: 0.34608816024511296\n",
      "AUC: 0.8289813486370158\n",
      "Accuracy: 0.7162162162162162\n",
      "PR_AUC: 0.34453634845179215\n",
      "AUC: 0.788178044940604\n",
      "Accuracy: 0.6651685393258427\n",
      "PR_AUC: 0.28404795664633603\n",
      "AUC: 0.7988407041648776\n",
      "Accuracy: 0.748314606741573\n",
      "PR_AUC: 0.27575579516719007\n",
      "AUC: 0.7459567768713325\n",
      "Accuracy: 0.7348314606741573\n",
      "PR_AUC: 0.24746648348068867\n",
      "AUC: 0.8287533991698869\n",
      "Accuracy: 0.7348314606741573\n",
      "PR_AUC: 0.30071192481425146\n",
      "AUC: 0.8371260913124374\n",
      "Accuracy: 0.7303370786516854\n",
      "PR_AUC: 0.33227486535250733\n",
      "AUC: 0.8429225704880493\n",
      "Accuracy: 0.7078651685393258\n",
      "PR_AUC: 0.45529429100726265\n",
      "AUC: 0.7091104734576757\n",
      "Accuracy: 0.7184684684684685\n",
      "PR_AUC: 0.24011268107476003\n",
      "AUC: 0.8364820380707028\n",
      "Accuracy: 0.7168539325842697\n",
      "PR_AUC: 0.35340933179866774\n",
      "Mean AUC: 0.7959871259167722\n",
      "Mean Accuracy: 0.7252011337179876\n"
     ]
    }
   ],
   "source": [
    "auc_scores_balanced = []\n",
    "pr_auc_balanced = []\n",
    "accuracy_scores_balanced = []\n",
    "\n",
    "np.random.seed(234) \n",
    "random.seed(234)\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "# X = np.asarray([model.docvecs[i] for i in label_dat.index.tolist()])\n",
    "# Y = np.asarray(label_dat['label'].tolist(), dtype=\"int\")\n",
    "model = Word2Vec.load(\"doc2vec_wordvecs.model\") \n",
    "annotated = pd.read_csv(\"All_annotated_data_round_1_high_pride.csv\")\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_high_pride/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_high_pride/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # merge test and train sets back onto the total df\n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test[\"speech_par_id\"]), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train[\"speech_par_id\"]), 1, 0)\n",
    "    # so that teh order of rows are the same with the embeddings\n",
    "    test_set = np.asarray([model.docvecs[i] for i in dat[dat['test'] == 1].index.tolist()])\n",
    "    train_set = np.asarray([model.docvecs[i] for i in dat[dat['train']== 1].index.tolist()])\n",
    "    ## Initialize a random forest classifier\n",
    "    brfc = BalancedRandomForestClassifier(n_estimators=5000, max_depth=10, random_state=0, max_features = \"sqrt\")\n",
    "    ## Fit the model to the training set\n",
    "    brfc.fit(train_set, dat[dat['train']==1].pred_class)\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = brfc.predict_proba(test_set)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores_balanced = auc_scores_balanced + [metrics.auc(fpr, tpr)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc_balanced = pr_auc_balanced + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr, tpr)))\n",
    "    accuracy_balanced = metrics.accuracy_score(dat[dat['test']==1].pred_class, brfc.predict(test_set))\n",
    "    accuracy_scores_balanced = accuracy_scores_balanced + [accuracy_balanced]\n",
    "    print(\"Accuracy: \" + str(accuracy_balanced))\n",
    "    print(\"PR_AUC: \" + str(lr_auc))\n",
    "\n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores_balanced)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"high_pride_pr_auc_balanced.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc_balanced:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"high_pride_auc_balanced.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores_balanced:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC-AUC: 0.7959871259167722\n",
      "Mean Accuracy: 0.7252011337179876\n",
      "Mean PR_AUC: 0.3207349621414406\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean ROC-AUC: \" + str(np.mean(auc_scores_balanced)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_balanced)))\n",
    "print(\"Mean PR_AUC: \" + str(np.mean(pr_auc_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
