{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import random \n",
    "import os\n",
    "import csv\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "## for word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "from numpy import loadtxt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9931972789115646\n",
      "PR AUC: 0.5674242424242424\n",
      "AUC: 0.8945578231292517\n",
      "PR AUC: 0.15192972141472239\n",
      "AUC: 0.9965986394557823\n",
      "PR AUC: 0.5041666666666667\n",
      "AUC: 0.8090909090909092\n",
      "PR AUC: 0.575508716251412\n",
      "AUC: 0.8497732426303855\n",
      "PR AUC: 0.08748819396240921\n",
      "AUC: 0.9642857142857143\n",
      "PR AUC: 0.19059068333832482\n",
      "AUC: 0.9778911564625851\n",
      "PR AUC: 0.19095441595441595\n",
      "AUC: 0.9778911564625851\n",
      "PR AUC: 0.4058531746031746\n",
      "AUC: 0.8441043083900227\n",
      "PR AUC: 0.2768570150075726\n",
      "AUC: 0.9937641723356009\n",
      "PR AUC: 0.5583333333333333\n",
      "AUC: 0.973922902494331\n",
      "PR AUC: 0.20149711399711398\n",
      "AUC: 0.9801587301587301\n",
      "PR AUC: 0.28397177419354835\n",
      "AUC: 0.9897727272727272\n",
      "PR AUC: 0.3092948717948718\n",
      "AUC: 0.764172335600907\n",
      "PR AUC: 0.088080600293397\n",
      "AUC: 0.8180272108843537\n",
      "PR AUC: 0.32822180208272617\n",
      "AUC: 0.884920634920635\n",
      "PR AUC: 0.2718755787963547\n",
      "AUC: 0.8477272727272727\n",
      "PR AUC: 0.07983657473672641\n",
      "AUC: 0.9909297052154196\n",
      "PR AUC: 0.31117424242424246\n",
      "AUC: 0.9931972789115646\n",
      "PR AUC: 0.5354166666666667\n",
      "AUC: 0.8985260770975056\n",
      "PR AUC: 0.23332072797511536\n",
      "AUC: 0.8747165532879819\n",
      "PR AUC: 0.07301393761439903\n",
      "AUC: 0.9835600907029478\n",
      "PR AUC: 0.42836791831357046\n",
      "AUC: 0.992063492063492\n",
      "PR AUC: 0.3238095238095238\n",
      "AUC: 0.9852272727272727\n",
      "PR AUC: 0.23803104575163397\n",
      "AUC: 0.869047619047619\n",
      "PR AUC: 0.537111882370503\n",
      "Mean AUC: 0.9258849721706865\n",
      "Mean PR AUC: 0.3100852169510667\n",
      "[[441   0]\n",
      " [  3   1]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores = []\n",
    "pr_auc = []\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "# dat['speech_par_id'] = dat['Speech_id'].astype(str) + \"_\" + dat['par_id'].astype(str)\n",
    "# X = np.asarray([model.docvecs[i] for i in label_dat.index.tolist()])\n",
    "# Y = np.asarray(label_dat['label'].tolist(), dtype=\"int\")\n",
    "annotated1 = pd.read_csv(\"All_annotated_data_round_1_auth.csv\")\n",
    "annotated2 = pd.read_csv(\"All_annotated_data_round_2_auth.csv\")\n",
    "annotated = annotated1.append(annotated2)\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # pipeline BOW, tfidf and RF\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', RandomForestClassifier(n_estimators=5000, max_depth=3,max_features='sqrt', random_state=0, class_weight=\"balanced\")),\n",
    "                                                 ])\n",
    "    ## Fit the model to the training set\n",
    "    text_clf = text_clf.fit(train['text'], train['pred_class'])\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = text_clf.predict_proba(test['text'])\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(test['true_class'], preds[:,1], pos_label=1)\n",
    "    auc_scores = auc_scores + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(test['true_class'],  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    print(\"PR AUC: \" + str(lr_auc))\n",
    "    \n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores)))\n",
    "print(\"Mean PR AUC: \" + str(np.mean(pr_auc)))\n",
    "\n",
    "confusion = confusion_matrix(test['true_class'],text_clf.predict(test['text']) )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"auth_pr_auc_tfidf.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"auth_auc_tfidf.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "\n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "dat[\"text_clean\"] = dat[\"text\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=True, flg_lemm=True,lst_stopwords=lst_stopwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75978,)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[\"text_clean\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases1 = Phrases(map(lambda x: x.split(), dat[\"text_clean\"].tolist())) #bigram\n",
    "phrases2 = Phrases(phrases1[map(lambda x: x.split(), dat[\"text_clean\"].tolist())]) #trigram\n",
    "dat[\"phrased_text\"] = dat[\"text_clean\"].apply(lambda x: \" \".join(phrases2[phrases1[x.split()]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    start talk economi best get right point jimmi_...\n",
       "1    secret group hit_hardest mr_carter inflationar...\n",
       "2    elderli work_hard enjoy retir year expect surv...\n",
       "3    believ social_secur one nation vital commit se...\n",
       "4    contrast commit econom program reduc inflat pu...\n",
       "Name: phrased_text, dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['phrased_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['phrased_tokens'] = dat.apply(lambda row: nltk.word_tokenize(row['phrased_text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Word2Vec (averaged within paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit Word2Vec model\n",
    "nlp = gensim.models.word2vec.Word2Vec(dat['phrased_tokens'], size=150, window=10, min_count=10, negative=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model\n",
    "nlp.wv.most_similar('tax') \n",
    "\n",
    "# save model\n",
    "nlp.save(\"word2vec_wordvecs.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# start empty matrix that hosts embeddings for each para, averaged from all words in the para\n",
    "# number of para x length of embedding (150)\n",
    "avg_embeddings = np.zeros((len(dat['phrased_tokens']), 150))\n",
    "\n",
    "# load model\n",
    "nlp = Word2Vec.load(\"word2vec_wordvecs.model\") \n",
    "#iterate through each para (rows of the df)\n",
    "for index, row in dat.iterrows():\n",
    "    # each row is a paragraph of tokens\n",
    "    tokens = row[\"phrased_tokens\"]\n",
    "    # start an empty embedding matrix for all tokens in a para\n",
    "    # number of tokens x 150\n",
    "    tokens_embedding = np.zeros((len(tokens), 150))\n",
    "    # loop through each token to delete non embedded tokens\n",
    "    for i in range(0, len(tokens)):\n",
    "        try:\n",
    "            tokens_embedding[i] = nlp[tokens[i]] # fill the matrix with word embedding\n",
    "        except:\n",
    "            pass # leave as 0 if the token is not in the model\n",
    "    avg_embedding = np.average(tokens_embedding, axis = 0) # average within para\n",
    "    avg_embeddings[index] = avg_embedding # fill the main matrix\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_embeddings.shape\n",
    "savetxt('avg_embeddings.csv', avg_embeddings, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Word2Vec RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9903628117913832\n",
      "PR AUC: 0.6388392857142857\n",
      "AUC: 0.9489795918367346\n",
      "PR AUC: 0.07771938374686052\n",
      "AUC: 0.949546485260771\n",
      "PR AUC: 0.1594644083085864\n",
      "AUC: 0.990909090909091\n",
      "PR AUC: 0.48080357142857144\n",
      "AUC: 0.9087301587301587\n",
      "PR AUC: 0.32587926183382987\n",
      "AUC: 0.8667800453514739\n",
      "PR AUC: 0.05765155020653751\n",
      "AUC: 0.9302721088435375\n",
      "PR AUC: 0.3713685277821604\n",
      "AUC: 0.8843537414965986\n",
      "PR AUC: 0.06412006176042784\n",
      "AUC: 0.8168934240362812\n",
      "PR AUC: 0.3571861456409572\n",
      "AUC: 0.9648526077097506\n",
      "PR AUC: 0.16079801980964772\n",
      "AUC: 0.9098639455782314\n",
      "PR AUC: 0.5138514051429026\n",
      "AUC: 0.8985260770975056\n",
      "PR AUC: 0.039183049985077475\n",
      "AUC: 0.9761363636363636\n",
      "PR AUC: 0.15021653398201384\n",
      "AUC: 0.9178004535147393\n",
      "PR AUC: 0.10697286100684232\n",
      "AUC: 0.7522675736961452\n",
      "PR AUC: 0.02790658664850475\n",
      "AUC: 0.903061224489796\n",
      "PR AUC: 0.07837903929101751\n",
      "AUC: 0.9267045454545455\n",
      "PR AUC: 0.41942040396638586\n",
      "AUC: 0.9659863945578231\n",
      "PR AUC: 0.5451526357833073\n",
      "AUC: 0.9965986394557823\n",
      "PR AUC: 0.6889880952380952\n",
      "AUC: 0.8775510204081632\n",
      "PR AUC: 0.09025426191546683\n",
      "AUC: 0.7080498866213152\n",
      "PR AUC: 0.018991535851843717\n",
      "AUC: 0.9750566893424035\n",
      "PR AUC: 0.355771067168126\n",
      "AUC: 0.9671201814058957\n",
      "PR AUC: 0.1327707107642241\n",
      "AUC: 0.9886363636363636\n",
      "PR AUC: 0.4541437728937729\n",
      "AUC: 0.9331065759637189\n",
      "PR AUC: 0.32168949219254095\n",
      "Mean AUC: 0.9179258400329828\n",
      "Mean PR AUC: 0.26550086672247947\n",
      "[[436   5]\n",
      " [  3   1]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores = []\n",
    "pr_auc = []\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "\n",
    "annotated1 = pd.read_csv(\"All_annotated_data_round_1_auth.csv\")\n",
    "annotated2 = pd.read_csv(\"All_annotated_data_round_2_auth.csv\")\n",
    "annotated = annotated1.append(annotated2)\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "avg_embeddings = loadtxt('avg_embeddings.csv', delimiter=',')\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # attach train/test set to original df\n",
    "    train_id = train['speech_par_id']\n",
    "    test_id = test['speech_par_id']    \n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test_id), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train_id), 1, 0)\n",
    "    # get the embeddings to train/test set\n",
    "    train_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(train_id)].index.tolist()])\n",
    "    test_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(test_id)].index.tolist()])\n",
    "    ## Fit the model to the training set\n",
    "    text_clf = RandomForestClassifier(n_estimators=500, max_depth=3, random_state=0, max_features = \"sqrt\", class_weight=\"balanced\")\n",
    "    text_clf.fit(train_vec,  dat[dat['train']==1].pred_class) # use the original df to match the order\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = text_clf.predict_proba(test_vec)\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores = auc_scores + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    print(\"PR AUC: \" + str(lr_auc))\n",
    "    \n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores)))\n",
    "print(\"Mean PR AUC: \" + str(np.mean(pr_auc)))\n",
    "\n",
    "confusion = confusion_matrix(dat[dat['test']==1].pred_class,text_clf.predict(test_vec) )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"auth_pr_auc_w2v.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"auth_auc_w2v.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = gensim_api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# dat[\"text_clean_pretrain\"] = dat[\"text\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=False,lst_stopwords=lst_stopwords))\n",
    "# dat['text_pretrain_token'] = dat.apply(lambda row: nltk.word_tokenize(row['text_clean_pretrain']), axis=1)\n",
    "# # start empty matrix that hosts embeddings for each para, averaged from all words in the para\n",
    "# # number of para x length of embedding (150)\n",
    "# avg_embeddings = np.zeros((len(dat['text_pretrain_token']), 300))\n",
    "\n",
    "# #iterate through each para (rows of the df)\n",
    "# for index, row in dat.iterrows():\n",
    "#     # each row is a paragraph of tokens\n",
    "#     tokens = row[\"text_pretrain_token\"]\n",
    "#     # start an empty embedding matrix for all tokens in a para\n",
    "#     # number of tokens x 150\n",
    "#     tokens_embedding = np.zeros((len(tokens), 300))\n",
    "#     # loop through each token to delete non embedded tokens\n",
    "#     for i in range(0, len(tokens)):\n",
    "#         try:\n",
    "#             tokens_embedding[i] = nlp[tokens[i]] # fill the matrix with word embedding\n",
    "#         except:\n",
    "#             pass # leave as 0 if the token is not in the model\n",
    "#     avg_embedding = np.average(tokens_embedding, axis = 0) # average within para\n",
    "#     avg_embeddings[index] = avg_embedding # fill the main matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savetxt('avg_embeddings_pretrained.csv', avg_embeddings, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9841269841269841\n",
      "PR AUC: 0.2761257763975155\n",
      "AUC: 0.9546485260770975\n",
      "PR AUC: 0.13719918074756784\n",
      "AUC: 0.9926303854875282\n",
      "PR AUC: 0.528422619047619\n",
      "AUC: 0.9840909090909091\n",
      "PR AUC: 0.7057123655913978\n",
      "AUC: 0.9427437641723357\n",
      "PR AUC: 0.4257922535211267\n",
      "AUC: 0.9342403628117913\n",
      "PR AUC: 0.357935601425036\n",
      "AUC: 0.9081632653061225\n",
      "PR AUC: 0.2072977761485826\n",
      "AUC: 0.8849206349206349\n",
      "PR AUC: 0.36784782056054405\n",
      "AUC: 0.9580498866213152\n",
      "PR AUC: 0.503129076973255\n",
      "AUC: 0.9750566893424035\n",
      "PR AUC: 0.13950216450216452\n",
      "AUC: 0.9586167800453516\n",
      "PR AUC: 0.40967228252676013\n",
      "AUC: 0.977891156462585\n",
      "PR AUC: 0.2195487382987383\n",
      "AUC: 0.99375\n",
      "PR AUC: 0.5827380952380952\n",
      "AUC: 0.9240362811791383\n",
      "PR AUC: 0.5363298632241802\n",
      "AUC: 0.9365079365079365\n",
      "PR AUC: 0.11648044525883386\n",
      "AUC: 0.971655328798186\n",
      "PR AUC: 0.6298039215686275\n",
      "AUC: 0.952840909090909\n",
      "PR AUC: 0.17932349513231866\n",
      "AUC: 0.9858276643990929\n",
      "PR AUC: 0.5272916666666667\n",
      "AUC: 0.988095238095238\n",
      "PR AUC: 0.5739177489177489\n",
      "AUC: 0.8594104308390024\n",
      "PR AUC: 0.321359288861799\n",
      "AUC: 0.8526077097505669\n",
      "PR AUC: 0.037261541741789236\n",
      "AUC: 0.9903628117913833\n",
      "PR AUC: 0.3148989898989899\n",
      "AUC: 0.9841269841269841\n",
      "PR AUC: 0.5639675697865353\n",
      "AUC: 0.9965909090909091\n",
      "PR AUC: 0.5193452380952381\n",
      "AUC: 0.9603174603174603\n",
      "PR AUC: 0.3480807627034042\n",
      "Mean AUC: 0.9540523603380744\n",
      "Mean PR AUC: 0.38115937131338135\n",
      "[[439   2]\n",
      " [  3   1]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores = []\n",
    "pr_auc = []\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "annotated1 = pd.read_csv(\"All_annotated_data_round_1_auth.csv\")\n",
    "annotated2 = pd.read_csv(\"All_annotated_data_round_2_auth.csv\")\n",
    "annotated = annotated1.append(annotated2)\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "avg_embeddings = loadtxt('avg_embeddings_pretrained.csv', delimiter=',')\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # attach train/test set to original df\n",
    "    train_text = train['speech_par_id']\n",
    "    test_text = test['speech_par_id']    \n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test_id), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train_id), 1, 0)\n",
    "    # get the embeddings to train/test set\n",
    "    train_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(train_id)].index.tolist()])\n",
    "    test_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(test_id)].index.tolist()])\n",
    "    ## Fit the model to the training set\n",
    "    text_clf = RandomForestClassifier(n_estimators=500, max_depth=3, random_state=0, max_features = \"sqrt\", class_weight=\"balanced\")\n",
    "    text_clf.fit(train_vec,  dat[dat['train']==1].pred_class) # use the original df to match the order\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = text_clf.predict_proba(test_vec)\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores = auc_scores + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    print(\"PR AUC: \" + str(lr_auc))\n",
    "    \n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores)))\n",
    "print(\"Mean PR AUC: \" + str(np.mean(pr_auc)))\n",
    "\n",
    "confusion = confusion_matrix(dat[dat['test']==1].pred_class,text_clf.predict(test_vec) )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"auth_pr_auc_pretrained.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"auth_auc_pretrained.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8520408163265306\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.416724380541544\n",
      "AUC: 0.945578231292517\n",
      "Accuracy: 0.9865168539325843\n",
      "PR_AUC: 0.11628207124774949\n",
      "AUC: 0.9982993197278911\n",
      "Accuracy: 0.9977528089887641\n",
      "PR_AUC: 0.7666666666666666\n",
      "AUC: 0.9403409090909092\n",
      "Accuracy: 0.9932432432432432\n",
      "PR_AUC: 0.6852176358601592\n",
      "AUC: 0.9160997732426305\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.2910415517106079\n",
      "AUC: 0.9501133786848073\n",
      "Accuracy: 0.9865168539325843\n",
      "PR_AUC: 0.3275834242589562\n",
      "AUC: 0.8100907029478458\n",
      "Accuracy: 0.9910112359550561\n",
      "PR_AUC: 0.3067897987808168\n",
      "AUC: 0.782312925170068\n",
      "Accuracy: 0.9865168539325843\n",
      "PR_AUC: 0.0628975343945541\n",
      "AUC: 0.9336734693877551\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.28680396643783374\n",
      "AUC: 0.8412698412698414\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.5111293142050152\n",
      "AUC: 0.9801587301587302\n",
      "Accuracy: 0.9842696629213483\n",
      "PR_AUC: 0.30987903225806446\n",
      "AUC: 0.935374149659864\n",
      "Accuracy: 0.9910112359550561\n",
      "PR_AUC: 0.17162490899955907\n",
      "AUC: 1.0\n",
      "Accuracy: 0.9954954954954955\n",
      "PR_AUC: 1.0\n",
      "AUC: 0.721655328798186\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.5070848909356729\n",
      "AUC: 0.8860544217687075\n",
      "Accuracy: 0.9910112359550561\n",
      "PR_AUC: 0.2828107160822537\n",
      "AUC: 0.9614512471655329\n",
      "Accuracy: 0.9932584269662922\n",
      "PR_AUC: 0.5854943064182194\n",
      "AUC: 0.9500000000000001\n",
      "Accuracy: 0.9887387387387387\n",
      "PR_AUC: 0.4302483358934972\n",
      "AUC: 0.9387755102040817\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.38282519361466727\n",
      "AUC: 0.8945578231292517\n",
      "Accuracy: 0.9932584269662922\n",
      "PR_AUC: 0.524659626929674\n",
      "AUC: 0.7840136054421768\n",
      "Accuracy: 0.9820224719101124\n",
      "PR_AUC: 0.16213311238667766\n",
      "AUC: 0.7494331065759636\n",
      "Accuracy: 0.9775280898876404\n",
      "PR_AUC: 0.06789743878465952\n",
      "AUC: 0.9467120181405896\n",
      "Accuracy: 0.9842696629213483\n",
      "PR_AUC: 0.5311112065018315\n",
      "AUC: 0.9665532879818594\n",
      "Accuracy: 0.9865168539325843\n",
      "PR_AUC: 0.5459451462740936\n",
      "AUC: 0.9994318181818181\n",
      "Accuracy: 0.9932432432432432\n",
      "PR_AUC: 0.94375\n",
      "AUC: 0.7636054421768708\n",
      "Accuracy: 0.9932584269662922\n",
      "PR_AUC: 0.26357399986381874\n",
      "Mean AUC: 0.8979038342609771\n",
      "Mean Accuracy: 0.9891209636602895\n",
      "[[441   0]\n",
      " [  3   1]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores_d2v = []\n",
    "pr_auc = []\n",
    "accuracy_scores_d2v = []\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "# dat['speech_par_id'] = dat['Speech_id'].astype(str) + \"_\" + dat['par_id'].astype(str)\n",
    "# X = np.asarray([model.docvecs[i] for i in label_dat.index.tolist()])\n",
    "# Y = np.asarray(label_dat['label'].tolist(), dtype=\"int\")\n",
    "model = Word2Vec.load(\"doc2vec_wordvecs.model\") \n",
    "annotated1 = pd.read_csv(\"All_annotated_data_round_1_auth.csv\")\n",
    "annotated2 = pd.read_csv(\"All_annotated_data_round_2_auth.csv\")\n",
    "annotated = annotated1.append(annotated2)\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # merge test and train sets back onto the total df\n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test[\"speech_par_id\"]), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train[\"speech_par_id\"]), 1, 0)\n",
    "    # so that teh order of rows are the same with the embeddings\n",
    "    test_set = np.asarray([model.docvecs[i] for i in dat[dat['test'] == 1].index.tolist()])\n",
    "    train_set = np.asarray([model.docvecs[i] for i in dat[dat['train']== 1].index.tolist()])\n",
    "    ## Initialize a random forest classifier\n",
    "    gbc = RandomForestClassifier(n_estimators=500, max_depth=3, random_state=0, max_features = \"sqrt\", class_weight=\"balanced\")\n",
    "    ## Fit the model to the training set\n",
    "    gbc.fit(train_set, dat[dat['train']==1].pred_class)\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = gbc.predict_proba(test_set)\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores_d2v = auc_scores_d2v + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    accuracy_d2v = metrics.accuracy_score(dat[dat['test']==1].pred_class, gbc.predict(test_set))\n",
    "    accuracy_scores_d2v = accuracy_scores_d2v + [accuracy_d2v]\n",
    "    print(\"Accuracy: \" + str(accuracy_d2v))\n",
    "    print(\"PR_AUC: \" + str(lr_auc))\n",
    "\n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores_d2v)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_d2v)))\n",
    "\n",
    "confusion = confusion_matrix(dat[dat['test']==1].pred_class, gbc.predict(test_set))\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"auth_pr_auc.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"auth_auc.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores_d2v:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC-AUC: 0.8979038342609771\n",
      "Mean Accuracy: 0.9891209636602895\n",
      "Mean PR_AUC: 0.41920697036186366\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean ROC-AUC: \" + str(np.mean(auc_scores_d2v)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_d2v)))\n",
    "print(\"Mean PR_AUC: \" + str(np.mean(pr_auc)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balanced RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8820861678004536\n",
      "Accuracy: 0.9191011235955057\n",
      "PR_AUC: 0.4159594249378409\n",
      "AUC: 0.95578231292517\n",
      "Accuracy: 0.9325842696629213\n",
      "PR_AUC: 0.12815554959250913\n",
      "AUC: 0.9994331065759636\n",
      "Accuracy: 0.946067415730337\n",
      "PR_AUC: 0.94375\n",
      "AUC: 0.9357954545454545\n",
      "Accuracy: 0.9572072072072072\n",
      "PR_AUC: 0.757506262894194\n",
      "AUC: 0.9212018140589568\n",
      "Accuracy: 0.9258426966292135\n",
      "PR_AUC: 0.29208665044965354\n",
      "AUC: 0.9631519274376418\n",
      "Accuracy: 0.9370786516853933\n",
      "PR_AUC: 0.34020008115402856\n",
      "AUC: 0.7922335600907029\n",
      "Accuracy: 0.9191011235955057\n",
      "PR_AUC: 0.4898935174788532\n",
      "AUC: 0.757936507936508\n",
      "Accuracy: 0.9303370786516854\n",
      "PR_AUC: 0.09778652101382089\n",
      "AUC: 0.9257369614512472\n",
      "Accuracy: 0.9101123595505618\n",
      "PR_AUC: 0.3275864639236732\n",
      "AUC: 0.8815192743764173\n",
      "Accuracy: 0.9258426966292135\n",
      "PR_AUC: 0.4124296584099125\n",
      "AUC: 0.9784580498866214\n",
      "Accuracy: 0.9438202247191011\n",
      "PR_AUC: 0.28761052166224577\n",
      "AUC: 0.9512471655328797\n",
      "Accuracy: 0.9213483146067416\n",
      "PR_AUC: 0.17970954306899703\n",
      "AUC: 1.0\n",
      "Accuracy: 0.9234234234234234\n",
      "PR_AUC: 1.0\n",
      "AUC: 0.7392290249433107\n",
      "Accuracy: 0.9483146067415731\n",
      "PR_AUC: 0.5084462757673182\n",
      "AUC: 0.9036281179138321\n",
      "Accuracy: 0.9191011235955057\n",
      "PR_AUC: 0.29843105549306337\n",
      "AUC: 0.9688208616780045\n",
      "Accuracy: 0.9325842696629213\n",
      "PR_AUC: 0.40116341991341986\n",
      "AUC: 0.9590909090909091\n",
      "Accuracy: 0.9414414414414415\n",
      "PR_AUC: 0.440530303030303\n",
      "AUC: 0.9665532879818595\n",
      "Accuracy: 0.9550561797752809\n",
      "PR_AUC: 0.4061591279997033\n",
      "AUC: 0.8900226757369615\n",
      "Accuracy: 0.950561797752809\n",
      "PR_AUC: 0.5238582174610896\n",
      "AUC: 0.7933673469387755\n",
      "Accuracy: 0.9303370786516854\n",
      "PR_AUC: 0.3811397231899068\n",
      "AUC: 0.752267573696145\n",
      "Accuracy: 0.9213483146067416\n",
      "PR_AUC: 0.07407592008375256\n",
      "AUC: 0.9637188208616779\n",
      "Accuracy: 0.9303370786516854\n",
      "PR_AUC: 0.5446195652173913\n",
      "AUC: 0.9659863945578231\n",
      "Accuracy: 0.9303370786516854\n",
      "PR_AUC: 0.4416861181789431\n",
      "AUC: 0.9994318181818181\n",
      "Accuracy: 0.9234234234234234\n",
      "PR_AUC: 0.94375\n",
      "AUC: 0.816609977324263\n",
      "Accuracy: 0.9483146067415731\n",
      "PR_AUC: 0.2716338711918301\n",
      "Mean AUC: 0.9065323644609359\n",
      "Mean Accuracy: 0.9329209434153255\n"
     ]
    }
   ],
   "source": [
    "auc_scores_balanced = []\n",
    "pr_auc_balanced = []\n",
    "accuracy_scores_balanced = []\n",
    "\n",
    "np.random.seed(234) \n",
    "random.seed(234)\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "# dat['speech_par_id'] = dat['Speech_id'].astype(str) + \"_\" + dat['par_id'].astype(str)\n",
    "# X = np.asarray([model.docvecs[i] for i in label_dat.index.tolist()])\n",
    "# Y = np.asarray(label_dat['label'].tolist(), dtype=\"int\")\n",
    "model = Word2Vec.load(\"doc2vec_wordvecs.model\") \n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "annotated1 = pd.read_csv(\"All_annotated_data_round_1_auth.csv\")\n",
    "annotated2 = pd.read_csv(\"All_annotated_data_round_2_auth.csv\")\n",
    "annotated = annotated1.append(annotated2)\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # merge test and train sets back onto the total df\n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test[\"speech_par_id\"]), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train[\"speech_par_id\"]), 1, 0)\n",
    "    # so that teh order of rows are the same with the embeddings\n",
    "    test_set = np.asarray([model.docvecs[i] for i in dat[dat['test'] == 1].index.tolist()])\n",
    "    train_set = np.asarray([model.docvecs[i] for i in dat[dat['train']== 1].index.tolist()])\n",
    "    ## Initialize a random forest classifier\n",
    "    brfc = BalancedRandomForestClassifier(n_estimators=5000, max_depth=10, random_state=0, max_features = \"sqrt\")\n",
    "    ## Fit the model to the training set\n",
    "    brfc.fit(train_set, dat[dat['train']==1].pred_class)\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = brfc.predict_proba(test_set)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores_balanced = auc_scores_balanced + [metrics.auc(fpr, tpr)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc_balanced = pr_auc_balanced + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr, tpr)))\n",
    "    accuracy_balanced = metrics.accuracy_score(dat[dat['test']==1].pred_class, brfc.predict(test_set))\n",
    "    accuracy_scores_balanced = accuracy_scores_balanced + [accuracy_balanced]\n",
    "    print(\"Accuracy: \" + str(accuracy_balanced))\n",
    "    print(\"PR_AUC: \" + str(lr_auc))\n",
    "\n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores_balanced)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"auth_pr_auc_balanced.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc_balanced:\n",
    "        csvwriter.writerow([row])    \n",
    "\n",
    "with open(\"auth_auc_balanced.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores_balanced:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC-AUC: 0.9065323644609359\n",
      "Mean Accuracy: 0.9329209434153255\n",
      "Mean PR_AUC: 0.436326711684498\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean ROC-AUC: \" + str(np.mean(auc_scores_balanced)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_balanced)))\n",
    "print(\"Mean PR_AUC: \" + str(np.mean(pr_auc_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[421  20]\n",
      " [  3   1]]\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(dat[dat['test']==1].pred_class, brfc.predict(test_set))\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "dat = pd.read_csv('20220131_all_paragraphs_2020_added_missings_added.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Speech_id</th>\n",
       "      <th>text</th>\n",
       "      <th>party</th>\n",
       "      <th>term</th>\n",
       "      <th>comp</th>\n",
       "      <th>populist_old_keywords</th>\n",
       "      <th>par_id</th>\n",
       "      <th>speech_par_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49468</th>\n",
       "      <td>49469</td>\n",
       "      <td>3369</td>\n",
       "      <td>So, that was a strong economy that Bill Clinto...</td>\n",
       "      <td>rep</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>3369_11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 Speech_id  \\\n",
       "49468       49469      3369   \n",
       "\n",
       "                                                    text party    term   comp  \\\n",
       "49468  So, that was a strong economy that Bill Clinto...   rep  1996.0  False   \n",
       "\n",
       "      populist_old_keywords  par_id speech_par_id  \n",
       "49468                 False      11       3369_11  "
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[dat[\"speech_par_id\"] == \"3369_11\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "dat = pd.read_csv(\"20220131_all_paragraphs_2020_added_missings_added.csv\")\n",
    "pop = pd.read_csv(\"All_annotated_data_populism.csv\")\n",
    "low_pr = pd.read_csv(\"All_annotated_data_round_1_low_pride.csv\")\n",
    "high_pr = pd.read_csv(\"All_annotated_data_round_1_high_pride.csv\")\n",
    "auth = pd.read_csv(\"All_annotated_data_round_1_auth.csv\")\n",
    "exc = pd.read_csv(\"All_annotated_data_exclusion_LO_recodedd.csv\")\n",
    "inc = pd.read_csv(\"All_annotated_data_inclusion.csv\")\n",
    "\n",
    "dat = dat.merge(pop[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "dat = dat.rename({'pred_class': 'pop'}, axis=1)\n",
    "dat = dat.merge(low_pr[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "dat = dat.rename({'pred_class': 'low_pr'}, axis=1)\n",
    "dat = dat.merge(high_pr[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "dat = dat.rename({'pred_class': 'high_pr'}, axis=1)\n",
    "dat = dat.merge(auth[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "dat = dat.rename({'pred_class': 'auth'}, axis=1)\n",
    "dat = dat.merge(exc[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "dat = dat.rename({'pred_class': 'exc'}, axis=1)\n",
    "dat = dat.merge(inc[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "dat = dat.rename({'pred_class': 'inc'}, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat[dat['pop'] + dat['low_pr'] + dat['high_pr']+ dat[\"auth\"] +dat[\"exc\"] + dat[\"inc\"] >1].to_csv(\"multiframe_pars.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_pred = pd.read_csv(\"pop_Iteration_3_predictions10239_1e-05_5_4_1_missings_merged.csv\")\n",
    "auth_pred = pd.read_csv(\"auth_Iteration_4_predictions10239_1e-05_5_4_1_missings_merged.csv\")\n",
    "exc_pred = pd.read_csv(\"exclusion_Iteration_3_predictions10239_1e-05_5_4_1_missings_merged.csv\")\n",
    "inc_pred = pd.read_csv(\"inclusion_Iteration_3_predictions10239_1e-05_5_4_1_missings_merged.csv\")\n",
    "high_pr_pred = pd.read_csv(\"high_pride_Iteration_3_predictions10239_1e-05_5_4_1_missings_merged.csv\")\n",
    "low_pr_pred = pd.read_csv(\"low_pride_Iteration_3_predictions10239_1e-05_5_4_1_missings_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dat = pop_pred.merge(low_pr_pred[[\"speech_par_id\", \"Predictions_prob_1\"]], how = \"left\", on = \"speech_par_id\")\n",
    "pred_dat = pred_dat.rename({'Predictions_prob_1_x': 'pop'}, axis=1)\n",
    "pred_dat = pred_dat.rename({'Predictions_prob_1_y': 'low_pr'}, axis=1)\n",
    "pred_dat = pred_dat.merge(high_pr_pred[[\"speech_par_id\", \"Predictions_prob_1\"]], how = \"left\", on = \"speech_par_id\")\n",
    "pred_dat = pred_dat.rename({'Predictions_prob_1': 'high_pr'}, axis=1)\n",
    "pred_dat = pred_dat.merge(auth_pred[[\"speech_par_id\", \"Predictions_prob_1\"]], how = \"left\", on = \"speech_par_id\")\n",
    "pred_dat = pred_dat.rename({'Predictions_prob_1': 'auth'}, axis=1)\n",
    "pred_dat = pred_dat.merge(exc_pred[[\"speech_par_id\", \"Predictions_prob_1\"]], how = \"left\", on = \"speech_par_id\")\n",
    "pred_dat = pred_dat.rename({'Predictions_prob_1': 'exc'}, axis=1)\n",
    "pred_dat = pred_dat.merge(inc_pred[[\"speech_par_id\", \"Predictions_prob_1\"]], how = \"left\", on = \"speech_par_id\")\n",
    "pred_dat = pred_dat.rename({'Predictions_prob_1': 'inc'}, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>pop</th>\n",
       "      <th>Predictions_prob_0</th>\n",
       "      <th>text</th>\n",
       "      <th>speech_par_id</th>\n",
       "      <th>uncertainty</th>\n",
       "      <th>low_pr</th>\n",
       "      <th>high_pr</th>\n",
       "      <th>auth</th>\n",
       "      <th>exc</th>\n",
       "      <th>inc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.999835</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>The truth is, for twenty-six years in Washingt...</td>\n",
       "      <td>2008-09-28-remarks-detroit-michigan_10</td>\n",
       "      <td>0.999671</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.999835</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>And last minute conversions aren't going to hi...</td>\n",
       "      <td>2854_5</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.999834</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>My opponent's first reaction to this crisis on...</td>\n",
       "      <td>2008-09-18-espanola-new-mexico_4</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.999834</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>We saw an abandonment of the commitment to bet...</td>\n",
       "      <td>2382_1</td>\n",
       "      <td>0.999667</td>\n",
       "      <td>0.999762</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.997079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.999833</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>Now, I certainly don't fault Senator McCain fo...</td>\n",
       "      <td>2008-09-17-remarks-elko-nevada_10</td>\n",
       "      <td>0.999666</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       pop  Predictions_prob_0  \\\n",
       "0           1  0.999835            0.000165   \n",
       "1           2  0.999835            0.000165   \n",
       "2           3  0.999834            0.000166   \n",
       "3           4  0.999834            0.000166   \n",
       "4           5  0.999833            0.000167   \n",
       "\n",
       "                                                text  \\\n",
       "0  The truth is, for twenty-six years in Washingt...   \n",
       "1  And last minute conversions aren't going to hi...   \n",
       "2  My opponent's first reaction to this crisis on...   \n",
       "3  We saw an abandonment of the commitment to bet...   \n",
       "4  Now, I certainly don't fault Senator McCain fo...   \n",
       "\n",
       "                            speech_par_id  uncertainty    low_pr   high_pr  \\\n",
       "0  2008-09-28-remarks-detroit-michigan_10     0.999671  0.000284  0.000236   \n",
       "1                                  2854_5     0.999669  0.000207  0.000222   \n",
       "2        2008-09-18-espanola-new-mexico_4     0.999669  0.000207  0.000575   \n",
       "3                                  2382_1     0.999667  0.999762  0.000529   \n",
       "4       2008-09-17-remarks-elko-nevada_10     0.999666  0.000262  0.000208   \n",
       "\n",
       "       auth       exc       inc  \n",
       "0  0.000052  0.000080  0.000317  \n",
       "1  0.000056  0.000077  0.000428  \n",
       "2  0.000051  0.000081  0.000321  \n",
       "3  0.000050  0.000092  0.997079  \n",
       "4  0.000051  0.000081  0.000317  "
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
