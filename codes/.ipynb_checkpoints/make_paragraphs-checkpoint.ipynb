{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract paragraphs, combine short paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yuchenluo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3a08088fe17c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;31m#get meta data of the speech\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mcand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-speech.*\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mterm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"speech-(.*)-\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"speech-(.*).txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mpars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspeech\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "col_names = [\"text\",\"candidate\", \"term\", \"title\", \"comp\"]\n",
    "rows = []\n",
    "for root, dirs, files, in os.walk('/Users/yuchenluo/Desktop/Measure_RadicalDiscourse'):\n",
    "    for file in files:\n",
    "        if file.endswith('txt'):\n",
    "            speech = open(os.path.join(root, file), \"r\").read()\n",
    "            #speech = f.read()\n",
    "            #get meta data of the speech\n",
    "            cand = re.sub(\"-speech.*\", \"\", file)\n",
    "            term = re.search(\"speech-(.*)-\", file).group(1)[0:4]\n",
    "            title = re.search(\"speech-(.*).txt\", file).group(1)\n",
    "            pars = speech.split('\\n')\n",
    "            pars = [i for i in pars if i] #remove empty strings\n",
    "            pars = [re.sub(\" \\[[^()]*\\]\", \"\", par) for par in pars] #remove strings inside brackets\n",
    "            # loop through all paragraphs inside each speech \n",
    "            i = 0    \n",
    "            while i < (len(pars) - 1):\n",
    "                n_sent = len(sent_tokenize(pars[i]))\n",
    "                if n_sent >2:\n",
    "                    row = [pars[i], cand, term, title, False]\n",
    "                    rows.append(row)\n",
    "                    i += 1\n",
    "                elif i < len(pars) - 1:\n",
    "                    row = [pars[i] + \" \" + pars[i +1], cand, term, title, True]\n",
    "                    i += 2\n",
    "                    if len(sent_tokenize(row[0])) < 3 and i < len(pars) - 1:\n",
    "                        row_2 = [row[0] + \" \" + pars[i], cand, term, title, True]\n",
    "                        i += 1\n",
    "                        rows.append(row_2)\n",
    "                    else: \n",
    "                        rows.append(row)\n",
    "                else: \n",
    "                    row = [pars[i - 1] + \" \" + pars[i], cand, term, title, True]\n",
    "                    rows.pop()\n",
    "                    rows.append(row)\n",
    "                            \n",
    "                                \n",
    "                            \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rows[0:10]  #check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"paragraphs.csv\", 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)   \n",
    "    writer.writerow(col_names)\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat the process for Annenberg data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "annen_meta = pd.read_csv(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/annenberg_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "root = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/Annenberg-data\" \n",
    "col_names = [\"Speech_id\",\"text\",\"party\", \"term\", \"comp\"] \n",
    "rows = [] \n",
    "\n",
    "for file in os.listdir('/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/Annenberg-data'): \n",
    "    if file.endswith('txt'): \n",
    "        speech = open(os.path.join(root, file), \"r\").read() \n",
    "        #get meta data of the speech \n",
    "        id_speech = re.search('File_(.*).txt', file).group(1) \n",
    "        party = annen_meta[annen_meta['id_speech'] == float(id_speech)]['party'].iloc[0] \n",
    "        term = annen_meta[annen_meta['id_speech'] == float(id_speech)]['year'].iloc[0] \n",
    "        pars = speech.split('\\n\\n') \n",
    "        Type = annen_meta[annen_meta['id_speech'] == float(id_speech)]['type'].iloc[0] \n",
    "        if Type == 'Speeches': #only keep speeches \n",
    "        # the last paragraph is not part of body text\n",
    "            del pars[-1] \n",
    "            pars = [i for i in pars if i] #remove empty strings \n",
    "            pars = [re.sub(\" \\[[^()]*\\]\", \"\", par) for par in pars] #remove strings inside brackets \n",
    "            pars = [re.sub(\"\\t\", \"\",par) for par in pars]\n",
    "            pars = [re.sub(\"\\n\", \"\",par) for par in pars] # remove \\t and \\n\n",
    "            # loop through all paragraphs inside each speech  \n",
    "            i = 0     \n",
    "            while i < (len(pars) - 1): \n",
    "                n_sent = len(sent_tokenize(pars[i])) \n",
    "                if n_sent >2: \n",
    "                    row = [id_speech, pars[i], party, term, False] \n",
    "                    rows.append(row) \n",
    "                    i += 1 \n",
    "                elif i < len(pars) - 1: \n",
    "                    row = [id_speech, pars[i] + \" \" + pars[i +1], party, term, True] \n",
    "                    i += 2 \n",
    "                    if len(sent_tokenize(row[0])) < 3 and i < len(pars) - 1: \n",
    "                        row_2 = [id_speech, row[0] + \" \" + pars[i], party, term, True] \n",
    "                        i += 1 \n",
    "                        rows.append(row_2) \n",
    "                    else:  \n",
    "                        rows.append(row) \n",
    "                else:  \n",
    "                    row = [id_speech, pars[i - 1] + \" \" + pars[i], party, term, True] \n",
    "                    rows.pop() \n",
    "                    rows.append(row) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"paragraphs_annenberg.csv\", 'w') as csvfile: \n",
    "    writer = csv.writer(csvfile)    \n",
    "    writer.writerow(col_names) \n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_speech</th>\n",
       "      <th>type</th>\n",
       "      <th>year</th>\n",
       "      <th>party</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ads</td>\n",
       "      <td>1952</td>\n",
       "      <td>dem</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ads</td>\n",
       "      <td>1952</td>\n",
       "      <td>dem</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Ads</td>\n",
       "      <td>1952</td>\n",
       "      <td>dem</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Ads</td>\n",
       "      <td>1952</td>\n",
       "      <td>dem</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Ads</td>\n",
       "      <td>1952</td>\n",
       "      <td>dem</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>869</td>\n",
       "      <td>Ads</td>\n",
       "      <td>1996</td>\n",
       "      <td>rep</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>870</td>\n",
       "      <td>Ads</td>\n",
       "      <td>1996</td>\n",
       "      <td>rep</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>871</td>\n",
       "      <td>Ads</td>\n",
       "      <td>1996</td>\n",
       "      <td>rep</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>872</td>\n",
       "      <td>Ads</td>\n",
       "      <td>1996</td>\n",
       "      <td>rep</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>873</td>\n",
       "      <td>Ads</td>\n",
       "      <td>1996</td>\n",
       "      <td>rep</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>873 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_speech type  year party state\n",
       "0            1  Ads  1952   dem   NaN\n",
       "1            2  Ads  1952   dem   NaN\n",
       "2            3  Ads  1952   dem   NaN\n",
       "3            4  Ads  1952   dem   NaN\n",
       "4            5  Ads  1952   dem   NaN\n",
       "..         ...  ...   ...   ...   ...\n",
       "868        869  Ads  1996   rep   NaN\n",
       "869        870  Ads  1996   rep   NaN\n",
       "870        871  Ads  1996   rep   NaN\n",
       "871        872  Ads  1996   rep   NaN\n",
       "872        873  Ads  1996   rep   NaN\n",
       "\n",
       "[873 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annen_meta[annen_meta['type'] == 'Ads']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine UCSB data and Annenberg data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ucsb = pd.read_csv('paragraphs.csv')\n",
    "annenberg = pd.read_csv('paragraphs_annenberg.csv',skipinitialspace=True)\n",
    "\n",
    "# remove \\n and \\t in annenberg data \n",
    "annenberg['text'] = annenberg['text'].replace(r'\\n',' ', regex=True) \n",
    "annenberg['text'] = annenberg['text'].replace(r'\\t',' ', regex=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a party variable for ucsb data to match the annenberg data\n",
    "dems = ['obama', 'gore-jr', 'clinton','kerry']\n",
    "ucsb['party'] = ['dem' if x in dems else 'rep' for x in ucsb['candidate']] \n",
    "\n",
    "# match columns\n",
    "ucsb_matched = ucsb[['title', 'text', 'party', 'term', 'comp']]\n",
    "ucsb_matched.rename(columns = {'title':'Speech_id'}, inplace = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = annenberg.append(ucsb_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv('all_paragraphs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = all_data.sample(n = 1000)\n",
    "sample1 = sample[0:500]\n",
    "sample2 = sample[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1.to_csv('yuchen_sample.csv')\n",
    "sample2.to_csv('bart_sample.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
