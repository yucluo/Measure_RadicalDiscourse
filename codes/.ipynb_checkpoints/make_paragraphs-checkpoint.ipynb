{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract paragraphs, combine short paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yuchenluo/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import nltk1\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"text\",\"candidate\", \"term\", \"title\", \"comp\"]\n",
    "rows = []\n",
    "for root, dirs, files, in os.walk('/Users/yuchenluo/Desktop/Measure_RadicalDiscourse'):\n",
    "    for file in files:\n",
    "        if file.endswith('txt'):\n",
    "            speech = open(os.path.join(root, file), \"r\").read()\n",
    "            #speech = f.read()\n",
    "            #get meta data of the speech\n",
    "            cand = re.sub(\"-speech.*\", \"\", file)\n",
    "            term = re.search(\"speech-(.*)-\", file).group(1)[0:4]\n",
    "            title = re.search(\"speech-(.*).txt\", file).group(1)\n",
    "            pars = speech.split('\\n')\n",
    "            pars = [i for i in pars if i] #remove empty strings\n",
    "            pars = [re.sub(\" \\[[^()]*\\]\", \"\", par) for par in pars] #remove strings inside brackets\n",
    "            # loop through all paragraphs inside each speech \n",
    "            i = 0    \n",
    "            while i < (len(pars) - 1):\n",
    "                n_sent = len(tokenize.sent_tokenize(pars[i]))\n",
    "                if n_sent >2:\n",
    "                    row = [pars[i], cand, term, title, False]\n",
    "                    rows.append(row)\n",
    "                    i += 1\n",
    "                elif i < len(pars) - 1:\n",
    "                    row = [pars[i] + \" \" + pars[i +1], cand, term, title, True]\n",
    "                    i += 2\n",
    "                    if len(tokenize.sent_tokenize(row[0])) < 3 and i < len(pars) - 1:\n",
    "                        row_2 = [row[0] + \" \" + pars[i], cand, term, title, True]\n",
    "                        i += 1\n",
    "                        rows.append(row_2)\n",
    "                    else: \n",
    "                        rows.append(row)\n",
    "                else: \n",
    "                    row = [pars[i - 1] + \" \" + pars[i], cand, term, title, True]\n",
    "                    rows.pop()\n",
    "                    rows.append(row)\n",
    "                            \n",
    "                                \n",
    "                            \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rows[0:10]  #check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"paragraphs.csv\", 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)   \n",
    "    writer.writerow(col_names)\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the process for Annenberg data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21178"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/Annenberg-data\" \n",
    "col_names = [\"Speech_id\",\"text\",\"party\", \"term\", \"comp\"] \n",
    "rows = [] \n",
    "\n",
    "for file in os.listdir('/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/Annenberg-data'): \n",
    "    if file.endswith('txt'): \n",
    "        speech = open(os.path.join(root, file), \"r\").read() \n",
    "        #get meta data of the speech \n",
    "        id_speech = re.search('File_(.*).txt', file).group(1) \n",
    "        party = annen_meta[annen_meta['id_speech'] == float(id_speech)]['party'].iloc[0] \n",
    "        term = annen_meta[annen_meta['id_speech'] == float(id_speech)]['year'].iloc[0] \n",
    "        pars = speech.split('\\n\\n') \n",
    "        # the last paragraph is not part of body text\n",
    "        del pars[-1] \n",
    "        pars = [i for i in pars if i] #remove empty strings \n",
    "        pars = [re.sub(\" \\[[^()]*\\]\", \"\", par) for par in pars] #remove strings inside brackets \n",
    "        # loop through all paragraphs inside each speech  \n",
    "        i = 0     \n",
    "        while i < (len(pars) - 1): \n",
    "            n_sent = len(tokenize.sent_tokenize(pars[i])) \n",
    "            if n_sent >2: \n",
    "                row = [id_speech, pars[i], party, term, False] \n",
    "                rows.append(row) \n",
    "                i += 1 \n",
    "                elif i < len(pars) - 1: \n",
    "                    row = [id_speech, pars[i] + \" \" + pars[i +1], party, term, True] \n",
    "                    i += 2 \n",
    "                    if len(tokenize.sent_tokenize(row[0])) < 3 and i < len(pars) - 1: \n",
    "                        row_2 = [id_speech, row[0] + \" \" + pars[i], party, term, True] \n",
    "                        i += 1 \n",
    "                        rows.append(row_2) \n",
    "                    else:  \n",
    "                        rows.append(row) \n",
    "                else:  \n",
    "                    row = [id_speech, pars[i - 1] + \" \" + pars[i], party, term, True] \n",
    "                    rows.pop() \n",
    "                    rows.append(row) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' To Chairman Dean and my great friend Dick Durbin; and to all my fellow citizens of this great nation; With profound gratitude and great humility, I accept your nomination for the presidency of the United States.']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"paragraphs_annenberg.csv\", 'w') as csvfile: \n",
    "    writer = csv.writer(csvfile)    \n",
    "    writer.writerow(col_names) \n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
