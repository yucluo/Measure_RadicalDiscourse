{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import random \n",
    "import os\n",
    "import csv\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "from numpy import loadtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7606150793650793\n",
      "PR AUC: 0.22538458654838897\n",
      "AUC: 0.7693452380952381\n",
      "PR AUC: 0.2921091965331401\n",
      "AUC: 0.7460411718131433\n",
      "PR AUC: 0.17151312430308696\n",
      "AUC: 0.7471298495645289\n",
      "PR AUC: 0.12002953128571925\n",
      "AUC: 0.7401029295328583\n",
      "PR AUC: 0.23064686482466365\n",
      "AUC: 0.7405753968253967\n",
      "PR AUC: 0.20074563969967418\n",
      "AUC: 0.7922604908946952\n",
      "PR AUC: 0.23978691047830994\n",
      "AUC: 0.8268012668250199\n",
      "PR AUC: 0.26233175899211947\n",
      "AUC: 0.7612103174603174\n",
      "PR AUC: 0.2145353725074794\n",
      "AUC: 0.8082937450514648\n",
      "PR AUC: 0.22959949976398206\n",
      "AUC: 0.7414885193982582\n",
      "PR AUC: 0.11931945348770756\n",
      "AUC: 0.8090277777777778\n",
      "PR AUC: 0.21146706017174238\n",
      "AUC: 0.7679563492063493\n",
      "PR AUC: 0.1946538398097317\n",
      "AUC: 0.7757936507936508\n",
      "PR AUC: 0.26018457572705267\n",
      "AUC: 0.6721100554235946\n",
      "PR AUC: 0.16293522427223336\n",
      "AUC: 0.8662905779889152\n",
      "PR AUC: 0.32668879061548095\n",
      "AUC: 0.7834520981789391\n",
      "PR AUC: 0.2649517656026478\n",
      "AUC: 0.821159936658749\n",
      "PR AUC: 0.23289874503271285\n",
      "AUC: 0.7917656373713381\n",
      "PR AUC: 0.29585478363316187\n",
      "AUC: 0.7673198733174981\n",
      "PR AUC: 0.14495598148188418\n",
      "AUC: 0.7905779889152811\n",
      "PR AUC: 0.28306781136604725\n",
      "AUC: 0.8160134600158353\n",
      "PR AUC: 0.24508920805278497\n",
      "AUC: 0.7845407759303247\n",
      "PR AUC: 0.2537134515660938\n",
      "AUC: 0.7764251781472684\n",
      "PR AUC: 0.2017337117144738\n",
      "AUC: 0.7647466349960412\n",
      "PR AUC: 0.1732667416690947\n",
      "Mean AUC: 0.7768417599819024\n",
      "Mean PR AUC: 0.22229854516557654\n",
      "[[421   0]\n",
      " [ 24   0]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores = []\n",
    "pr_auc = []\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "annotated = pd.read_csv(\"All_annotated_data_round_1_low_pride.csv\")\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_low_pride/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_low_pride/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # pipeline BOW, tfidf and RF\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', RandomForestClassifier(n_estimators=5000, max_depth=3,max_features='sqrt', random_state=0, class_weight=\"balanced\")),\n",
    "                                                 ])\n",
    "    ## Fit the model to the training set\n",
    "    text_clf = text_clf.fit(train['text'], train['pred_class'])\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = text_clf.predict_proba(test['text'])\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(test['true_class'], preds[:,1], pos_label=1)\n",
    "    auc_scores = auc_scores + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(test['true_class'],  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    print(\"PR AUC: \" + str(lr_auc))\n",
    "    \n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores)))\n",
    "print(\"Mean PR AUC: \" + str(np.mean(pr_auc)))\n",
    "\n",
    "confusion = confusion_matrix(test['true_class'],text_clf.predict(test['text']) )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"low_pride_pr_auc_tfidf.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"low_pride_auc_tfidf.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Word2Vec RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7962193190815519\n",
      "PR AUC: 0.2452714731276421\n",
      "AUC: 0.841765873015873\n",
      "PR AUC: 0.3437750214520384\n",
      "AUC: 0.7718253968253967\n",
      "PR AUC: 0.14939514863686978\n",
      "AUC: 0.7315476190476191\n",
      "PR AUC: 0.2036877338582541\n",
      "AUC: 0.8190815518606492\n",
      "PR AUC: 0.26481856870786036\n",
      "AUC: 0.8729216152019003\n",
      "PR AUC: 0.3422544116413206\n",
      "AUC: 0.8370942201108472\n",
      "PR AUC: 0.27704317838743586\n",
      "AUC: 0.8387767220902612\n",
      "PR AUC: 0.3242568673905283\n",
      "AUC: 0.8435273159144893\n",
      "PR AUC: 0.3530411680109525\n",
      "AUC: 0.757818685669042\n",
      "PR AUC: 0.20803165080201466\n",
      "AUC: 0.803147268408551\n",
      "PR AUC: 0.16490232040518385\n",
      "AUC: 0.8077988915281077\n",
      "PR AUC: 0.2139843885123077\n",
      "AUC: 0.8037410926365796\n",
      "PR AUC: 0.1936424477080629\n",
      "AUC: 0.8341250989707047\n",
      "PR AUC: 0.2920422331587763\n",
      "AUC: 0.7372327790973872\n",
      "PR AUC: 0.14605246539784233\n",
      "AUC: 0.8111638954869359\n",
      "PR AUC: 0.2763073033538633\n",
      "AUC: 0.7490079365079365\n",
      "PR AUC: 0.1933868744933464\n",
      "AUC: 0.8371031746031746\n",
      "PR AUC: 0.2831083593844067\n",
      "AUC: 0.7463293650793651\n",
      "PR AUC: 0.25696760035136806\n",
      "AUC: 0.8511480601741884\n",
      "PR AUC: 0.27905814143944974\n",
      "AUC: 0.7564484126984128\n",
      "PR AUC: 0.17424502244976262\n",
      "AUC: 0.8225455265241488\n",
      "PR AUC: 0.1793871152905795\n",
      "AUC: 0.7427579365079365\n",
      "PR AUC: 0.1701099377441366\n",
      "AUC: 0.7646825396825397\n",
      "PR AUC: 0.20098481027884552\n",
      "AUC: 0.7276326207442596\n",
      "PR AUC: 0.20735471585550516\n",
      "Mean AUC: 0.7962177166987143\n",
      "Mean PR AUC: 0.2377243583135341\n",
      "[[417   4]\n",
      " [ 21   3]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores = []\n",
    "pr_auc = []\n",
    "\n",
    "dat = pd.read_csv(\"20220131_all_paragraphs_2020_added_missings_added.csv\")\n",
    "annotated = pd.read_csv(\"All_annotated_data_round_1_low_pride_20220209.csv\")\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "avg_embeddings = loadtxt('avg_embeddings.csv', delimiter=',')\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_low_pride/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_low_pride/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # attach train/test set to original df\n",
    "    train_id = train['speech_par_id']\n",
    "    test_id = test['speech_par_id']    \n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test_id), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train_id), 1, 0)\n",
    "    # get the embeddings to train/test set\n",
    "    train_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(train_id)].index.tolist()])\n",
    "    test_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(test_id)].index.tolist()])\n",
    "    ## Fit the model to the training set\n",
    "    text_clf = RandomForestClassifier(n_estimators=5000, max_depth=10, random_state=0, max_features = 0.2, class_weight=\"balanced\")\n",
    "    text_clf.fit(train_vec,  dat[dat['train']==1].pred_class) # use the original df to match the order\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = text_clf.predict_proba(test_vec)\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores = auc_scores + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    print(\"PR AUC: \" + str(lr_auc))\n",
    "    \n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores)))\n",
    "print(\"Mean PR AUC: \" + str(np.mean(pr_auc)))\n",
    "\n",
    "confusion = confusion_matrix(dat[dat['test']==1].pred_class,text_clf.predict(test_vec) )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"low_pride_pr_auc_w2v.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"low_pride_auc_w2v.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8417458432304038\n",
      "PR AUC: 0.340082054419441\n",
      "AUC: 0.8682539682539683\n",
      "PR AUC: 0.34267495235981443\n",
      "AUC: 0.8269841269841269\n",
      "PR AUC: 0.21849476098478235\n",
      "AUC: 0.7945436507936509\n",
      "PR AUC: 0.28551953481772424\n",
      "AUC: 0.869061757719715\n",
      "PR AUC: 0.3218574614774693\n",
      "AUC: 0.8743072050673001\n",
      "PR AUC: 0.4214816086846277\n",
      "AUC: 0.8776722090261282\n",
      "PR AUC: 0.23233452505245822\n",
      "AUC: 0.8431314330958036\n",
      "PR AUC: 0.2541904344864129\n",
      "AUC: 0.8742082343626286\n",
      "PR AUC: 0.4499977901374089\n",
      "AUC: 0.7845407759303247\n",
      "PR AUC: 0.19764987608130694\n",
      "AUC: 0.8135391923990499\n",
      "PR AUC: 0.23720117189025916\n",
      "AUC: 0.8538202692003167\n",
      "PR AUC: 0.3758590764816222\n",
      "AUC: 0.7991884402216943\n",
      "PR AUC: 0.2599769211047365\n",
      "AUC: 0.8520387965162312\n",
      "PR AUC: 0.3258949339537761\n",
      "AUC: 0.785827395091053\n",
      "PR AUC: 0.2126309874651194\n",
      "AUC: 0.8657957244655582\n",
      "PR AUC: 0.3761872670199383\n",
      "AUC: 0.7966269841269841\n",
      "PR AUC: 0.26029777479078137\n",
      "AUC: 0.8137896825396825\n",
      "PR AUC: 0.3656808477689209\n",
      "AUC: 0.7841269841269841\n",
      "PR AUC: 0.2978480916390276\n",
      "AUC: 0.8406571654790183\n",
      "PR AUC: 0.2098802744106158\n",
      "AUC: 0.7843253968253968\n",
      "PR AUC: 0.2098841668702028\n",
      "AUC: 0.8539192399049881\n",
      "PR AUC: 0.26132051507712223\n",
      "AUC: 0.749702380952381\n",
      "PR AUC: 0.25533010412385376\n",
      "AUC: 0.768154761904762\n",
      "PR AUC: 0.2729734170149794\n",
      "AUC: 0.78701504354711\n",
      "PR AUC: 0.27112102704983015\n",
      "Mean AUC: 0.8241190664706104\n",
      "Mean PR AUC: 0.29025478300648927\n",
      "[[421   0]\n",
      " [ 23   1]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores = []\n",
    "pr_auc = []\n",
    "\n",
    "dat = pd.read_csv(\"20220131_all_paragraphs_2020_added_missings_added.csv\")\n",
    "annotated = pd.read_csv(\"All_annotated_data_round_1_low_pride_20220209.csv\")\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "avg_embeddings = loadtxt('avg_embeddings_pretrained.csv', delimiter=',')\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_low_pride/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_low_pride/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "# attach train/test set to original df\n",
    "    train_id = train['speech_par_id']\n",
    "    test_id = test['speech_par_id']    \n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test_id), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train_id), 1, 0)\n",
    "    # get the embeddings to train/test set\n",
    "    train_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(train_id)].index.tolist()])\n",
    "    test_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(test_id)].index.tolist()])\n",
    "    ## Fit the model to the training set\n",
    "    text_clf = RandomForestClassifier(n_estimators=5000, max_depth=10, random_state=0, max_features = \"sqrt\", class_weight=\"balanced\")\n",
    "    text_clf.fit(train_vec,  dat[dat['train']==1].pred_class) # use the original df to match the order\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = text_clf.predict_proba(test_vec)\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores = auc_scores + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    print(\"PR AUC: \" + str(lr_auc))\n",
    "    \n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores)))\n",
    "print(\"Mean PR AUC: \" + str(np.mean(pr_auc)))\n",
    "\n",
    "confusion = confusion_matrix(dat[dat['test']==1].pred_class,text_clf.predict(test_vec) )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"low_pride_pr_auc_pretrained.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"low_pride_auc_pretrained.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8477830562153602\n",
      "Accuracy: 0.946067415730337\n",
      "PR_AUC: 0.29798266007344465\n",
      "AUC: 0.7546626984126984\n",
      "Accuracy: 0.9459459459459459\n",
      "PR_AUC: 0.25004884692263635\n",
      "AUC: 0.7875894988066825\n",
      "Accuracy: 0.945823927765237\n",
      "PR_AUC: 0.2948317954081235\n",
      "AUC: 0.7618337311058075\n",
      "Accuracy: 0.945823927765237\n",
      "PR_AUC: 0.16610900034432255\n",
      "AUC: 0.8149247822644496\n",
      "Accuracy: 0.946067415730337\n",
      "PR_AUC: 0.22014143810201348\n",
      "AUC: 0.7526722090261282\n",
      "Accuracy: 0.946067415730337\n",
      "PR_AUC: 0.13345744300941714\n",
      "AUC: 0.7855158730158731\n",
      "Accuracy: 0.9459459459459459\n",
      "PR_AUC: 0.20166848519952504\n",
      "AUC: 0.870249406175772\n",
      "Accuracy: 0.946067415730337\n",
      "PR_AUC: 0.4493529028636205\n",
      "AUC: 0.8214568487727633\n",
      "Accuracy: 0.946067415730337\n",
      "PR_AUC: 0.22321908068681476\n",
      "AUC: 0.7584125098970704\n",
      "Accuracy: 0.946067415730337\n",
      "PR_AUC: 0.23834850114260503\n",
      "AUC: 0.7821428571428571\n",
      "Accuracy: 0.9459459459459459\n",
      "PR_AUC: 0.2628000584783819\n",
      "AUC: 0.8357086302454474\n",
      "Accuracy: 0.946067415730337\n",
      "PR_AUC: 0.20471283512235255\n",
      "AUC: 0.7922604908946952\n",
      "Accuracy: 0.946067415730337\n",
      "PR_AUC: 0.1797995146953486\n",
      "AUC: 0.7774801587301587\n",
      "Accuracy: 0.9459459459459459\n",
      "PR_AUC: 0.26398449450396483\n",
      "AUC: 0.7871140142517814\n",
      "Accuracy: 0.946067415730337\n",
      "PR_AUC: 0.25641658538202855\n",
      "AUC: 0.8515439429928741\n",
      "Accuracy: 0.946067415730337\n",
      "PR_AUC: 0.32334087896219244\n",
      "AUC: 0.779563492063492\n",
      "Accuracy: 0.9459459459459459\n",
      "PR_AUC: 0.2061555914011213\n",
      "AUC: 0.8341269841269842\n",
      "Accuracy: 0.9459459459459459\n",
      "PR_AUC: 0.33003328381300606\n",
      "AUC: 0.8185515873015873\n",
      "Accuracy: 0.9459459459459459\n",
      "PR_AUC: 0.28295699435835014\n",
      "AUC: 0.754265873015873\n",
      "Accuracy: 0.9459459459459459\n",
      "PR_AUC: 0.21026132735712166\n",
      "AUC: 0.8103619729514717\n",
      "Accuracy: 0.945823927765237\n",
      "PR_AUC: 0.3139865332748725\n",
      "AUC: 0.8296626984126984\n",
      "Accuracy: 0.9459459459459459\n",
      "PR_AUC: 0.2630248809090478\n",
      "AUC: 0.7696428571428572\n",
      "Accuracy: 0.9459459459459459\n",
      "PR_AUC: 0.3354031177028603\n",
      "AUC: 0.7436507936507937\n",
      "Accuracy: 0.9459459459459459\n",
      "PR_AUC: 0.27971099964681095\n",
      "AUC: 0.8399643705463182\n",
      "Accuracy: 0.946067415730337\n",
      "PR_AUC: 0.2334836309742008\n",
      "Mean AUC: 0.7984456534864998\n",
      "Mean Accuracy: 0.945984750469393\n",
      "[[421   0]\n",
      " [ 24   0]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores_d2v = []\n",
    "pr_auc = []\n",
    "accuracy_scores_d2v = []\n",
    "\n",
    "dat = pd.read_csv(\"20201115_all_paragraphs.csv\")\n",
    "dat['speech_par_id'] = dat['Speech_id'].astype(str) + \"_\" + dat['par_id'].astype(str)\n",
    "\n",
    "model = Word2Vec.load(\"doc2vec_wordvecs.model\") \n",
    "annotated = pd.read_csv(\"All_annotated_data_round_1_low_pride_20220209.csv\")\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_low_pride/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_low_pride/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # merge test and train sets back onto the total df\n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test[\"speech_par_id\"]), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train[\"speech_par_id\"]), 1, 0)\n",
    "    # so that teh order of rows are the same with the embeddings\n",
    "    test_set = np.asarray([model.docvecs[i] for i in dat[dat['test'] == 1].index.tolist()])\n",
    "    train_set = np.asarray([model.docvecs[i] for i in dat[dat['train']== 1].index.tolist()])\n",
    "    ## Initialize a random forest classifier\n",
    "    gbc = RandomForestClassifier(n_estimators=500, max_depth=10, random_state=0, max_features = \"sqrt\", class_weight=\"balanced\")\n",
    "    ## Fit the model to the training set\n",
    "    gbc.fit(train_set, dat[dat['train']==1].pred_class)\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = gbc.predict_proba(test_set)\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores_d2v = auc_scores_d2v + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    accuracy_d2v = metrics.accuracy_score(dat[dat['test']==1].pred_class, gbc.predict(test_set))\n",
    "    accuracy_scores_d2v = accuracy_scores_d2v + [accuracy_d2v]\n",
    "    print(\"Accuracy: \" + str(accuracy_d2v))\n",
    "    print(\"PR_AUC: \" + str(lr_auc))\n",
    "\n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores_d2v)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_d2v)))\n",
    "\n",
    "confusion = confusion_matrix(dat[dat['test']==1].pred_class, gbc.predict(test_set))\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"low_pride_d2v_pr_auc.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])\n",
    "    \n",
    "with open(\"low_pride_d2v_auc.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores_d2v:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC-AUC: 0.7984456534864998\n",
      "Mean Accuracy: 0.945984750469393\n",
      "Mean PR_AUC: 0.25684923521336733\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean ROC-AUC: \" + str(np.mean(auc_scores_d2v)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_d2v)))\n",
    "print(\"Mean PR_AUC: \" + str(np.mean(pr_auc)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balanced RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8525336500395883\n",
      "Accuracy: 0.6966292134831461\n",
      "PR_AUC: 0.34895721705593585\n",
      "AUC: 0.8099206349206349\n",
      "Accuracy: 0.722972972972973\n",
      "PR_AUC: 0.29771768877035487\n",
      "AUC: 0.8299522673031026\n",
      "Accuracy: 0.7042889390519187\n",
      "PR_AUC: 0.4059061806659194\n",
      "AUC: 0.7900755767700876\n",
      "Accuracy: 0.7110609480812641\n",
      "PR_AUC: 0.16247843063160616\n",
      "AUC: 0.8185866983372921\n",
      "Accuracy: 0.7078651685393258\n",
      "PR_AUC: 0.21768531155269744\n",
      "AUC: 0.7915676959619953\n",
      "Accuracy: 0.750561797752809\n",
      "PR_AUC: 0.15715766763653932\n",
      "AUC: 0.8582341269841269\n",
      "Accuracy: 0.7364864864864865\n",
      "PR_AUC: 0.22746274123923202\n",
      "AUC: 0.8624307205067301\n",
      "Accuracy: 0.7303370786516854\n",
      "PR_AUC: 0.41404618217774375\n",
      "AUC: 0.8705463182897862\n",
      "Accuracy: 0.7168539325842697\n",
      "PR_AUC: 0.30127957795354526\n",
      "AUC: 0.7896872525732382\n",
      "Accuracy: 0.698876404494382\n",
      "PR_AUC: 0.2858382265095982\n",
      "AUC: 0.8111111111111111\n",
      "Accuracy: 0.7094594594594594\n",
      "PR_AUC: 0.27469317867091897\n",
      "AUC: 0.8506532066508313\n",
      "Accuracy: 0.7101123595505618\n",
      "PR_AUC: 0.27088692308705115\n",
      "AUC: 0.8195764053840063\n",
      "Accuracy: 0.7235955056179775\n",
      "PR_AUC: 0.22288244040693111\n",
      "AUC: 0.8379960317460318\n",
      "Accuracy: 0.7387387387387387\n",
      "PR_AUC: 0.308196177632181\n",
      "AUC: 0.8186856690419636\n",
      "Accuracy: 0.7078651685393258\n",
      "PR_AUC: 0.2516643825654042\n",
      "AUC: 0.8885589865399842\n",
      "Accuracy: 0.701123595505618\n",
      "PR_AUC: 0.2815105165550654\n",
      "AUC: 0.7796626984126984\n",
      "Accuracy: 0.7274774774774775\n",
      "PR_AUC: 0.22886852527011137\n",
      "AUC: 0.8303571428571428\n",
      "Accuracy: 0.6644144144144144\n",
      "PR_AUC: 0.2819962828354822\n",
      "AUC: 0.8289682539682539\n",
      "Accuracy: 0.740990990990991\n",
      "PR_AUC: 0.21386319672252116\n",
      "AUC: 0.8197420634920635\n",
      "Accuracy: 0.7364864864864865\n",
      "PR_AUC: 0.21384550594167634\n",
      "AUC: 0.7956443914081146\n",
      "Accuracy: 0.6839729119638827\n",
      "PR_AUC: 0.25103018814585437\n",
      "AUC: 0.855654761904762\n",
      "Accuracy: 0.7319819819819819\n",
      "PR_AUC: 0.24158669047088782\n",
      "AUC: 0.7886904761904762\n",
      "Accuracy: 0.7432432432432432\n",
      "PR_AUC: 0.354490268509612\n",
      "AUC: 0.7774801587301587\n",
      "Accuracy: 0.7094594594594594\n",
      "PR_AUC: 0.2814737059473535\n",
      "AUC: 0.8267022961203484\n",
      "Accuracy: 0.7168539325842697\n",
      "PR_AUC: 0.36381641572828083\n",
      "Mean AUC: 0.8241207438097811\n",
      "Mean Accuracy: 0.7168683467244858\n"
     ]
    }
   ],
   "source": [
    "auc_scores_balanced = []\n",
    "pr_auc_balanced = []\n",
    "accuracy_scores_balanced = []\n",
    "\n",
    "dat = pd.read_csv(\"20201115_all_paragraphs.csv\")\n",
    "dat['speech_par_id'] = dat['Speech_id'].astype(str) + \"_\" + dat['par_id'].astype(str)\n",
    "# X = np.asarray([model.docvecs[i] for i in label_dat.index.tolist()])\n",
    "# Y = np.asarray(label_dat['label'].tolist(), dtype=\"int\")\n",
    "model = Word2Vec.load(\"doc2vec_wordvecs.model\") \n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_low_pride/\"):\n",
    "    ## load the test set \n",
    "    annotated = pd.read_csv(\"All_annotated_data_round_1_low_pride_20220209.csv\")\n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_low_pride/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # merge test and train sets back onto the total df\n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test[\"speech_par_id\"]), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train[\"speech_par_id\"]), 1, 0)\n",
    "    # so that teh order of rows are the same with the embeddings\n",
    "    test_set = np.asarray([model.docvecs[i] for i in dat[dat['test'] == 1].index.tolist()])\n",
    "    train_set = np.asarray([model.docvecs[i] for i in dat[dat['train']== 1].index.tolist()])\n",
    "    ## Initialize a random forest classifier\n",
    "    brfc = BalancedRandomForestClassifier(n_estimators=5000, max_depth=3, random_state=0, max_features = \"sqrt\")\n",
    "    ## Fit the model to the training set\n",
    "    brfc.fit(train_set, dat[dat['train']==1].pred_class)\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = brfc.predict_proba(test_set)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores_balanced = auc_scores_balanced + [metrics.auc(fpr, tpr)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc_balanced = pr_auc_balanced + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr, tpr)))\n",
    "    accuracy_balanced = metrics.accuracy_score(dat[dat['test']==1].pred_class, brfc.predict(test_set))\n",
    "    accuracy_scores_balanced = accuracy_scores_balanced + [accuracy_balanced]\n",
    "    print(\"Accuracy: \" + str(accuracy_balanced))\n",
    "    print(\"PR_AUC: \" + str(lr_auc))\n",
    "\n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores_balanced)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"low_pride_pr_auc_balanced.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc_balanced:\n",
    "        csvwriter.writerow([row])\n",
    "    \n",
    "with open(\"low_pride_auc_balanced.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores_balanced:\n",
    "        csvwriter.writerow([row])\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC-AUC: 0.8241207438097811\n",
      "Mean Accuracy: 0.7168683467244858\n",
      "Mean PR_AUC: 0.27437334490730014\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean ROC-AUC: \" + str(np.mean(auc_scores_balanced)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_balanced)))\n",
    "print(\"Mean PR_AUC: \" + str(np.mean(pr_auc_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: neuralcoref 4.0\n",
      "Uninstalling neuralcoref-4.0:\n",
      "  Would remove:\n",
      "    /opt/anaconda3/lib/python3.7/site-packages/neuralcoref-4.0.dist-info/*\n",
      "    /opt/anaconda3/lib/python3.7/site-packages/neuralcoref/*\n",
      "Proceed (y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-9063a9f0e032>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
