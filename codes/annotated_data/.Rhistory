descriptives2 = data.frame(Mean = colMeans(descriptives2),
sd = apply(descriptives2, 2, sd),
t(apply(descriptives2, 2, quantile, probs = c(0, 0.25, 0.5, 0.75, 1)))
)
descriptives = rbind(descriptives1, descriptives2)
descriptives
?kable()
descriptives = rbind(descriptives1, descriptives2)                                                          kable(descriptives, format = "latex", col.names = c("Mean", "Sd", "Min", "1st Percentile", "Median", "3rd Percentile", "max"))
kable(descriptives, format = "latex", col.names = c("Mean", "Sd", "Min", "1st Percentile", "Median", "3rd Percentile", "max"))
kable(descriptives, format = "latex", col.names = c("Mean", "Sd", "Min", "1st Percentile", "Median", "3rd Percentile", "max"), )
kable(descriptives, format = "latex", col.names = c("Mean", "Sd", "Min", "1st Percentile", "Median", "3rd Percentile", "max") )
descriptives = rbind(descriptives1, descriptives2)
descriptives = rbind(descriptives1, descriptives2)
descriptives
prop_in_speech = function(file){
dat = read.csv(file)
dat = left_join(dat, all_data %>% select(speech_par_id,Speech_id, term, party, text), by = "speech_par_id")
# use 0.5 as cutoff
dat$predicted = ifelse(dat$Predictions_prob_1 > dat$Predictions_prob_0, 1, 0)
# drop bush 2000
dat = dat[dat$term != 2000 | dat$party != "rep", ]
# aggregate paragraph labels into speech level label
setDT(dat)
speech_level_df = NULL
for (i in unique(dat$Speech_id)){
df = dat[Speech_id == i, ] #isolate each speech
df$speech_prop = sum(df$predicted)/nrow(df)
speech_level_df = rbind(speech_level_df, df)
}
speech_level_df$wordcount = sapply(strsplit(as.character(speech_level_df$text), " "), length)
word_count = speech_level_df %>% group_by(Speech_id) %>% summarise(wordcount = sum(wordcount))
speech_labels = speech_level_df %>% select(Speech_id, term, party,speech_pred)
# drop duplicates
speech_labels = speech_labels[!duplicated(speech_labels)]
speech_labels = left_join(speech_labels, word_count, by = "Speech_id")
return(speech_labels)
}
pop_in_speech = prop_in_speech("Iteration_3_predictions_populism.csv")
speech_level_prediction = function(file){
dat = read.csv(file)
dat = left_join(dat, all_data %>% select(speech_par_id,Speech_id, term, party), by = "speech_par_id")
# use 0.5 as cutoff
dat$predicted = ifelse(dat$Predictions_prob_1 > dat$Predictions_prob_0, 1, 0)
# drop bush 2000
dat = dat[dat$term != 2000 | dat$party != "rep", ]
# aggregate paragraph labels into speech level label
setDT(dat)
speech_level_df = NULL
for (i in unique(dat$Speech_id)){
df = dat[Speech_id == i, ] #isolate each speech
df$speech_pred = ifelse(sum(df$predicted) >0, 1, 0) # create speech label for each line
speech_level_df = rbind(speech_level_df, df)
}
speech_level_df$wordcount = sapply(strsplit(as.character(speech_level_df$text), " "), length)
word_count = speech_level_df %>% group_by(Speech_id) %>% summarise(wordcount = sum(wordcount))
speech_labels = speech_level_df %>% select(Speech_id, term, party,speech_pred)
# drop duplicates
speech_labels = speech_labels[!duplicated(speech_labels)]
speech_labels = left_join(speech_labels, word_count, by = "Speech_id")
return(speech_labels)
}
getwd()
# write a function to get speech level pravalence
speech_level_pravalence = function(file){
dat = read.csv(file)
dat = left_join(dat, all_data %>% select(speech_par_id,Speech_id, term, party, text), by = "speech_par_id")
# use 0.5 as cutoff
dat$predicted = ifelse(dat$Predictions_prob_1 > dat$Predictions_prob_0, 1, 0)
# drop bush 2000
dat = dat[dat$term != 2000 | dat$party != "rep", ]
# aggregate paragraph labels into speech level label
setDT(dat)
speech_level_df = NULL
for (i in unique(dat$Speech_id)){
df = dat[Speech_id == i, ] #isolate each speech
df$speech_pred = ifelse(sum(df$predicted) >0, 1, 0) # create speech label for each line
speech_level_df = rbind(speech_level_df, df)
}
speech_labels = speech_level_df %>% select(Speech_id, term, party, speech_pred)
# drop duplicates
speech_labels = speech_labels[!duplicated(speech_labels)]
# speech level plot
# generate frequency percentage
counts_overall = speech_labels %>%
group_by(term, party) %>%
summarise(total = n())
counts_by_party = speech_labels %>%
group_by(term, party, speech_pred) %>%
summarise(counts = n())
merged_counts = merge(counts_by_party, counts_overall, by = c("term", "party"))
setDT(merged_counts)
merged_counts[, perc := counts/total]
merged_counts[, pos_perc := ifelse(speech_pred == 1, perc, 1- perc)]
# drop duplicates
merged_counts = merged_counts[!duplicated(merged_counts[ , c("term","party")]),]
return(merged_counts %>% select(term, party, pos_perc))
}
getwd()
getwd()
setwd("/Users/yuchenluo/Desktop/social_networks/")
getwd()
knitr::opts_chunk$set(echo = TRUE)
library(igraph)
setwd("/Users/yuchenluo/Desktop/social_networks/")
knitr::opts_chunk$set(echo = TRUE)
library(igraph)
#  Import Lazega friend adjacency matrix
Krack_report_matrix <- as.matrix(read.csv("Krack-Report-no-row (1).csv",header=TRUE,row.names=NULL,check.names=FALSE))
Krack_report_matrix
knitr::opts_chunk$set(echo = TRUE)
library(igraph)
#  Create igraph object from this matrix, "undirected" will put a maximum of 1
#  for both edges (A->B and B->A) if either edge has a 1; other options are
#  available, including "directed", "max", "min", etc
Krack_report_graph <- graph.adjacency(Krack_report_matrix,mode="directed",weighted=NULL)
# attach the attributes to the Vertices of the igraph
vertex_attr(Krack_report_graph, index=Krack_report_attributes$ID) <- Krack_report_attributes
# import attribute data (as a data.frame)
Krack_report_attributes <- read.csv("Krack-Atts%2B_2_.csv", header=TRUE)
# attach the attributes to the Vertices of the igraph
vertex_attr(Krack_report_graph, index=Krack_report_attributes$ID) <- Krack_report_attributes
?transivity
?transitivity
transitivity(Krack_report_graph, type="global")
Krack_report_graph
transitivity(Krack_report_graph, type="average")
?edge_density
edge_density(Krack_report_graph)
Krack_report_attributes
summary(Krack_report_attributes)
table(Krack_report_attributes)
table(Krack_report_attributes[,4:5])
# for level
table(Krack_report_attributes[,4])
# for Dept
table(Krack_report_attributes[,5])
Krack_report_matrix
# calculate other centrality measures
Krack_report_attributes <- merge(Krack_report_attributes,  #  Merge attributes
data.frame(  # With a new data.frame
ID=V(Krack_report_graph)$ID,  # Where the ID is the ID of each vertex
in.deg= degree(Krack_report_graph, mode = c("in"), loops = TRUE, normalized = FALSE),
out.deg= degree(Krack_report_graph, mode = c("out"), loops = TRUE, normalized = FALSE),
btwn= betweenness(Krack_report_graph, directed = T),
close = closeness(Krack_report_graph, mode = c("all")),
eigen <- evcent(Krack_report_graph),
bon <- bonpow(Krack_report_graph)
),
by='ID')
Krack_report_attributes
Krack_report_attributes = Krack_report_attributes[,c(1:14, 36)]
Krack_report_attributes = Krack_report_attributes[,c(1:11, 32)]
Krack_report_attributes
# calculate other centrality measures
Krack_report_attributes <- merge(Krack_report_attributes,  #  Merge attributes
data.frame(  # With a new data.frame
ID=V(Krack_report_graph)$ID,  # Where the ID is the ID of each vertex
in.deg= degree(Krack_report_graph, mode = c("in"), loops = TRUE, normalized = FALSE),
out.deg= degree(Krack_report_graph, mode = c("out"), loops = TRUE, normalized = FALSE),
btwn= betweenness(Krack_report_graph, directed = T),
close = closeness(Krack_report_graph, mode = c("all")),
eigen = evcent(Krack_report_graph),
bon = bonpow(Krack_report_graph)
),
by='ID')
Krack_report_attributes = Krack_report_attributes[,c(1:11, 32)]
Krack_report_attributes
# import attribute data (as a data.frame)
Krack_report_attributes <- read.csv("Krack-Atts%2B_2_.csv", header=TRUE)
# calculate other centrality measures
Krack_report_attributes <- merge(Krack_report_attributes,  #  Merge attributes
data.frame(  # With a new data.frame
ID=V(Krack_report_graph)$ID,  # Where the ID is the ID of each vertex
in.deg= degree(Krack_report_graph, mode = c("in"), loops = TRUE, normalized = FALSE),
out.deg= degree(Krack_report_graph, mode = c("out"), loops = TRUE, normalized = FALSE),
btwn= betweenness(Krack_report_graph, directed = T),
close = closeness(Krack_report_graph, mode = c("all")),
eigen = evcent(Krack_report_graph),
bon = bonpow(Krack_report_graph)
),
by='ID')
Krack_report_attributes
# clean up
Krack_report_attributes = Krack_report_attributes[,c(1:11, 32)]
names(Krack_report_attributes)[names(Krack_report_attributes)=="bon....bonpow.lazega_friends_graph."] <- "bon"
Krack_report_attributes
cor(Krack_report_attributes)
# import attribute data (as a data.frame)
Krack_report_attributes <- read.csv("Krack-Atts%2B_2_.csv", header=TRUE)
# calculate other centrality measures
Krack_report_attributes <- merge(Krack_report_attributes,  #  Merge attributes
data.frame(  # With a new data.frame
ID=V(Krack_report_graph)$ID,  # Where the ID is the ID of each vertex
in.deg= degree(Krack_report_graph, mode = c("in"), loops = TRUE, normalized = FALSE),
out.deg= degree(Krack_report_graph, mode = c("out"), loops = TRUE, normalized = FALSE),
btwn= betweenness(Krack_report_graph, directed = T),
close = closeness(Krack_report_graph, mode = c("all")),
eigen = evcent(Krack_report_graph),
bon = bonpow(Krack_report_graph)
),
by='ID')
# clean up
Krack_report_attributes = Krack_report_attributes[,c(1:10, 32)]
names(Krack_report_attributes)[names(Krack_report_attributes)=="bon....bonpow.lazega_friends_graph."] <- "bon"
cor(Krack_report_attributes)
cor(Krack_report_attributes[,6:11])
Krack_report_attributes
Krack_report_attributes[which.max(Krack_report_attributes$in.deg), ]
?bonpow
Krack_report_attributes[which.min(Krack_report_attributes$in.deg), ]
Krack_report_matrix
Krack_report_graph
Krack_report_attributes[which.max(Krack_report_attributes$out.deg), ]
Krack_report_attributes[which.min(Krack_report_attributes$out.deg), ]
Krack_report_attributes[which.max(Krack_report_attributes$out.deg), ]
Krack_report_attributes[which.max(Krack_report_attributes$out.deg), ]
Krack_report_attributes
Krack_report_attributes[which.max(Krack_report_attributes$btwn), ]
Krack_report_attributes[which.min(Krack_report_attributes$btwn), ]
Krack_report_attributes[which.max(Krack_report_attributes$close), ]
Krack_report_attributes[which.min(Krack_report_attributes$close), ]
Krack_report_attributes[which.min(Krack_report_attributes$btwn), ]
Krack_report_attributes[which.max(Krack_report_attributes$close), ]
Krack_report_attributes[which.min(Krack_report_attributes$close), ]
Krack_report_attributes[which.max(Krack_report_attributes$eigen.vector), ]
Krack_report_attributes[which.min(Krack_report_attributes$eigen.vector), ]
Krack_report_attributes[which.max(Krack_report_attributes$btwn), ]
Krack_report_attributes[which.min(Krack_report_attributes$btwn), ]
Krack_report_attributes[which.max(Krack_report_attributes$close), ]
Krack_report_attributes[which.max(Krack_report_attributes$eigen.vector), ]
Krack_report_attributes[which.min(Krack_report_attributes$eigen.vector), ]
Krack_report_attributes[which.min(Krack_report_attributes$close), ]
Krack_report_attributes[which.max(Krack_report_attributes$eigen.vector), ]
Krack_report_attributes[which.min(Krack_report_attributes$eigen.vector), ]
# list nodes of max/min central
Krack_report_attributes[which.max(Krack_report_attributes$in.deg), ]
Krack_report_attributes[which.min(Krack_report_attributes$in.deg), ]
Krack_report_attributes[which.max(Krack_report_attributes$out.deg), ]
Krack_report_attributes[which.min(Krack_report_attributes$out.deg), ]
Krack_report_attributes[which.max(Krack_report_attributes$btwn), ]
Krack_report_attributes[which.max(Krack_report_attributes$btwn), ]
Krack_report_attributes[which.min(Krack_report_attributes$btwn), ]
Krack_report_attributes[which.max(Krack_report_attributes$close), ]
Krack_report_attributes[which.min(Krack_report_attributes$close), ]
Krack_report_attributes[which.max(Krack_report_attributes$eigen.vector), ]
Krack_report_attributes[which.min(Krack_report_attributes$eigen.vector), ]
Krack_report_attributes[which.max(Krack_report_attributes$close), ]
install.packages("ergm")
library(ergm)
?network
summary(lmp(in.deg ~ tenure, Krack_report_attributes,perm="Prob"))
library(lmPerm)
installed.packages("lmPerm")
library(lmPerm)
install.packages("lmPerm")
library(lmPerm)
summary(lmp(in.deg ~ tenure, Krack_report_attributes,perm="Prob"))
Krack_report_attributes
summary(lmp(in.deg ~ TENURE, Krack_report_attributes,perm="Prob"))
summary(lmp(in.deg ~ LEVEL, Krack_report_attributes,perm="Prob"))
summary(lmp(close ~ TENURE, Krack_report_attributes,perm="Prob"))
summary(lmp(between ~ TENURE, Krack_report_attributes,perm="Prob"))
summary(lmp(btwn ~ TENURE, Krack_report_attributes,perm="Prob"))
summary(lmp(eigen.vector ~ TENURE, Krack_report_attributes,perm="Prob"))
summary(lmp(close ~ TENURE, Krack_report_attributes,perm="Prob"))
summary(lmp(close ~ TENURE, Krack_report_attributes, perm="Prob"))
summary(lmp(close ~ TENURE, Krack_report_attributes, perm="Prob"))
summary(lmp(btwn ~ TENURE, Krack_report_attributes, perm="Prob"))
summary(lmp(btwn ~ TENURE + as.factor(LEVEL) + AGE, Krack_report_attributes, perm="Prob"))
summary(lmp(in.deg ~ TENURE + as.factor(LEVEL) + AGE, Krack_report_attributes, perm="Prob"))
summary(lmp(in.deg ~ TENURE + as.factor(LEVEL) + AGE, Krack_report_attributes + DEPT, perm="Prob"))
Krack_report_attributes
summary(lmp(in.deg ~ TENURE + as.factor(LEVEL) + AGE+ DEPT, Krack_report_attributes , perm="Prob"))
summary(lmp(out.deg ~ TENURE + as.factor(LEVEL) + AGE+ DEPT, Krack_report_attributes , perm="Prob"))
summary(lmp(out.deg ~ TENURE + as.factor(LEVEL) + AGE+ DEPT, Krack_report_attributes , perm="Prob"))
summary(lmp(in.deg ~ TENURE + as.factor(LEVEL) + AGE+ as.factor(DEPT), Krack_report_attributes , perm="Prob"))
summary(lmp(out.deg ~ TENURE + as.factor(LEVEL) + AGE+ as.factor(DEPT), Krack_report_attributes , perm="Prob"))
# for Dept (categorical)
table(Krack_report_attributes[,5])
Krack_report_attributes
summary(lmp(in.deg ~ TENURE + LEVEL + AGE+ as.factor(DEPT), Krack_report_attributes , perm="Prob"))
summary(lmp(out.deg ~ TENURE + LEVEL + AGE+ as.factor(DEPT), Krack_report_attributes , perm="Prob"))
string = "And let me conclude by saying: Jerry Ford hasn't and won't let you down.  Thank you very much. AIR FORCEECONOMY--GREAT BRITAININCOME TAX--CLASS ISSUESMILITARY CUTBACKS"
str_extract('.+(?=[A-Z]{3}), string)
)
str_extract('.+(?=[A-Z]{3})', string)
library(stringr)
str_extract('.+(?=[A-Z]{3})', string)
str_extract('.[A-Z]{3})', string)
str_extract()
?str_extract()
str_extract(string, '.[A-Z]{3})')
str_extract(string, '.+(?=[A-Z]{3})')
str_extract(string, '^.+(?=[A-Z]{3})')
str_match(string, '^.+(?=[A-Z]{3})')
str_extract(string, '^.+(?=[A-Z]{3})')
str_extract(string, '^.+(?=[A-Z]{3,})')
str_extract(string, '^.+(?=[^A-Z]{3,})')
str_extract(string, '^.+(?=[[^:upper:]]{3,})')
str_extract(string, '^.[[^:upper:]]{3,})')
str_extract(string, '^.+(?=[[^A-Z]]{3,})')
str_extract(string, '^.+(?=[[^A-Z]]{3})')
str_extract(string, '^.+(?=^[A-Z]{3,})')
str_extract(string, '^.+(?=[A-Z]{3,})')
str_extract(string, '.+(?=[A-Z]{3,})')
str_extract(string, '.+(?=[^A-Z]{3,})')
str_extract(string, '$.+(?=[^A-Z]{3,})')
str_extract(string, '.+(?=$[^A-Z]{3,})')
str_split(string, " ")
apply(str_split(string, " "), str_extract, '^[A-Z ]+$')
apply(str_split(string, " "), str_extract, pattern = '^[A-Z ]+$')
?apply
apply(str_split(string, " "), MARGIN = c(1, 2), FUN = str_extract, pattern = '^[A-Z ]+$')
apply(str_split(string, " "), MARGIN = 1, FUN = str_extract, pattern = '^[A-Z ]+$')
class(str_split(string, " "))
?lapply
lapply(str_split(string, " "), FUN = str_extract, pattern = '^[A-Z ]+$')
library(tokenizers)
?tokenize_sentences
tokenize_sentences(string)
tokenize_sentences("Thank you very, very much. CRIME--JUVENILE OFFENDERSCRIME--REPEAT OFFENDERSCRIME PREVENTION--COMMUNITY INVOLVEMENTCRIME VICTIMSCRIME, DRUG-RELATEDCRIMINAL SENTENCINGDRUG TRADE--FOREIGN COUNTRIESDRUG TRADE--GENERALEMPLOYMENT--YOUTHMEXICOSPORTS/ATHLETESCRIME--GENERAL")
tokenize_sentences("But the most important thing that I want to tell you is this: I have spoken of the world.  Along with Mrs. Nixon, we have visited most of the countries of the world, in fact, over eighty.  And every time you come back to the United States you know that the people fortunate enough to live here are the most fortunate people in the world.  This is a great country.  Let's never forget that.DIVERSITY/MULTICULTURALISMFOREIGN AFFAIRS--GENERALMEDICAL RESEARCH/ADVANCESMILITARY PREPAREDNESSNUCLEAR ARMS CONTROL/DISARMAMENT (COLD WAR ERA)PEOPLE'S REPUBLIC OF CHINASOVIET UNION (COLD WAR ERA)")
summary(lmp(in.deg ~ TENURE* LEVEL + AGE+ as.factor(DEPT), Krack_report_attributes , perm="Prob"))
summary(lmp(out.deg ~ TENURE*LEVEL + AGE+ as.factor(DEPT), Krack_report_attributes , perm="Prob"))
getwd()
knitr::opts_chunk$set(echo = TRUE)
#  Import adjacency matrix
Krack_report_matrix <- as.matrix(read.csv("Krack-Report-no-row (1).csv",header=TRUE,row.names=NULL,check.names=FALSE))
#  Create igraph object from this matrix
Krack_report_graph <- graph.adjacency(Krack_report_matrix,mode="directed",weighted=NULL)
knitr::opts_chunk$set(echo = TRUE)
library(igraph)
#  Import adjacency matrix
Krack_report_matrix <- as.matrix(read.csv("Krack-Report-no-row (1).csv",header=TRUE,row.names=NULL,check.names=FALSE))
#  Create igraph object from this matrix
Krack_report_graph <- graph.adjacency(Krack_report_matrix,mode="directed",weighted=NULL)
# import attribute data (as a data.frame)
Krack_report_attributes <- read.csv("Krack-Atts%2B_2_.csv", header=TRUE)
# attach the attributes to the Vertices of the igraph
vertex_attr(Krack_report_graph, index=Krack_report_attributes$ID) <- Krack_report_attributes
# caculate density
edge_density(Krack_report_graph)
# check the distribution of the numerical attributes
summary(Krack_report_attributes[, 2:3])
# for level (categorical)
table(Krack_report_attributes[,4])
# for Dept (categorical)
table(Krack_report_attributes[,5])
gn = edge.betweenness.community (Krack_report_graph, directed = TRUE)
head(gn)
plot(gn, Krack_report_graph)
memb = data.frame(gn$membership)
summary(memb)
# --- Random walk partitioning ---
walk = walktrap.community(Krack_report_graph)
head(walk)
plot(walk, dmg)
plot(walk, Krack_report_graph)
walk.memb = data.frame(walk$membership)
plot(gn, Krack_report_graph)
plot(walk, Krack_report_graph)
plot(gn, Krack_report_graph)
plot(walk, Krack_report_graph)
summary(memb)
head(gn)
head(walk)
head(gn)
Krack_report_graph
Krack_report_matrix
Krack_report_attributes
knitr::opts_chunk$set(echo = TRUE)
library(igraph)
?edge.betweenness.community
setwd("/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/performance_metrics/")
low_pride_metrics = read.csv("low_pride_pr_auc.csv")
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
roc_auc  = read.csv("low_pride_auc.csv")
roc_auc$model_iteration <- factor(roc_auc$model_iteration,
levels = c('Random Forest','Balanced RFC', 'RoBERTa -1', 'RoBERTa -2','RoBERTa -3'),ordered = TRUE)
roc_auc$colour = ifelse(roc_auc$model_iteration %in% c("Random Forest", "Balanced RFC"), 1, 0)
ggplot(roc_auc, aes(x = model_iteration, y = roc_auc, color =as.factor(colour))) +
geom_boxplot() + ggtitle("ROC AUC Across Different Classifers and Iterations, for Low Pride Dimension") + scale_color_manual(values = c("#52854C", "#D16103"))  + theme(legend.position = "none")
library(dplyr)
View(low_pride_metrics)
low_pride_metrics %>% group_by(model_iteration) %>% summarise(mean = mean())
low_pride_metrics %>% group_by(model_iteration) %>% summarise(mean = mean(PR_AUC))
low_pride_metrics %>% group_by(model_iteration) %>% summarise(max = max(PR_AUC))
pr_auc = read.csv("auth_pr_auc.csv")
pr_auc %>% group_by(model_iteration) %>% summarise(max = max(PR_AUC))
pr_auc %>% group_by(model_iteration) %>% summarise(max = max(pr_auc))
0.94 - 0.787
getwd()
setwd("/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/annotated_data/)
setwd("/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/annotated_data/)
setwd("/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/annotated_data/"
)
pop_1 = read.csv("Annotated_Iteration_1_sample_uncertainty_combined.csv")
pop_2 = read.csv("Annotated_Iteration_2_sample_uncertainty_bart.csv")
pop_3 = read.csv("annotated_par_bart_1_crossannotations_oscar.csv")
pop_4 = read.csv("annotated_par_bart_2_crossannotations_yuchen.csv")
pop_5 = read.csv("annotated_par_oscar_1_crossannotations_bart.csv")
pop_6 = read.csv("annotated_par_yuchen_1_crossannotations_bart.csv")
pop_7 = read.csv("annotated_par_yuchen_2_crossannotations_bart.csv")
pop_1$agreement
agreement = pop_1$agreement
pop_2$agreement = ifelse(pop_2$label == pop_2$yuchen_label, TRUE, FALSE)
agreement = rbind(agreement, pop_2$agreement)
agreement
agreement = pop_1$agreement
agreement = c(agreement, pop_2$agreement)
pop_3$agreement
agreement = c(agreement, pop_3$agreement)
pop_4$agreement = ifelse(pop_4$label == pop_4$label2, TRUE, FALSE)
agreement = c(agreement, pop_4$agreement)
pop_5$l1vsl2
agreement = c(agreement, pop_5$agreement)
pop_6$l1vsl2
agreement = c(agreement, pop_6$agreement)
pop_7$l1vsl2
agreement = c(agreement, pop_7$agreement)
table(pop_1$agreement)
table(pop_2$agreement)
table(pop_3$agreement)
table(pop_4$agreement)
table(pop_5$agreement)
table(pop_5$l1vsl2)
table(pop_6$l1vsl2)
table(pop_7$l1vsl2)
nat = read.csv("all_annotated_nat.csv")
nat$agreement
table(nat$agreement)
pride_auth1 = read.csv("pride_auth_bb_sample1_yl_crossannotated (1).csv")
table(pride_auth1$agree_high_pride)
pride_auth2 = read.csv("pride_auth_bb_sample2_yl_crossannotated (1).csv")
pride_auth2 = read.csv("pride_auth_bb_sample2_yl_crossannotated.csv")
table(pride_auth2$agree_high_pride)
pride_auth3 = read.csv("pride_auth_os-1_150_crossannotated_yl resolved.csv")
table(pride_auth3$high_pride_agree)
pride_auth4 = read.csv("pride_auth_yl_sample1_bb-revised (1).csv")
table(pride_auth4$high_pride_agree)
table(pride_auth4$agree_high_pride)
pride_auth5 = read.csv("pride_auth_yl_sample2_bb_crossannotated.csv")
table(pride_auth5$agree_high_pride)
table(pride_auth5$high_pride_agree)
pride_auth5$agree_hp = ifelse(pride_auth5$high_pride_yl == pride_auth5$high_pride_bb, 1, 0)
table(pride_auth5$agree_hp)
pride_auth5$agree_lp = ifelse(pride_auth5$low_pride_yl == pride_auth5$low_pride_bb, 1, 0)
table(pride_auth5$agree_lp)
table(pride_auth1$agree_low_pride)
table(pride_auth2$agree_low_pride)
table(pride_auth3$agree_low_pride)
table(pride_auth3$low_pride_agree)
table(pride_auth4$low_pride_agree)
table(pride_auth4$agree_low_pride)
table(pride_auth1$agree_auth)
table(pride_auth2$agree_auth)
table(pride_auth3$agree_auth)
table(pride_auth3$auth_agree)
table(pride_auth4$auth_agree)
table(pride_auth4$agree_auth)
pride_auth5$agree_auth = ifelse(pride_auth5$auth_yl == pride_auth5$auth_bb, 1, 0)
table(pride_auth5$agree_auth)
pride_auth6 = read.csv("pride_auth_250_bb_sample2 (1).csv")
pride_auth6$agree_hp = ifelse(pride_auth6$high_pride_bb == pride_auth6$high_pride_os, 1, 0)
pride_auth6$agree_lp = ifelse(pride_auth6$low_pride_bb == pride_auth6$low_pride_os, 1, 0)
pride_auth6$agree_auth = ifelse(pride_auth6$auth_bb == pride_auth6$auth_os, 1, 0)
table(pride_auth6$agree_hp)
pride_auth6 = read.csv("pride_auth_250_bb_sample2 (1).csv")
pride_auth6$agree_hp = ifelse(pride_auth6$high_pride_bb == pride_auth6$high_pride_os, 1, 0)
pride_auth6$agree_lp = ifelse(pride_auth6$low_pride_bb == pride_auth6$low_pride_os, 1, 0)
pride_auth6$agree_auth = ifelse(pride_auth6$auth_bb == pride_auth6$auth_os, 1, 0)
table(pride_auth6$agree_hp)
pride_auth6 = read.csv("pride_auth_250_bb_sample2 (1).csv")
pride_auth6$agree_hp = ifelse(pride_auth6$high_pride_bb == pride_auth6$high_pride_os, 1, 0)
pride_auth6$agree_lp = ifelse(pride_auth6$low_pride_bb == pride_auth6$low_pride_os, 1, 0)
pride_auth6$agree_auth = ifelse(pride_auth6$auth_bb == pride_auth6$auth_os, 1, 0)
table(pride_auth6$agree_hp)
table(pride_auth6$agree_lp)
table(pride_auth6$agree_auth)
pride_auth7 = read.csv("Iteration_1_sample_uncertainty_auth_bb_yl (1).csv")
table(pride_auth7$agree)
pride_auth7 = read.csv("Iteration_1_sample_uncertainty_highpride_bb_yl.csv")
table(pride_auth7$agree)
pride_auth7$agree_hp = ifelse(pride_auth7$high_pride_bb == pride_auth7$high_pride_os, 1, 0)
pride_auth7$agree_hp = ifelse(pride_auth7$high_pride_bb == pride_auth7$high_pride_yl, 1, 0)
table(pride_auth7$agree_hp)
pride_auth7 = read.csv("Iteration_1_sample_uncertainty_lowpride_bb_yl.csv")
table(pride_auth7$agree)
auth_8 = read.csv("Iteration_2_sample_uncertainty_auth_yl_bb_crossannotated.csv")
auth_8$agree = ifelse(auth_8$label_yl == auth_8$label_bb, 1, 0)
table(auth_8$agree
)
auth_9 = read.csv("Iteration_3_sample_uncertainty_auth_yl_crossannotated_os_resolved.csv")
auth_9$agree = ifelse(auth_9$label_yl == auth_9$label_bb, 1, 0)
auth_9$agree = ifelse(auth_9$label_yl == auth_9$label_os, 1, 0)
table(auth_9$agree)
high_pride7 = read.csv("Iteration_2_sample_uncertainty_high_pride_yl_bb.csv")
low_pride7 = read.csv("Iteration_2_sample_uncertainty_low_pride_yl_bb.csv")
high_pride7$agree = ifelse(high_pride7$label_yl == high_pride7$label_bb, 1, 0)
table(high_pride7$agree)
low_pride7$agree = ifelse(low_pride7$label_yl == low_pride7$label_bb, 1, 0)
table(low_pride7$agree)
inc_1 = read.csv("Iteration_1_inclusion_sample_uncertainty_bart_and_yuchen.csv")
inc_1
table(inc_1$X.1)
inc_2 = read.csv("Iteration_2_inclusion_sample_uncertainty_2_YL_cross_OS_resolved.csv")
table(inc_2$agreement)
exc_1 = read.csv("Iteration_1_sample_uncertainty_ex - BB_YL.csv")
table(exc_1$code_bb == exc_1$code_yl)
exc_2 = read.csv("Iteration_2_exclusion_uncertainty combined_resolved.csv")
table(exc_2$oscar_label == exc_2$yuchen_label)
