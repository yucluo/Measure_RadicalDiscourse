{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import random \n",
    "import os\n",
    "import csv\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "## for word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "from numpy import loadtxt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9931972789115646\n",
      "PR AUC: 0.5674242424242424\n",
      "AUC: 0.8945578231292517\n",
      "PR AUC: 0.15192972141472239\n",
      "AUC: 0.9965986394557823\n",
      "PR AUC: 0.5041666666666667\n",
      "AUC: 0.8090909090909092\n",
      "PR AUC: 0.575508716251412\n",
      "AUC: 0.8497732426303855\n",
      "PR AUC: 0.08748819396240921\n",
      "AUC: 0.9642857142857143\n",
      "PR AUC: 0.19059068333832482\n",
      "AUC: 0.9778911564625851\n",
      "PR AUC: 0.19095441595441595\n",
      "AUC: 0.9778911564625851\n",
      "PR AUC: 0.4058531746031746\n",
      "AUC: 0.8441043083900227\n",
      "PR AUC: 0.2768570150075726\n",
      "AUC: 0.9937641723356009\n",
      "PR AUC: 0.5583333333333333\n",
      "AUC: 0.973922902494331\n",
      "PR AUC: 0.20149711399711398\n",
      "AUC: 0.9801587301587301\n",
      "PR AUC: 0.28397177419354835\n",
      "AUC: 0.9897727272727272\n",
      "PR AUC: 0.3092948717948718\n",
      "AUC: 0.764172335600907\n",
      "PR AUC: 0.088080600293397\n",
      "AUC: 0.8180272108843537\n",
      "PR AUC: 0.32822180208272617\n",
      "AUC: 0.884920634920635\n",
      "PR AUC: 0.2718755787963547\n",
      "AUC: 0.8477272727272727\n",
      "PR AUC: 0.07983657473672641\n",
      "AUC: 0.9909297052154196\n",
      "PR AUC: 0.31117424242424246\n",
      "AUC: 0.9931972789115646\n",
      "PR AUC: 0.5354166666666667\n",
      "AUC: 0.8985260770975056\n",
      "PR AUC: 0.23332072797511536\n",
      "AUC: 0.8747165532879819\n",
      "PR AUC: 0.07301393761439903\n",
      "AUC: 0.9835600907029478\n",
      "PR AUC: 0.42836791831357046\n",
      "AUC: 0.992063492063492\n",
      "PR AUC: 0.3238095238095238\n",
      "AUC: 0.9852272727272727\n",
      "PR AUC: 0.23803104575163397\n",
      "AUC: 0.869047619047619\n",
      "PR AUC: 0.537111882370503\n",
      "Mean AUC: 0.9258849721706865\n",
      "Mean PR AUC: 0.3100852169510667\n",
      "[[441   0]\n",
      " [  3   1]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores = []\n",
    "pr_auc = []\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "# dat['speech_par_id'] = dat['Speech_id'].astype(str) + \"_\" + dat['par_id'].astype(str)\n",
    "# X = np.asarray([model.docvecs[i] for i in label_dat.index.tolist()])\n",
    "# Y = np.asarray(label_dat['label'].tolist(), dtype=\"int\")\n",
    "annotated1 = pd.read_csv(\"All_annotated_data_round_1_auth.csv\")\n",
    "annotated2 = pd.read_csv(\"All_annotated_data_round_2_auth.csv\")\n",
    "annotated = annotated1.append(annotated2)\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # pipeline BOW, tfidf and RF\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', RandomForestClassifier(n_estimators=5000, max_depth=3,max_features='sqrt', random_state=0, class_weight=\"balanced\")),\n",
    "                                                 ])\n",
    "    ## Fit the model to the training set\n",
    "    text_clf = text_clf.fit(train['text'], train['pred_class'])\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = text_clf.predict_proba(test['text'])\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(test['true_class'], preds[:,1], pos_label=1)\n",
    "    auc_scores = auc_scores + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(test['true_class'],  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    print(\"PR AUC: \" + str(lr_auc))\n",
    "    \n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores)))\n",
    "print(\"Mean PR AUC: \" + str(np.mean(pr_auc)))\n",
    "\n",
    "confusion = confusion_matrix(test['true_class'],text_clf.predict(test['text']) )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"auth_pr_auc_tfidf.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"auth_auc_tfidf.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
    "    ## clean (convert to lowercase and remove punctuations and characters and then strip)\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "            \n",
    "    ## Tokenize (convert from string to list)\n",
    "    lst_text = text.split()\n",
    "    ## remove Stopwords\n",
    "    if lst_stopwords is not None:\n",
    "        lst_text = [word for word in lst_text if word not in \n",
    "                    lst_stopwords]\n",
    "                \n",
    "    ## Stemming (remove -ing, -ly, ...)\n",
    "    if flg_stemm == True:\n",
    "        ps = nltk.stem.porter.PorterStemmer()\n",
    "        lst_text = [ps.stem(word) for word in lst_text]\n",
    "                \n",
    "    ## Lemmatisation (convert the word into root word)\n",
    "    if flg_lemm == True:\n",
    "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        lst_text = [lem.lemmatize(word) for word in lst_text]\n",
    "            \n",
    "    ## back to string from list\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "\n",
    "lst_stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "dat[\"text_clean\"] = dat[\"text\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=True, flg_lemm=True,lst_stopwords=lst_stopwords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75978,)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[\"text_clean\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases1 = Phrases(map(lambda x: x.split(), dat[\"text_clean\"].tolist())) #bigram\n",
    "phrases2 = Phrases(phrases1[map(lambda x: x.split(), dat[\"text_clean\"].tolist())]) #trigram\n",
    "dat[\"phrased_text\"] = dat[\"text_clean\"].apply(lambda x: \" \".join(phrases2[phrases1[x.split()]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    start talk economi best get right point jimmi_...\n",
       "1    secret group hit_hardest mr_carter inflationar...\n",
       "2    elderli work_hard enjoy retir year expect surv...\n",
       "3    believ social_secur one nation vital commit se...\n",
       "4    contrast commit econom program reduc inflat pu...\n",
       "Name: phrased_text, dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['phrased_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat['phrased_tokens'] = dat.apply(lambda row: nltk.word_tokenize(row['phrased_text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Word2Vec (averaged within paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit Word2Vec model\n",
    "nlp = gensim.models.word2vec.Word2Vec(dat['phrased_tokens'], size=150, window=10, min_count=10, negative=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model\n",
    "nlp.wv.most_similar('tax') \n",
    "\n",
    "# save model\n",
    "nlp.save(\"word2vec_wordvecs.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# start empty matrix that hosts embeddings for each para, averaged from all words in the para\n",
    "# number of para x length of embedding (150)\n",
    "avg_embeddings = np.zeros((len(dat['phrased_tokens']), 150))\n",
    "\n",
    "# load model\n",
    "nlp = Word2Vec.load(\"word2vec_wordvecs.model\") \n",
    "#iterate through each para (rows of the df)\n",
    "for index, row in dat.iterrows():\n",
    "    # each row is a paragraph of tokens\n",
    "    tokens = row[\"phrased_tokens\"]\n",
    "    # start an empty embedding matrix for all tokens in a para\n",
    "    # number of tokens x 150\n",
    "    tokens_embedding = np.zeros((len(tokens), 150))\n",
    "    # loop through each token to delete non embedded tokens\n",
    "    for i in range(0, len(tokens)):\n",
    "        try:\n",
    "            tokens_embedding[i] = nlp[tokens[i]] # fill the matrix with word embedding\n",
    "        except:\n",
    "            pass # leave as 0 if the token is not in the model\n",
    "    avg_embedding = np.average(tokens_embedding, axis = 0) # average within para\n",
    "    avg_embeddings[index] = avg_embedding # fill the main matrix\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_embeddings.shape\n",
    "savetxt('avg_embeddings.csv', avg_embeddings, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Word2Vec RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8134920634920635\n",
      "PR AUC: 0.06880285045563078\n",
      "AUC: 0.9670454545454544\n",
      "PR AUC: 0.3824015022675737\n",
      "AUC: 0.9484126984126984\n",
      "PR AUC: 0.07441325014854426\n",
      "AUC: 0.9336734693877551\n",
      "PR AUC: 0.14153649498076742\n",
      "AUC: 0.8931818181818181\n",
      "PR AUC: 0.3174445552289614\n",
      "AUC: 0.9931972789115646\n",
      "PR AUC: 0.6822115384615385\n",
      "AUC: 0.75\n",
      "PR AUC: 0.02583622166394559\n",
      "AUC: 0.8741496598639457\n",
      "PR AUC: 0.05100745836153926\n",
      "AUC: 0.9019274376417233\n",
      "PR AUC: 0.04129261852899963\n",
      "AUC: 0.8713151927437642\n",
      "PR AUC: 0.06722602286607539\n",
      "AUC: 0.9818181818181819\n",
      "PR AUC: 0.179229797979798\n",
      "AUC: 0.8798185941043084\n",
      "PR AUC: 0.5124690750842051\n",
      "AUC: 0.9142045454545454\n",
      "PR AUC: 0.07708309665622583\n",
      "AUC: 0.9223356009070296\n",
      "PR AUC: 0.16904357304697168\n",
      "AUC: 0.9625850340136054\n",
      "PR AUC: 0.15420747730530338\n",
      "AUC: 0.8015873015873015\n",
      "PR AUC: 0.34568201591105896\n",
      "AUC: 0.9965986394557823\n",
      "PR AUC: 0.6889880952380952\n",
      "AUC: 0.9255681818181818\n",
      "PR AUC: 0.5247532766080263\n",
      "AUC: 0.9710884353741496\n",
      "PR AUC: 0.5552009246088194\n",
      "AUC: 0.9756235827664399\n",
      "PR AUC: 0.1686091463837459\n",
      "AUC: 0.9903409090909091\n",
      "PR AUC: 0.47501352813852815\n",
      "AUC: 0.9183673469387755\n",
      "PR AUC: 0.3013753127243226\n",
      "AUC: 0.7676136363636363\n",
      "PR AUC: 0.02559718280573614\n",
      "AUC: 0.8752834467120182\n",
      "PR AUC: 0.07340208493650259\n",
      "AUC: 0.9807256235827664\n",
      "PR AUC: 0.38130411255411256\n",
      "Mean AUC: 0.9123981653267369\n",
      "Mean PR AUC: 0.2593652485178011\n",
      "[[430  11]\n",
      " [  2   2]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores = []\n",
    "pr_auc = []\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "\n",
    "annotated1 = pd.read_csv(\"All_annotated_data_round_1_auth_20220209.csv\")\n",
    "annotated2 = pd.read_csv(\"All_annotated_data_round_2_auth.csv\")\n",
    "annotated = annotated1.append(annotated2)\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "avg_embeddings = loadtxt('avg_embeddings.csv', delimiter=',')\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # attach train/test set to original df\n",
    "    train_id = train['speech_par_id']\n",
    "    test_id = test['speech_par_id']    \n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test_id), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train_id), 1, 0)\n",
    "    # get the embeddings to train/test set\n",
    "    train_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(train_id)].index.tolist()])\n",
    "    test_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(test_id)].index.tolist()])\n",
    "    ## Fit the model to the training set\n",
    "    text_clf = RandomForestClassifier(n_estimators=500, max_depth=3, random_state=0, max_features = \"sqrt\", class_weight=\"balanced\")\n",
    "    text_clf.fit(train_vec,  dat[dat['train']==1].pred_class) # use the original df to match the order\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = text_clf.predict_proba(test_vec)\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores = auc_scores + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    print(\"PR AUC: \" + str(lr_auc))\n",
    "    \n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores)))\n",
    "print(\"Mean PR AUC: \" + str(np.mean(pr_auc)))\n",
    "\n",
    "confusion = confusion_matrix(dat[dat['test']==1].pred_class,text_clf.predict(test_vec) )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"auth_pr_auc_w2v.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"auth_auc_w2v.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = gensim_api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# dat[\"text_clean_pretrain\"] = dat[\"text\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=False,lst_stopwords=lst_stopwords))\n",
    "# dat['text_pretrain_token'] = dat.apply(lambda row: nltk.word_tokenize(row['text_clean_pretrain']), axis=1)\n",
    "# # start empty matrix that hosts embeddings for each para, averaged from all words in the para\n",
    "# # number of para x length of embedding (150)\n",
    "# avg_embeddings = np.zeros((len(dat['text_pretrain_token']), 300))\n",
    "\n",
    "# #iterate through each para (rows of the df)\n",
    "# for index, row in dat.iterrows():\n",
    "#     # each row is a paragraph of tokens\n",
    "#     tokens = row[\"text_pretrain_token\"]\n",
    "#     # start an empty embedding matrix for all tokens in a para\n",
    "#     # number of tokens x 150\n",
    "#     tokens_embedding = np.zeros((len(tokens), 300))\n",
    "#     # loop through each token to delete non embedded tokens\n",
    "#     for i in range(0, len(tokens)):\n",
    "#         try:\n",
    "#             tokens_embedding[i] = nlp[tokens[i]] # fill the matrix with word embedding\n",
    "#         except:\n",
    "#             pass # leave as 0 if the token is not in the model\n",
    "#     avg_embedding = np.average(tokens_embedding, axis = 0) # average within para\n",
    "#     avg_embeddings[index] = avg_embedding # fill the main matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savetxt('avg_embeddings_pretrained.csv', avg_embeddings, delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "AUC: 0.9920634920634921\n",
      "PR AUC: 0.37387716450216446\n",
      "Mean AUC: 0.9920634920634922\n",
      "Mean PR AUC: 0.37387716450216446\n",
      "[[434   7]\n",
      " [  1   3]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores = []\n",
    "pr_auc = []\n",
    "\n",
    "dat = pd.read_csv(\"20220131_all_paragraphs_2020_added_missings_added.csv\")\n",
    "annotated1 = pd.read_csv(\"All_annotated_data_round_1_auth_20220209.csv\")\n",
    "annotated2 = pd.read_csv(\"All_annotated_data_round_2_auth.csv\")\n",
    "annotated = annotated1.append(annotated2)\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "avg_embeddings = loadtxt('avg_embeddings_pretrained.csv', delimiter=',')\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # attach train/test set to original df\n",
    "    train_text = train['speech_par_id']\n",
    "    test_text = test['speech_par_id']    \n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test_id), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train_id), 1, 0)\n",
    "    # get the embeddings to train/test set\n",
    "    train_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(train_id)].index.tolist()])\n",
    "    test_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(test_id)].index.tolist()])\n",
    "    ## Fit the model to the training set\n",
    "    text_clf = RandomForestClassifier(n_estimators=500, max_depth=3, random_state=0, max_features = \"sqrt\", class_weight=\"balanced\")\n",
    "    text_clf.fit(train_vec,  dat[dat['train']==1].pred_class) # use the original df to match the order\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = text_clf.predict_proba(test_vec)\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores = auc_scores + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    print(\"PR AUC: \" + str(lr_auc))\n",
    "    \n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores)))\n",
    "print(\"Mean PR AUC: \" + str(np.mean(pr_auc)))\n",
    "\n",
    "confusion = confusion_matrix(dat[dat['test']==1].pred_class,text_clf.predict(test_vec) )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"auth_pr_auc_pretrained.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"auth_auc_pretrained.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9278409090909091\n",
      "Accuracy: 0.990990990990991\n",
      "PR_AUC: 0.33866672462333497\n",
      "AUC: 0.9083143507972666\n",
      "Accuracy: 0.9909706546275395\n",
      "PR_AUC: 0.7553168883961567\n",
      "AUC: 0.9552154195011338\n",
      "Accuracy: 0.9887640449438202\n",
      "PR_AUC: 0.12715698393902056\n",
      "AUC: 0.9994331065759637\n",
      "Accuracy: 0.9910112359550561\n",
      "PR_AUC: 0.94375\n",
      "AUC: 0.9107954545454545\n",
      "Accuracy: 0.990990990990991\n",
      "PR_AUC: 0.29620811466055585\n",
      "AUC: 0.9937641723356009\n",
      "Accuracy: 0.9910112359550561\n",
      "PR_AUC: 0.8101190476190476\n",
      "AUC: 0.9246031746031745\n",
      "Accuracy: 0.9910112359550561\n",
      "PR_AUC: 0.30775305068100794\n",
      "AUC: 0.9573863636363638\n",
      "Accuracy: 0.990990990990991\n",
      "PR_AUC: 0.39699561403508765\n",
      "AUC: 0.9478458049886621\n",
      "Accuracy: 0.9910112359550561\n",
      "PR_AUC: 0.2409233977070789\n",
      "AUC: 0.8704545454545454\n",
      "Accuracy: 0.990990990990991\n",
      "PR_AUC: 0.5318037259405952\n",
      "AUC: 1.0\n",
      "Accuracy: 0.9909706546275395\n",
      "PR_AUC: 1.0\n",
      "AUC: 0.98125\n",
      "Accuracy: 0.990990990990991\n",
      "PR_AUC: 0.22192852437417654\n",
      "AUC: 0.9390660592255125\n",
      "Accuracy: 0.9909706546275395\n",
      "PR_AUC: 0.30090376220140286\n",
      "AUC: 0.9840909090909091\n",
      "Accuracy: 0.990990990990991\n",
      "PR_AUC: 0.643134236453202\n",
      "AUC: 0.9580498866213152\n",
      "Accuracy: 0.9910112359550561\n",
      "PR_AUC: 0.6885110503531556\n",
      "AUC: 0.9214123006833712\n",
      "Accuracy: 0.9909706546275395\n",
      "PR_AUC: 0.26483554449972363\n",
      "AUC: 0.9909297052154196\n",
      "Accuracy: 0.9910112359550561\n",
      "PR_AUC: 0.7947368421052632\n",
      "AUC: 0.9329545454545455\n",
      "Accuracy: 0.990990990990991\n",
      "PR_AUC: 0.37964257637017074\n",
      "AUC: 0.9426136363636364\n",
      "Accuracy: 0.990990990990991\n",
      "PR_AUC: 0.37350453339230427\n",
      "AUC: 0.9801587301587301\n",
      "Accuracy: 0.9910112359550561\n",
      "PR_AUC: 0.5726768968456948\n",
      "AUC: 0.9994318181818181\n",
      "Accuracy: 0.990990990990991\n",
      "PR_AUC: 0.94375\n",
      "AUC: 0.9568181818181819\n",
      "Accuracy: 0.990990990990991\n",
      "PR_AUC: 0.5364626169616783\n",
      "AUC: 0.95625\n",
      "Accuracy: 0.990990990990991\n",
      "PR_AUC: 0.10599246183923604\n",
      "AUC: 0.961451247165533\n",
      "Accuracy: 0.9910112359550561\n",
      "PR_AUC: 0.3595773759461732\n",
      "AUC: 0.9654195011337868\n",
      "Accuracy: 0.9910112359550561\n",
      "PR_AUC: 0.44093172551278575\n",
      "Mean AUC: 0.9546219929056734\n",
      "Mean Accuracy: 0.9909059475180154\n",
      "[[441   0]\n",
      " [  4   0]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores_d2v = []\n",
    "pr_auc = []\n",
    "accuracy_scores_d2v = []\n",
    "\n",
    "dat = pd.read_csv(\"20201115_all_paragraphs.csv\")\n",
    "dat['speech_par_id'] = dat['Speech_id'].astype(str) + \"_\" + dat['par_id'].astype(str)\n",
    "annotated1 = pd.read_csv(\"All_annotated_data_round_1_auth_20220209.csv\")\n",
    "annotated2 = pd.read_csv(\"All_annotated_data_round_2_auth.csv\")\n",
    "annotated = annotated1.append(annotated2)\n",
    "\n",
    "model = Word2Vec.load(\"doc2vec_wordvecs.model\") \n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # merge test and train sets back onto the total df\n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test[\"speech_par_id\"]), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train[\"speech_par_id\"]), 1, 0)\n",
    "    # so that teh order of rows are the same with the embeddings\n",
    "    test_set = np.asarray([model.docvecs[i] for i in dat[dat['test'] == 1].index.tolist()])\n",
    "    train_set = np.asarray([model.docvecs[i] for i in dat[dat['train']== 1].index.tolist()])\n",
    "    ## Initialize a random forest classifier\n",
    "    gbc = RandomForestClassifier(n_estimators=500, max_depth=10, random_state=0, max_features = \"sqrt\", class_weight=\"balanced\")\n",
    "    ## Fit the model to the training set\n",
    "    gbc.fit(train_set, dat[dat['train']==1].pred_class)\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = gbc.predict_proba(test_set)\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores_d2v = auc_scores_d2v + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    accuracy_d2v = metrics.accuracy_score(dat[dat['test']==1].pred_class, gbc.predict(test_set))\n",
    "    accuracy_scores_d2v = accuracy_scores_d2v + [accuracy_d2v]\n",
    "    print(\"Accuracy: \" + str(accuracy_d2v))\n",
    "    print(\"PR_AUC: \" + str(lr_auc))\n",
    "\n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores_d2v)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_d2v)))\n",
    "\n",
    "confusion = confusion_matrix(dat[dat['test']==1].pred_class, gbc.predict(test_set))\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"auth_pr_auc_d2v.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"auth_auc_d2v.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores_d2v:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC-AUC: 0.9546219929056734\n",
      "Mean Accuracy: 0.9909059475180154\n",
      "Mean PR_AUC: 0.49501126777827403\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean ROC-AUC: \" + str(np.mean(auc_scores_d2v)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_d2v)))\n",
    "print(\"Mean PR_AUC: \" + str(np.mean(pr_auc)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balanced RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9494318181818182\n",
      "Accuracy: 0.9301801801801802\n",
      "PR_AUC: 0.315367724012682\n",
      "AUC: 0.9555808656036446\n",
      "Accuracy: 0.963882618510158\n",
      "PR_AUC: 0.7607271906052394\n",
      "AUC: 0.9671201814058957\n",
      "Accuracy: 0.946067415730337\n",
      "PR_AUC: 0.1567598505098505\n",
      "AUC: 1.0\n",
      "Accuracy: 0.946067415730337\n",
      "PR_AUC: 1.0\n",
      "AUC: 0.9357954545454545\n",
      "Accuracy: 0.9301801801801802\n",
      "PR_AUC: 0.3079714398711176\n",
      "AUC: 0.985827664399093\n",
      "Accuracy: 0.9280898876404494\n",
      "PR_AUC: 0.780634236453202\n",
      "AUC: 0.917233560090703\n",
      "Accuracy: 0.9483146067415731\n",
      "PR_AUC: 0.2986927382339074\n",
      "AUC: 0.9693181818181819\n",
      "Accuracy: 0.9436936936936937\n",
      "PR_AUC: 0.4014520202020202\n",
      "AUC: 0.953514739229025\n",
      "Accuracy: 0.9303370786516854\n",
      "PR_AUC: 0.2435840033994862\n",
      "AUC: 0.9022727272727273\n",
      "Accuracy: 0.9481981981981982\n",
      "PR_AUC: 0.5236968723764499\n",
      "AUC: 1.0\n",
      "Accuracy: 0.9322799097065463\n",
      "PR_AUC: 1.0\n",
      "AUC: 0.9818181818181818\n",
      "Accuracy: 0.9504504504504504\n",
      "PR_AUC: 0.2951505016722408\n",
      "AUC: 0.9518792710706151\n",
      "Accuracy: 0.9390519187358917\n",
      "PR_AUC: 0.3274140211640212\n",
      "AUC: 0.9812500000000001\n",
      "Accuracy: 0.9211711711711712\n",
      "PR_AUC: 0.6385695187165775\n",
      "AUC: 0.9659863945578232\n",
      "Accuracy: 0.9325842696629213\n",
      "PR_AUC: 0.7637648809523809\n",
      "AUC: 0.9561503416856492\n",
      "Accuracy: 0.9142212189616253\n",
      "PR_AUC: 0.3157456140350877\n",
      "AUC: 0.9909297052154196\n",
      "Accuracy: 0.9640449438202248\n",
      "PR_AUC: 0.7947368421052632\n",
      "AUC: 0.959659090909091\n",
      "Accuracy: 0.9414414414414415\n",
      "PR_AUC: 0.4503842213114754\n",
      "AUC: 0.9585227272727272\n",
      "Accuracy: 0.9527027027027027\n",
      "PR_AUC: 0.43690191098209963\n",
      "AUC: 0.9665532879818594\n",
      "Accuracy: 0.946067415730337\n",
      "PR_AUC: 0.549034783046411\n",
      "AUC: 1.0\n",
      "Accuracy: 0.9279279279279279\n",
      "PR_AUC: 1.0\n",
      "AUC: 0.9267045454545454\n",
      "Accuracy: 0.9572072072072072\n",
      "PR_AUC: 0.5220404908106385\n",
      "AUC: 0.9624999999999999\n",
      "Accuracy: 0.9324324324324325\n",
      "PR_AUC: 0.11046924351877913\n",
      "AUC: 0.9727891156462584\n",
      "Accuracy: 0.9415730337078652\n",
      "PR_AUC: 0.45213034978112115\n",
      "AUC: 0.9699546485260772\n",
      "Accuracy: 0.9370786516853933\n",
      "PR_AUC: 0.4494835060624534\n",
      "Mean AUC: 0.9632317001073916\n",
      "Mean Accuracy: 0.9402098388240371\n"
     ]
    }
   ],
   "source": [
    "auc_scores_balanced = []\n",
    "pr_auc_balanced = []\n",
    "accuracy_scores_balanced = []\n",
    "\n",
    "np.random.seed(234) \n",
    "random.seed(234)\n",
    "\n",
    "dat = pd.read_csv(\"20201115_all_paragraphs.csv\")\n",
    "dat['speech_par_id'] = dat['Speech_id'].astype(str) + \"_\" + dat['par_id'].astype(str)\n",
    "# X = np.asarray([model.docvecs[i] for i in label_dat.index.tolist()])\n",
    "# Y = np.asarray(label_dat['label'].tolist(), dtype=\"int\")\n",
    "model = Word2Vec.load(\"doc2vec_wordvecs.model\") \n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "annotated1 = pd.read_csv(\"All_annotated_data_round_1_auth_20220209.csv\")\n",
    "annotated2 = pd.read_csv(\"All_annotated_data_round_2_auth.csv\")\n",
    "annotated = annotated1.append(annotated2)\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_auth/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # merge test and train sets back onto the total df\n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test[\"speech_par_id\"]), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train[\"speech_par_id\"]), 1, 0)\n",
    "    # so that teh order of rows are the same with the embeddings\n",
    "    test_set = np.asarray([model.docvecs[i] for i in dat[dat['test'] == 1].index.tolist()])\n",
    "    train_set = np.asarray([model.docvecs[i] for i in dat[dat['train']== 1].index.tolist()])\n",
    "    ## Initialize a random forest classifier\n",
    "    brfc = BalancedRandomForestClassifier(n_estimators=5000, max_depth=10, random_state=0, max_features = \"sqrt\")\n",
    "    ## Fit the model to the training set\n",
    "    brfc.fit(train_set, dat[dat['train']==1].pred_class)\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = brfc.predict_proba(test_set)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores_balanced = auc_scores_balanced + [metrics.auc(fpr, tpr)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc_balanced = pr_auc_balanced + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr, tpr)))\n",
    "    accuracy_balanced = metrics.accuracy_score(dat[dat['test']==1].pred_class, brfc.predict(test_set))\n",
    "    accuracy_scores_balanced = accuracy_scores_balanced + [accuracy_balanced]\n",
    "    print(\"Accuracy: \" + str(accuracy_balanced))\n",
    "    print(\"PR_AUC: \" + str(lr_auc))\n",
    "\n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores_balanced)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"auth_pr_auc_balanced.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc_balanced:\n",
    "        csvwriter.writerow([row])    \n",
    "\n",
    "with open(\"auth_auc_balanced.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores_balanced:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC-AUC: 0.9632317001073916\n",
      "Mean Accuracy: 0.9402098388240371\n",
      "Mean PR_AUC: 0.5157884783929002\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean ROC-AUC: \" + str(np.mean(auc_scores_balanced)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_balanced)))\n",
    "print(\"Mean PR_AUC: \" + str(np.mean(pr_auc_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[421  20]\n",
      " [  3   1]]\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(dat[dat['test']==1].pred_class, brfc.predict(test_set))\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "dat = pd.read_csv('20220131_all_paragraphs_2020_added_missings_added.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Speech_id</th>\n",
       "      <th>text</th>\n",
       "      <th>party</th>\n",
       "      <th>term</th>\n",
       "      <th>comp</th>\n",
       "      <th>populist_old_keywords</th>\n",
       "      <th>par_id</th>\n",
       "      <th>speech_par_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10590</th>\n",
       "      <td>10591</td>\n",
       "      <td>1013</td>\n",
       "      <td>Well, the best answer I know to Republican fea...</td>\n",
       "      <td>dem</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>1013_6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 Speech_id  \\\n",
       "10590       10591      1013   \n",
       "\n",
       "                                                    text party    term   comp  \\\n",
       "10590  Well, the best answer I know to Republican fea...   dem  1952.0  False   \n",
       "\n",
       "      populist_old_keywords  par_id speech_par_id  \n",
       "10590                 False       6        1013_6  "
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[dat[\"speech_par_id\"] == \"1013_6\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2423"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_ids = []\n",
    "for id in annotated['speech_par_id']:\n",
    "    if id not in dat['speech_par_id']:\n",
    "        missing_ids = missing_ids + [id]\n",
    "        \n",
    "len(missing_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (5,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "dat = pd.read_csv(\"20220131_all_paragraphs_2020_added_missings_added.csv\")\n",
    "pop = pd.read_csv(\"All_annotated_data_populism.csv\")\n",
    "low_pr = pd.read_csv(\"All_annotated_data_round_1_low_pride.csv\")\n",
    "high_pr = pd.read_csv(\"All_annotated_data_round_1_high_pride.csv\")\n",
    "auth = pd.read_csv(\"All_annotated_data_round_1_auth.csv\")\n",
    "exc = pd.read_csv(\"All_annotated_data_exclusion_LO_recodedd.csv\")\n",
    "inc = pd.read_csv(\"All_annotated_data_inclusion.csv\")\n",
    "\n",
    "dat = dat.merge(pop[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "dat = dat.rename({'pred_class': 'pop'}, axis=1)\n",
    "dat = dat.merge(low_pr[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "dat = dat.rename({'pred_class': 'low_pr'}, axis=1)\n",
    "dat = dat.merge(high_pr[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "dat = dat.rename({'pred_class': 'high_pr'}, axis=1)\n",
    "dat = dat.merge(auth[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "dat = dat.rename({'pred_class': 'auth'}, axis=1)\n",
    "dat = dat.merge(exc[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "dat = dat.rename({'pred_class': 'exc'}, axis=1)\n",
    "dat = dat.merge(inc[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "dat = dat.rename({'pred_class': 'inc'}, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat[dat['pop'] + dat['low_pr'] + dat['high_pr']+ dat[\"auth\"] +dat[\"exc\"] + dat[\"inc\"] >1].to_csv(\"multiframe_pars.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_pred = pd.read_csv(\"pop_Iteration_3_predictions10239_1e-05_5_4_1_missings_merged.csv\")\n",
    "auth_pred = pd.read_csv(\"auth_Iteration_4_predictions10239_1e-05_5_4_1_missings_merged.csv\")\n",
    "exc_pred = pd.read_csv(\"exclusion_Iteration_3_predictions10239_1e-05_5_4_1_missings_merged.csv\")\n",
    "inc_pred = pd.read_csv(\"inclusion_Iteration_3_predictions10239_1e-05_5_4_1_missings_merged.csv\")\n",
    "high_pr_pred = pd.read_csv(\"high_pride_Iteration_3_predictions10239_1e-05_5_4_1_missings_merged.csv\")\n",
    "low_pr_pred = pd.read_csv(\"low_pride_Iteration_3_predictions10239_1e-05_5_4_1_missings_merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dat = pop_pred.merge(low_pr_pred[[\"speech_par_id\", \"Predictions_prob_1\"]], how = \"left\", on = \"speech_par_id\")\n",
    "pred_dat = pred_dat.rename({'Predictions_prob_1_x': 'pop'}, axis=1)\n",
    "pred_dat = pred_dat.rename({'Predictions_prob_1_y': 'low_pr'}, axis=1)\n",
    "pred_dat = pred_dat.merge(high_pr_pred[[\"speech_par_id\", \"Predictions_prob_1\"]], how = \"left\", on = \"speech_par_id\")\n",
    "pred_dat = pred_dat.rename({'Predictions_prob_1': 'high_pr'}, axis=1)\n",
    "pred_dat = pred_dat.merge(auth_pred[[\"speech_par_id\", \"Predictions_prob_1\"]], how = \"left\", on = \"speech_par_id\")\n",
    "pred_dat = pred_dat.rename({'Predictions_prob_1': 'auth'}, axis=1)\n",
    "pred_dat = pred_dat.merge(exc_pred[[\"speech_par_id\", \"Predictions_prob_1\"]], how = \"left\", on = \"speech_par_id\")\n",
    "pred_dat = pred_dat.rename({'Predictions_prob_1': 'exc'}, axis=1)\n",
    "pred_dat = pred_dat.merge(inc_pred[[\"speech_par_id\", \"Predictions_prob_1\"]], how = \"left\", on = \"speech_par_id\")\n",
    "pred_dat = pred_dat.rename({'Predictions_prob_1': 'inc'}, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dat[pred_dat['pop'] + pred_dat['low_pr'] + pred_dat['high_pr']+ pred_dat[\"auth\"] + pred_dat[\"exc\"] + pred_dat[\"inc\"] >2].to_csv(\"multiframes_BERT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
