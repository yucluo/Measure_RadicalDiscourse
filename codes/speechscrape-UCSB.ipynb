{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base URL for scrapping\n",
    "root = 'https://www.presidency.ucsb.edu'\n",
    "\n",
    "# search based on name, title, and date\n",
    "# date is between 1) receives party nomination and 2) election day \n",
    "cand_dict = {1: {'first': 'George-w', 'last': 'Bush', 'title': 'President', 'start': date(2000, 8, 3), 'end': date(2000, 12, 12)},#2000\n",
    "             2: {'first': 'Albert', 'last': 'Gore-jr', 'title': 'VP', 'start': date(2000, 8, 17), 'end': date(2000, 12, 12)},\n",
    "             3: {'first': 'George-w', 'last': 'Bush', 'title': 'President', 'start': date(2004, 9, 2), 'end': date(2004, 11, 2)}, #2004\n",
    "             4: {'first': 'John-f', 'last': 'Kerry', 'title': 'Senator', 'start': date(2004, 7, 29), 'end': date(2004, 11, 2)},\n",
    "             5: {'first': 'Barack', 'last': 'Obama', 'title': 'President', 'start': date(2008, 8, 28), 'end': date(2008, 11, 3)},#2008\n",
    "             6: {'first': 'John', 'last': 'McCain', 'title': 'Senator', 'start': date(2008, 9, 4), 'end': date(2008, 11, 3)}, \n",
    "             7: {'first': 'Barack', 'last': 'Obama', 'title': 'President', 'start': date(2012, 9, 6), 'end': date(2012, 11, 6)},#2012\n",
    "             8: {'first': 'Mitt', 'last': 'Romney', 'title': 'Governor', 'start': date(2012, 8, 30), 'end': date(2012, 11, 6)},\n",
    "             9: {'first': 'Donald-j', 'last': 'Trump', 'title': 'President', 'start': date(2016, 7, 21), 'end': date(2016, 11, 8)}, #2016\n",
    "             10: {'first': 'Hillary', 'last': 'Clinton', 'title': 'Secretary', 'start': date(2016, 7, 28), 'end': date(2016, 11, 8)}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape, write txt and meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cand in range(1,11):  \n",
    "    #create candidate specific url string\n",
    "    if cand_dict[cand]['title'] == \"President\":\n",
    "        candidate = '/people/president/' + cand_dict[cand]['first'] + '-' + cand_dict[cand]['last'] + '?page=' \n",
    "    else:\n",
    "        candidate = '/people/other/' + cand_dict[cand]['first'] + '-' + cand_dict[cand]['last'] + '?page=' \n",
    "    candidate_iter = 0\n",
    "\n",
    "    speech_path = 'speeches_' + cand_dict[cand]['last'].lower() + '_ucsb'\n",
    "    try:  \n",
    "        os.mkdir(speech_path)\n",
    "    except OSError:\n",
    "        pass\n",
    "    # first go into candidate page\n",
    "    r = requests.get(root + candidate.lower() + str(candidate_iter))\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    # locate the final page\n",
    "    try: \n",
    "        fin = soup.find_all('a', title = 'Go to last page')\n",
    "     # find max page number\n",
    "        max_p = int(re.findall(r'page=([0-9]+)', str(fin[0]))[0])\n",
    "    except: \n",
    "        max_p = 1\n",
    "\n",
    "\n",
    "    # initialize metadata dict\n",
    "    metadata = defaultdict(list)\n",
    "\n",
    "    for iter in range(candidate_iter, max_p + 1):\n",
    "\n",
    "        r = requests.get(root + candidate.lower() + str(iter))\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        titles_raw = soup.findAll(class_=\"views-field views-field-title\")\n",
    "        dates_raw = soup.findAll(class_=\"date-display-single\")\n",
    "\n",
    "        \n",
    "        links = []\n",
    "        for x in titles_raw:\n",
    "            links.append(x.a['href'])\n",
    "        titles = []\n",
    "        for x in titles_raw:\n",
    "            titles.append(x.a.contents[0])\n",
    "        dates = []\n",
    "        for x in dates_raw: \n",
    "            date_raw = re.findall(r'content=\"([0-9]{4})-([0-9]{2})-([0-9]{2})', str(x))\n",
    "#             print(int(date_raw[0][2]))\n",
    "            if int(date_raw[0][0]) < 1980:\n",
    "                continue\n",
    "            speechdate = date(int(date_raw[0][0]), int(date_raw[0][1]), int(date_raw[0][2]))\n",
    "            dates.append(speechdate)\n",
    "\n",
    "        assert len(links) == len(titles) == len(dates)\n",
    "\n",
    "        lengthbefore = len(metadata)\n",
    "\n",
    "        for i in range(len(metadata), len(metadata) + len(links)):\n",
    "            metadata[i] = [links[i - lengthbefore], titles[i - lengthbefore], dates[i - lengthbefore]]\n",
    "            \n",
    "    ### save all speeches in csv files\n",
    "#     with open('metadata_all' + str(cand_dict[cand]['last']) + str(cand_dict[cand]['start'])[2:4] + '.csv', 'w') as csvfile:  \n",
    "#     # creating a csv writer object  \n",
    "#         csvwriter = csv.writer(csvfile)     \n",
    "#     # writing the data rows  \n",
    "#         csvwriter.writerows(metadata.values()) \n",
    "\n",
    "### start extracting speeches that fit our parameters\n",
    "    speechnum = 0\n",
    "    \n",
    "    meta = [] # also save meta of those speeches separately \n",
    "    for i in range(0, len(metadata)):\n",
    "        # only keep within the pre-specified dates, and only if it comes with \"remarks\"\n",
    "        if metadata[i][2] >= cand_dict[cand]['start'] and metadata[i][2] <= cand_dict[cand]['end'] and any(re.match(regex, metadata[i][1]) for regex in ['[Rr]emarks','[Aa]ddress']):\n",
    "            speechnum += 1\n",
    "            meta.append(metadata[i])\n",
    "\n",
    "            r = requests.get(root + metadata[i][0])\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            text_full = soup.findAll(class_='field-docs-content')[0].get_text()\n",
    "            text = text_full\n",
    "            try:\n",
    "                filename = cand_dict[cand]['last'].lower() + '-speech-' + str(metadata[i][2]) + '-' + re.findall('[\\w]+-[\\w]+-[\\w]+$', metadata[i][0])[0]\n",
    "                print(filename)\n",
    "            except:  \n",
    "                continue\n",
    "            with open(os.getcwd() + '/' + speech_path + '/' + filename + '.txt', 'w') as text_file:\n",
    "                text_file.write(text)\n",
    "        \n",
    "    with open(\"META\" + str(cand_dict[cand]['last']) + str(cand_dict[cand]['start'])[2:4] + '.csv', 'w') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)   \n",
    "        #write header\n",
    "        csvwriter.wrtie\n",
    "    # writing the data rows  \n",
    "        csvwriter.writerows(meta) \n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
