{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import random \n",
    "import os\n",
    "import csv\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "from numpy import loadtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf RF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8994561328180908\n",
      "PR AUC: 0.46307102434292435\n",
      "AUC: 0.8526549305853729\n",
      "PR AUC: 0.4114321821991345\n",
      "AUC: 0.8738371260913125\n",
      "PR AUC: 0.48183202495918653\n",
      "AUC: 0.8761270931730355\n",
      "PR AUC: 0.46443772592711435\n",
      "AUC: 0.8594532703592386\n",
      "PR AUC: 0.5062518174590759\n",
      "AUC: 0.7774748923959828\n",
      "PR AUC: 0.32999202276437317\n",
      "AUC: 0.833476456275941\n",
      "PR AUC: 0.392916544318665\n",
      "AUC: 0.8189494776012596\n",
      "PR AUC: 0.3046579340549502\n",
      "AUC: 0.8713324745956776\n",
      "PR AUC: 0.44520121719302047\n",
      "AUC: 0.8580220409331615\n",
      "PR AUC: 0.40510400875135855\n",
      "AUC: 0.8822098182338629\n",
      "PR AUC: 0.5260744227138304\n",
      "AUC: 0.8799198511521397\n",
      "PR AUC: 0.44511781360051894\n",
      "AUC: 0.853084299413196\n",
      "PR AUC: 0.3846262563327213\n",
      "AUC: 0.856312769010043\n",
      "PR AUC: 0.49336092122129793\n",
      "AUC: 0.8800629740947474\n",
      "PR AUC: 0.4776010258230655\n",
      "AUC: 0.8623157292113925\n",
      "PR AUC: 0.320394045041141\n",
      "AUC: 0.8837159253945481\n",
      "PR AUC: 0.4591976122651373\n",
      "AUC: 0.841133533705453\n",
      "PR AUC: 0.3202335894521821\n",
      "AUC: 0.8524402461714613\n",
      "PR AUC: 0.38175511429046627\n",
      "AUC: 0.8757692858165164\n",
      "PR AUC: 0.45475670236087495\n",
      "AUC: 0.8286102762272792\n",
      "PR AUC: 0.3848152142039679\n",
      "AUC: 0.8806354658651782\n",
      "PR AUC: 0.5113659544849714\n",
      "AUC: 0.8766995849434664\n",
      "PR AUC: 0.40327546983835627\n",
      "AUC: 0.8749641319942612\n",
      "PR AUC: 0.47161336763308515\n",
      "AUC: 0.8482181193645341\n",
      "PR AUC: 0.31880888215502884\n",
      "Mean AUC: 0.8598750362170863\n",
      "Mean PR AUC: 0.42231571573545795\n",
      "[[392  19]\n",
      " [ 24  10]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores = []\n",
    "pr_auc = []\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "annotated = pd.read_csv(\"All_annotated_data_round_1_high_pride.csv\")\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_high_pride/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_high_pride/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # pipeline BOW, tfidf and RF\n",
    "    text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf', RandomForestClassifier(n_estimators=5000, max_depth=3,max_features='sqrt', random_state=0, class_weight=\"balanced\")),\n",
    "                                                 ])\n",
    "      ## Fit the model to the training set\n",
    "    text_clf = text_clf.fit(train['text'], train['pred_class'])\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = text_clf.predict_proba(test['text'])\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(test['true_class'], preds[:,1], pos_label=1)\n",
    "    auc_scores = auc_scores + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(test['true_class'],  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    print(\"PR AUC: \" + str(lr_auc))\n",
    "    \n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores)))\n",
    "print(\"Mean PR AUC: \" + str(np.mean(pr_auc)))\n",
    "\n",
    "confusion = confusion_matrix(test['true_class'],text_clf.predict(test['text']) )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"high_pride_pr_auc_tfidf.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"high_pride_auc_tfidf.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Word2Vec RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8180057388809182\n",
      "PR AUC: 0.442222635602635\n",
      "AUC: 0.8570917418062115\n",
      "PR AUC: 0.34507768364641117\n",
      "AUC: 0.8300415056533562\n",
      "PR AUC: 0.39752272300070157\n",
      "AUC: 0.8600973236009732\n",
      "PR AUC: 0.41467470069598406\n",
      "AUC: 0.8236725347073136\n",
      "PR AUC: 0.34746236432557553\n",
      "AUC: 0.849713055954089\n",
      "PR AUC: 0.37704024329418273\n",
      "AUC: 0.8079626972740316\n",
      "PR AUC: 0.35298389224376986\n",
      "AUC: 0.8096464863317591\n",
      "PR AUC: 0.3420993310852235\n",
      "AUC: 0.8178760555317017\n",
      "PR AUC: 0.44885006048891235\n",
      "AUC: 0.7893830703012913\n",
      "PR AUC: 0.34242156327699774\n",
      "AUC: 0.795334192070989\n",
      "PR AUC: 0.295998945455875\n",
      "AUC: 0.8877200515242594\n",
      "PR AUC: 0.5196919017352609\n",
      "AUC: 0.7565997130559541\n",
      "PR AUC: 0.2773729157310143\n",
      "AUC: 0.8600430416068867\n",
      "PR AUC: 0.3711884609467261\n",
      "AUC: 0.8436381852010877\n",
      "PR AUC: 0.3651305209372267\n",
      "AUC: 0.7914698726205811\n",
      "PR AUC: 0.2853724949695878\n",
      "AUC: 0.8241750358680058\n",
      "PR AUC: 0.33267491714530806\n",
      "AUC: 0.823385588954408\n",
      "PR AUC: 0.3889122795607916\n",
      "AUC: 0.7928295405753543\n",
      "PR AUC: 0.2652908992060442\n",
      "AUC: 0.8725490196078431\n",
      "PR AUC: 0.4950255828974715\n",
      "AUC: 0.8483500717360114\n",
      "PR AUC: 0.4016024147174888\n",
      "AUC: 0.7820953198797768\n",
      "PR AUC: 0.3442130677591771\n",
      "AUC: 0.8426363246028338\n",
      "PR AUC: 0.38410712426731636\n",
      "AUC: 0.8534421067697152\n",
      "PR AUC: 0.40719303452501554\n",
      "AUC: 0.8235294117647058\n",
      "PR AUC: 0.3669796863425782\n",
      "Mean AUC: 0.8264515074352023\n",
      "Mean PR AUC: 0.37244437775429107\n",
      "[[342  68]\n",
      " [ 15  19]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores = []\n",
    "pr_auc = []\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "annotated = pd.read_csv(\"All_annotated_data_round_1_high_pride_20220209.csv\")\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "avg_embeddings = loadtxt('avg_embeddings.csv', delimiter=',')\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_high_pride/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_high_pride/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # attach train/test set to original df\n",
    "    train_id = train['speech_par_id']\n",
    "    test_id = test['speech_par_id']    \n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test_id), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train_id), 1, 0)\n",
    "    # get the embeddings to train/test set\n",
    "    train_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(train_id)].index.tolist()])\n",
    "    test_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(test_id)].index.tolist()])\n",
    "    ## Fit the model to the training set\n",
    "    text_clf = RandomForestClassifier(n_estimators=500, max_depth=3, random_state=0, max_features = \"sqrt\", class_weight=\"balanced\")\n",
    "    text_clf.fit(train_vec,  dat[dat['train']==1].pred_class) # use the original df to match the order\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = text_clf.predict_proba(test_vec)\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores = auc_scores + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    print(\"PR AUC: \" + str(lr_auc))\n",
    "    \n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores)))\n",
    "print(\"Mean PR AUC: \" + str(np.mean(pr_auc)))\n",
    "\n",
    "confusion = confusion_matrix(dat[dat['test']==1].pred_class,text_clf.predict(test_vec) )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"high_pride_pr_auc_w2v.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"high_pride_auc_w2v.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8251793400286944\n",
      "PR AUC: 0.40192543580128953\n",
      "AUC: 0.8314727350794332\n",
      "PR AUC: 0.37246305584621925\n",
      "AUC: 0.8424932016602261\n",
      "PR AUC: 0.3431718278291743\n",
      "AUC: 0.8542292829540575\n",
      "PR AUC: 0.4234265759811332\n",
      "AUC: 0.8577357950479462\n",
      "PR AUC: 0.3849908788034974\n",
      "AUC: 0.8949784791965566\n",
      "PR AUC: 0.46788137908162664\n",
      "AUC: 0.8158536585365854\n",
      "PR AUC: 0.38359594896684096\n",
      "AUC: 0.8550164591383999\n",
      "PR AUC: 0.4372844785920216\n",
      "AUC: 0.8276799771003293\n",
      "PR AUC: 0.42025494770499255\n",
      "AUC: 0.8258967001434719\n",
      "PR AUC: 0.35323759117011805\n",
      "AUC: 0.8780592528982396\n",
      "PR AUC: 0.3999754712374212\n",
      "AUC: 0.9028195219693718\n",
      "PR AUC: 0.4792163744768058\n",
      "AUC: 0.784648493543759\n",
      "PR AUC: 0.2905315955999843\n",
      "AUC: 0.8585365853658536\n",
      "PR AUC: 0.32712559668590907\n",
      "AUC: 0.8442106769715185\n",
      "PR AUC: 0.3137022509185967\n",
      "AUC: 0.8447831687419494\n",
      "PR AUC: 0.3136624145143112\n",
      "AUC: 0.8543758967001435\n",
      "PR AUC: 0.3036514781879088\n",
      "AUC: 0.8208686897741982\n",
      "PR AUC: 0.38049962288493433\n",
      "AUC: 0.7941176470588235\n",
      "PR AUC: 0.3207199906245569\n",
      "AUC: 0.8623872906826964\n",
      "PR AUC: 0.47953791116647704\n",
      "AUC: 0.8644189383070302\n",
      "PR AUC: 0.3920431483615426\n",
      "AUC: 0.8188779161299555\n",
      "PR AUC: 0.2471374904875939\n",
      "AUC: 0.8727637040217547\n",
      "PR AUC: 0.38825237957513775\n",
      "AUC: 0.8682553313296122\n",
      "PR AUC: 0.46600568342583265\n",
      "AUC: 0.9035150645624104\n",
      "PR AUC: 0.4850548796575416\n",
      "Mean AUC: 0.8481269522777207\n",
      "Mean PR AUC: 0.3830139363032587\n",
      "[[378  32]\n",
      " [ 14  20]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores = []\n",
    "pr_auc = []\n",
    "\n",
    "dat = pd.read_csv(\"20220115_all_paragraphs_2020_added.csv\")\n",
    "annotated = pd.read_csv(\"All_annotated_data_round_1_high_pride_20220209.csv\")\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "avg_embeddings = loadtxt('avg_embeddings_pretrained.csv', delimiter=',')\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_high_pride/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_high_pride/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # attach train/test set to original df\n",
    "    train_id = train['speech_par_id']\n",
    "    test_id = test['speech_par_id']    \n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test_id), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train_id), 1, 0)\n",
    "    # get the embeddings to train/test set\n",
    "    train_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(train_id)].index.tolist()])\n",
    "    test_vec = np.asarray([avg_embeddings[i] for i in dat[dat['speech_par_id'].isin(test_id)].index.tolist()])\n",
    "    ## Fit the model to the training set\n",
    "    text_clf = RandomForestClassifier(n_estimators=500, max_depth=3, random_state=0, max_features = \"sqrt\", class_weight=\"balanced\")\n",
    "    text_clf.fit(train_vec,  dat[dat['train']==1].pred_class) # use the original df to match the order\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = text_clf.predict_proba(test_vec)\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores = auc_scores + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    print(\"PR AUC: \" + str(lr_auc))\n",
    "    \n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores)))\n",
    "print(\"Mean PR AUC: \" + str(np.mean(pr_auc)))\n",
    "\n",
    "confusion = confusion_matrix(dat[dat['test']==1].pred_class,text_clf.predict(test_vec) )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"high_pride_pr_auc_pretrained.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"high_pride_auc_pretrained.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.835466705019416\n",
      "Accuracy: 0.9232505643340858\n",
      "PR_AUC: 0.39994181156287323\n",
      "AUC: 0.8451219512195122\n",
      "Accuracy: 0.9234234234234234\n",
      "PR_AUC: 0.3055014082745046\n",
      "AUC: 0.8227403156384504\n",
      "Accuracy: 0.9256756756756757\n",
      "PR_AUC: 0.41585992996961074\n",
      "AUC: 0.8711621233859397\n",
      "Accuracy: 0.9234234234234234\n",
      "PR_AUC: 0.4590812155542532\n",
      "AUC: 0.8052812365822241\n",
      "Accuracy: 0.9235955056179775\n",
      "PR_AUC: 0.3089456809181288\n",
      "AUC: 0.8645189126995542\n",
      "Accuracy: 0.9255079006772009\n",
      "PR_AUC: 0.49701786180441687\n",
      "AUC: 0.8701999137063138\n",
      "Accuracy: 0.9232505643340858\n",
      "PR_AUC: 0.44378342556800904\n",
      "AUC: 0.8328324030342064\n",
      "Accuracy: 0.9235955056179775\n",
      "PR_AUC: 0.37137146948231115\n",
      "AUC: 0.8727637040217546\n",
      "Accuracy: 0.9235955056179775\n",
      "PR_AUC: 0.42687718477745434\n",
      "AUC: 0.840028694404591\n",
      "Accuracy: 0.9234234234234234\n",
      "PR_AUC: 0.3625260915100735\n",
      "AUC: 0.8632245074068746\n",
      "Accuracy: 0.9232505643340858\n",
      "PR_AUC: 0.40579683227609853\n",
      "AUC: 0.8890081580077286\n",
      "Accuracy: 0.9235955056179775\n",
      "PR_AUC: 0.4352501896973803\n",
      "AUC: 0.7949067431850789\n",
      "Accuracy: 0.9234234234234234\n",
      "PR_AUC: 0.3021449893348238\n",
      "AUC: 0.853589965397924\n",
      "Accuracy: 0.9230769230769231\n",
      "PR_AUC: 0.3719605931713883\n",
      "AUC: 0.8802776585086588\n",
      "Accuracy: 0.9235955056179775\n",
      "PR_AUC: 0.4721662212634252\n",
      "AUC: 0.8533705452984113\n",
      "Accuracy: 0.9235955056179775\n",
      "PR_AUC: 0.28567248690954805\n",
      "AUC: 0.8992109038737447\n",
      "Accuracy: 0.9234234234234234\n",
      "PR_AUC: 0.42847105565913485\n",
      "AUC: 0.8230979433338128\n",
      "Accuracy: 0.9232505643340858\n",
      "PR_AUC: 0.4045613219800557\n",
      "AUC: 0.8271790468012022\n",
      "Accuracy: 0.9235955056179775\n",
      "PR_AUC: 0.31590749837718773\n",
      "AUC: 0.917918992414484\n",
      "Accuracy: 0.9235955056179775\n",
      "PR_AUC: 0.546304045128617\n",
      "AUC: 0.8675753228120516\n",
      "Accuracy: 0.9234234234234234\n",
      "PR_AUC: 0.42870819923515713\n",
      "AUC: 0.8678975239730928\n",
      "Accuracy: 0.9235955056179775\n",
      "PR_AUC: 0.36851027549967\n",
      "AUC: 0.8509374552740805\n",
      "Accuracy: 0.9235955056179775\n",
      "PR_AUC: 0.37513480198585974\n",
      "AUC: 0.8723343351939316\n",
      "Accuracy: 0.9235955056179775\n",
      "PR_AUC: 0.35402463732114925\n",
      "AUC: 0.9031563845050216\n",
      "Accuracy: 0.9234234234234234\n",
      "PR_AUC: 0.4877085637419188\n",
      "Mean AUC: 0.8569520578279224\n",
      "Mean Accuracy: 0.9236310913011144\n",
      "[[410   0]\n",
      " [ 34   0]]\n"
     ]
    }
   ],
   "source": [
    "auc_scores_d2v = []\n",
    "pr_auc = []\n",
    "accuracy_scores_d2v = []\n",
    "\n",
    "dat = pd.read_csv(\"20201115_all_paragraphs.csv\")\n",
    "dat['speech_par_id'] = dat['Speech_id'].astype(str) + \"_\" + dat['par_id'].astype(str)\n",
    "\n",
    "model = Word2Vec.load(\"doc2vec_wordvecs.model\") \n",
    "annotated = pd.read_csv(\"All_annotated_data_round_1_high_pride_20220209.csv\")\n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_high_pride/\"):\n",
    "    ## load the test set \n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_high_pride/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # merge test and train sets back onto the total df\n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test[\"speech_par_id\"]), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train[\"speech_par_id\"]), 1, 0)\n",
    "    # so that teh order of rows are the same with the embeddings\n",
    "    test_set = np.asarray([model.docvecs[i] for i in dat[dat['test'] == 1].index.tolist()])\n",
    "    train_set = np.asarray([model.docvecs[i] for i in dat[dat['train']== 1].index.tolist()])\n",
    "    ## Initialize a random forest classifier\n",
    "    gbc = RandomForestClassifier(n_estimators=500, max_depth=10, random_state=0, max_features = \"sqrt\", class_weight=\"balanced\")\n",
    "    ## Fit the model to the training set\n",
    "    gbc.fit(train_set, dat[dat['train']==1].pred_class)\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = gbc.predict_proba(test_set)\n",
    "    fpr_d2v, tpr_d2v, thresholds_d2v = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores_d2v = auc_scores_d2v + [metrics.auc(fpr_d2v, tpr_d2v)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc = pr_auc + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr_d2v, tpr_d2v)))\n",
    "    accuracy_d2v = metrics.accuracy_score(dat[dat['test']==1].pred_class, gbc.predict(test_set))\n",
    "    accuracy_scores_d2v = accuracy_scores_d2v + [accuracy_d2v]\n",
    "    print(\"Accuracy: \" + str(accuracy_d2v))\n",
    "    print(\"PR_AUC: \" + str(lr_auc))\n",
    "\n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores_d2v)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_d2v)))\n",
    "\n",
    "confusion = confusion_matrix(dat[dat['test']==1].pred_class, gbc.predict(test_set))\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"high_pride_pr_auc_d2v.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"high_pride_auc_d2v.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores_d2v:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC-AUC: 0.8569520578279224\n",
      "Mean Accuracy: 0.9236310913011144\n",
      "Mean PR_AUC: 0.39892911164012196\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean ROC-AUC: \" + str(np.mean(auc_scores_d2v)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_d2v)))\n",
    "print(\"Mean PR_AUC: \" + str(np.mean(pr_auc)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# balanced RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8607076082266648\n",
      "Accuracy: 0.7923250564334086\n",
      "PR_AUC: 0.4817791637319441\n",
      "AUC: 0.8456241032998565\n",
      "Accuracy: 0.7747747747747747\n",
      "PR_AUC: 0.323554349948828\n",
      "AUC: 0.849426111908178\n",
      "Accuracy: 0.7725225225225225\n",
      "PR_AUC: 0.4915918237882323\n",
      "AUC: 0.8779770444763271\n",
      "Accuracy: 0.786036036036036\n",
      "PR_AUC: 0.4262213918082809\n",
      "AUC: 0.837770144554172\n",
      "Accuracy: 0.7640449438202247\n",
      "PR_AUC: 0.39071580318101157\n",
      "AUC: 0.8850855745721271\n",
      "Accuracy: 0.8171557562076749\n",
      "PR_AUC: 0.504705332103836\n",
      "AUC: 0.8691931540342299\n",
      "Accuracy: 0.7404063205417607\n",
      "PR_AUC: 0.49366847079231163\n",
      "AUC: 0.8465722055245455\n",
      "Accuracy: 0.8067415730337079\n",
      "PR_AUC: 0.3902174669453447\n",
      "AUC: 0.8661800486618004\n",
      "Accuracy: 0.8157303370786517\n",
      "PR_AUC: 0.4859437738085214\n",
      "AUC: 0.8454088952654232\n",
      "Accuracy: 0.795045045045045\n",
      "PR_AUC: 0.43171761563685407\n",
      "AUC: 0.8839349920897455\n",
      "Accuracy: 0.7562076749435666\n",
      "PR_AUC: 0.4054418670851804\n",
      "AUC: 0.90353513668241\n",
      "Accuracy: 0.7955056179775281\n",
      "PR_AUC: 0.5234028755985269\n",
      "AUC: 0.798206599713056\n",
      "Accuracy: 0.7522522522522522\n",
      "PR_AUC: 0.3024328976930591\n",
      "AUC: 0.8760092272203\n",
      "Accuracy: 0.7624434389140271\n",
      "PR_AUC: 0.46401993696780786\n",
      "AUC: 0.876914269357378\n",
      "Accuracy: 0.7932584269662921\n",
      "PR_AUC: 0.3624558522760812\n",
      "AUC: 0.8671819092600543\n",
      "Accuracy: 0.7820224719101123\n",
      "PR_AUC: 0.3167181744283737\n",
      "AUC: 0.8988522238163558\n",
      "Accuracy: 0.7747747747747747\n",
      "PR_AUC: 0.3965761362006157\n",
      "AUC: 0.8411477060261757\n",
      "Accuracy: 0.7607223476297968\n",
      "PR_AUC: 0.3712458724802566\n",
      "AUC: 0.8314727350794332\n",
      "Accuracy: 0.7617977528089888\n",
      "PR_AUC: 0.3840643919776882\n",
      "AUC: 0.9328753399169887\n",
      "Accuracy: 0.7752808988764045\n",
      "PR_AUC: 0.5846066268400114\n",
      "AUC: 0.8707317073170733\n",
      "Accuracy: 0.7837837837837838\n",
      "PR_AUC: 0.3979064381248565\n",
      "AUC: 0.8787033061399743\n",
      "Accuracy: 0.7685393258426966\n",
      "PR_AUC: 0.35958664303029114\n",
      "AUC: 0.856376127093173\n",
      "Accuracy: 0.7573033707865169\n",
      "PR_AUC: 0.43689150484050576\n",
      "AUC: 0.8625304136253041\n",
      "Accuracy: 0.7887640449438202\n",
      "PR_AUC: 0.4237259151277424\n",
      "AUC: 0.9114060258249641\n",
      "Accuracy: 0.7972972972972973\n",
      "PR_AUC: 0.5560041556820289\n",
      "Mean AUC: 0.8669529043874283\n",
      "Mean Accuracy: 0.7789894338080665\n"
     ]
    }
   ],
   "source": [
    "auc_scores_balanced = []\n",
    "pr_auc_balanced = []\n",
    "accuracy_scores_balanced = []\n",
    "\n",
    "dat = pd.read_csv(\"20201115_all_paragraphs.csv\")\n",
    "dat['speech_par_id'] = dat['Speech_id'].astype(str) + \"_\" + dat['par_id'].astype(str)\n",
    "# X = np.asarray([model.docvecs[i] for i in label_dat.index.tolist()])\n",
    "# Y = np.asarray(label_dat['label'].tolist(), dtype=\"int\")\n",
    "model = Word2Vec.load(\"doc2vec_wordvecs.model\") \n",
    "dat = dat.merge(annotated[[\"speech_par_id\", \"pred_class\"]], how = \"left\", on = \"speech_par_id\")\n",
    "\n",
    "\n",
    "for filename in os.listdir(\"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_high_pride/\"):\n",
    "    ## load the test set \n",
    "    annotated = pd.read_csv(\"All_annotated_data_round_1_high_pride_20220209.csv\")\n",
    "    file = \"/Users/yuchenluo/Desktop/Measure_RadicalDiscourse/codes/Prediction_of_test_high_pride/\" + \"/\" + filename\n",
    "    test = pd.read_csv(file)\n",
    "    train = annotated[~annotated['speech_par_id'].isin(test[\"speech_par_id\"])]\n",
    "    test = test[test['speech_par_id'].isin(dat['speech_par_id'])]\n",
    "    train = train[train['speech_par_id'].isin(dat['speech_par_id'])] # make sure annotated data and the original df match\n",
    "    # merge test and train sets back onto the total df\n",
    "    dat['test'] = np.where(dat['speech_par_id'].isin(test[\"speech_par_id\"]), 1, 0)\n",
    "    dat['train'] = np.where(dat['speech_par_id'].isin(train[\"speech_par_id\"]), 1, 0)\n",
    "    # so that teh order of rows are the same with the embeddings\n",
    "    test_set = np.asarray([model.docvecs[i] for i in dat[dat['test'] == 1].index.tolist()])\n",
    "    train_set = np.asarray([model.docvecs[i] for i in dat[dat['train']== 1].index.tolist()])\n",
    "    ## Initialize a random forest classifier\n",
    "    brfc = BalancedRandomForestClassifier(n_estimators=5000, max_depth=3, random_state=0, max_features = \"sqrt\")\n",
    "    ## Fit the model to the training set\n",
    "    brfc.fit(train_set, dat[dat['train']==1].pred_class)\n",
    "    ## Predict out-of-sample on the test set and compute AUC\n",
    "    preds = brfc.predict_proba(test_set)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(dat[dat['test']==1].pred_class, preds[:,1], pos_label=1)\n",
    "    auc_scores_balanced = auc_scores_balanced + [metrics.auc(fpr, tpr)]\n",
    "    #PR AUC\n",
    "    lr_precision, lr_recall, _ = precision_recall_curve(dat[dat['test']==1].pred_class,  preds[:,1].tolist())\n",
    "    lr_auc = auc(lr_recall, lr_precision)\n",
    "    pr_auc_balanced = pr_auc_balanced + [lr_auc]\n",
    "    print(\"AUC: \"+str(metrics.auc(fpr, tpr)))\n",
    "    accuracy_balanced = metrics.accuracy_score(dat[dat['test']==1].pred_class, brfc.predict(test_set))\n",
    "    accuracy_scores_balanced = accuracy_scores_balanced + [accuracy_balanced]\n",
    "    print(\"Accuracy: \" + str(accuracy_balanced))\n",
    "    print(\"PR_AUC: \" + str(lr_auc))\n",
    "\n",
    "print(\"Mean AUC: \" + str(np.mean(auc_scores_balanced)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"high_pride_pr_auc_balanced.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in pr_auc_balanced:\n",
    "        csvwriter.writerow([row])    \n",
    "    \n",
    "with open(\"high_pride_auc_balanced.csv\", \"w\") as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for row in auc_scores_balanced:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ROC-AUC: 0.8669529043874283\n",
      "Mean Accuracy: 0.7789894338080665\n",
      "Mean PR_AUC: 0.42820777920392755\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean ROC-AUC: \" + str(np.mean(auc_scores_balanced)))\n",
    "print(\"Mean Accuracy: \" + str(np.mean(accuracy_scores_balanced)))\n",
    "print(\"Mean PR_AUC: \" + str(np.mean(pr_auc_balanced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
